//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;
.extern .func free
(
	.param .b64 free_param_0
)
;
.const .align 8 .b8 CUDA_KECCAK_CONSTS[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};

.func  (.param .b32 func_retval0) _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_(
	.param .b64 _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0,
	.param .b64 _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<10>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd9, [_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0];
	ld.param.u64 	%rd10, [_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1];
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.local.u64 	%rd1, %rd9;
	ld.global.u64 	%rd3, [%rd2+24];
	ld.local.u64 	%rd4, [%rd1+24];
	setp.gt.u64 	%p1, %rd4, %rd3;
	mov.u16 	%rs3, 0;
	mov.u16 	%rs9, %rs3;
	@%p1 bra 	$L__BB0_7;

	setp.lt.u64 	%p2, %rd4, %rd3;
	mov.u16 	%rs4, 1;
	mov.u16 	%rs9, %rs4;
	@%p2 bra 	$L__BB0_7;

	ld.global.u64 	%rd5, [%rd2+16];
	ld.local.u64 	%rd6, [%rd1+16];
	setp.gt.u64 	%p3, %rd6, %rd5;
	mov.u16 	%rs9, %rs3;
	@%p3 bra 	$L__BB0_7;

	setp.lt.u64 	%p4, %rd6, %rd5;
	mov.u16 	%rs9, %rs4;
	@%p4 bra 	$L__BB0_7;

	ld.global.u64 	%rd7, [%rd2+8];
	ld.local.u64 	%rd8, [%rd1+8];
	setp.gt.u64 	%p5, %rd8, %rd7;
	mov.u16 	%rs9, %rs3;
	@%p5 bra 	$L__BB0_7;

	setp.lt.u64 	%p6, %rd8, %rd7;
	mov.u16 	%rs9, %rs4;
	@%p6 bra 	$L__BB0_7;

	ld.local.u64 	%rd11, [%rd1];
	ld.global.u64 	%rd12, [%rd2];
	setp.le.u64 	%p7, %rd11, %rd12;
	selp.u16 	%rs9, 1, 0, %p7;

$L__BB0_7:
	cvt.u32.u16 	%r1, %rs9;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	kernel_lilypad_pow
.visible .entry kernel_lilypad_pow(
	.param .u64 kernel_lilypad_pow_param_0,
	.param .u64 kernel_lilypad_pow_param_1,
	.param .u64 kernel_lilypad_pow_param_2,
	.param .u32 kernel_lilypad_pow_param_3,
	.param .u32 kernel_lilypad_pow_param_4,
	.param .u64 kernel_lilypad_pow_param_5
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot1[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<75>;
	.reg .b32 	%r<86>;
	.reg .b64 	%rd<338>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [kernel_lilypad_pow_param_0];
	ld.param.u64 	%rd67, [kernel_lilypad_pow_param_1];
	ld.param.u64 	%rd68, [kernel_lilypad_pow_param_2];
	ld.param.u32 	%r9, [kernel_lilypad_pow_param_3];
	ld.param.u32 	%r8, [kernel_lilypad_pow_param_4];
	ld.param.u64 	%rd69, [kernel_lilypad_pow_param_5];
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r11, %r10, %r12;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB1_9;

	mul.lo.s32 	%r84, %r1, %r8;
	add.s32 	%r3, %r84, %r8;
	setp.ge.u32 	%p2, %r84, %r3;
	@%p2 bra 	$L__BB1_9;

	cvta.to.global.u64 	%rd1, %rd69;
	add.u64 	%rd70, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	cvta.to.global.u64 	%rd89, %rd66;
	cvta.to.global.u64 	%rd91, %rd67;
	mov.u64 	%rd71, CUDA_KECCAK_CONSTS;

$L__BB1_3:
	mov.u64 	%rd90, 32;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd90;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd3, [retval0+0];
	} // callseq 0
	mov.u32 	%r85, 0;
	ld.global.nc.u64 	%rd92, [%rd91];
	cvt.u64.u32 	%rd93, %r84;
	add.s64 	%rd316, %rd92, %rd93;
	setp.lt.u64 	%p3, %rd316, %rd92;
	mov.u64 	%rd312, 0;
	st.u64 	[%rd3], %rd316;
	selp.u64 	%rd94, 1, 0, %p3;
	ld.global.nc.u64 	%rd95, [%rd91+8];
	add.s64 	%rd335, %rd95, %rd94;
	setp.lt.u64 	%p4, %rd335, %rd95;
	st.u64 	[%rd3+8], %rd335;
	selp.u64 	%rd96, 1, 0, %p4;
	ld.global.nc.u64 	%rd97, [%rd91+16];
	add.s64 	%rd330, %rd97, %rd96;
	setp.lt.u64 	%p5, %rd330, %rd97;
	st.u64 	[%rd3+16], %rd330;
	selp.u64 	%rd98, 1, 0, %p5;
	ld.global.nc.u64 	%rd99, [%rd91+24];
	add.s64 	%rd325, %rd99, %rd98;
	st.u64 	[%rd3+24], %rd325;
	ld.global.nc.u8 	%rs1, [%rd89];
	cvt.u64.u8 	%rd100, %rs1;
	ld.global.nc.u8 	%rs2, [%rd89+1];
	cvt.u64.u8 	%rd101, %rs2;
	bfi.b64 	%rd102, %rd101, %rd100, 8, 8;
	ld.global.nc.u8 	%rs3, [%rd89+2];
	cvt.u64.u8 	%rd103, %rs3;
	ld.global.nc.u8 	%rs4, [%rd89+3];
	cvt.u64.u8 	%rd104, %rs4;
	bfi.b64 	%rd105, %rd104, %rd103, 8, 8;
	bfi.b64 	%rd106, %rd105, %rd102, 16, 16;
	ld.global.nc.u8 	%rs5, [%rd89+4];
	cvt.u64.u8 	%rd107, %rs5;
	ld.global.nc.u8 	%rs6, [%rd89+5];
	cvt.u64.u8 	%rd108, %rs6;
	bfi.b64 	%rd109, %rd108, %rd107, 8, 8;
	ld.global.nc.u8 	%rs7, [%rd89+6];
	cvt.u64.u8 	%rd110, %rs7;
	ld.global.nc.u8 	%rs8, [%rd89+7];
	cvt.u64.u8 	%rd111, %rs8;
	bfi.b64 	%rd112, %rd111, %rd110, 8, 8;
	bfi.b64 	%rd113, %rd112, %rd109, 16, 16;
	bfi.b64 	%rd336, %rd113, %rd106, 32, 32;
	ld.global.nc.u8 	%rs9, [%rd89+8];
	cvt.u64.u8 	%rd114, %rs9;
	ld.global.nc.u8 	%rs10, [%rd89+9];
	cvt.u64.u8 	%rd115, %rs10;
	bfi.b64 	%rd116, %rd115, %rd114, 8, 8;
	ld.global.nc.u8 	%rs11, [%rd89+10];
	cvt.u64.u8 	%rd117, %rs11;
	ld.global.nc.u8 	%rs12, [%rd89+11];
	cvt.u64.u8 	%rd118, %rs12;
	bfi.b64 	%rd119, %rd118, %rd117, 8, 8;
	bfi.b64 	%rd120, %rd119, %rd116, 16, 16;
	ld.global.nc.u8 	%rs13, [%rd89+12];
	cvt.u64.u8 	%rd121, %rs13;
	ld.global.nc.u8 	%rs14, [%rd89+13];
	cvt.u64.u8 	%rd122, %rs14;
	bfi.b64 	%rd123, %rd122, %rd121, 8, 8;
	ld.global.nc.u8 	%rs15, [%rd89+14];
	cvt.u64.u8 	%rd124, %rs15;
	ld.global.nc.u8 	%rs16, [%rd89+15];
	cvt.u64.u8 	%rd125, %rs16;
	bfi.b64 	%rd126, %rd125, %rd124, 8, 8;
	bfi.b64 	%rd127, %rd126, %rd123, 16, 16;
	bfi.b64 	%rd331, %rd127, %rd120, 32, 32;
	ld.global.nc.u8 	%rs17, [%rd89+16];
	cvt.u64.u8 	%rd128, %rs17;
	ld.global.nc.u8 	%rs18, [%rd89+17];
	cvt.u64.u8 	%rd129, %rs18;
	bfi.b64 	%rd130, %rd129, %rd128, 8, 8;
	ld.global.nc.u8 	%rs19, [%rd89+18];
	cvt.u64.u8 	%rd131, %rs19;
	ld.global.nc.u8 	%rs20, [%rd89+19];
	cvt.u64.u8 	%rd132, %rs20;
	bfi.b64 	%rd133, %rd132, %rd131, 8, 8;
	bfi.b64 	%rd134, %rd133, %rd130, 16, 16;
	ld.global.nc.u8 	%rs21, [%rd89+20];
	cvt.u64.u8 	%rd135, %rs21;
	ld.global.nc.u8 	%rs22, [%rd89+21];
	cvt.u64.u8 	%rd136, %rs22;
	bfi.b64 	%rd137, %rd136, %rd135, 8, 8;
	ld.global.nc.u8 	%rs23, [%rd89+22];
	cvt.u64.u8 	%rd138, %rs23;
	ld.global.nc.u8 	%rs24, [%rd89+23];
	cvt.u64.u8 	%rd139, %rs24;
	bfi.b64 	%rd140, %rd139, %rd138, 8, 8;
	bfi.b64 	%rd141, %rd140, %rd137, 16, 16;
	bfi.b64 	%rd326, %rd141, %rd134, 32, 32;
	ld.global.nc.u8 	%rs25, [%rd89+24];
	cvt.u64.u8 	%rd142, %rs25;
	ld.global.nc.u8 	%rs26, [%rd89+25];
	cvt.u64.u8 	%rd143, %rs26;
	bfi.b64 	%rd144, %rd143, %rd142, 8, 8;
	ld.global.nc.u8 	%rs27, [%rd89+26];
	cvt.u64.u8 	%rd145, %rs27;
	ld.global.nc.u8 	%rs28, [%rd89+27];
	cvt.u64.u8 	%rd146, %rs28;
	bfi.b64 	%rd147, %rd146, %rd145, 8, 8;
	bfi.b64 	%rd148, %rd147, %rd144, 16, 16;
	ld.global.nc.u8 	%rs29, [%rd89+28];
	cvt.u64.u8 	%rd149, %rs29;
	ld.global.nc.u8 	%rs30, [%rd89+29];
	cvt.u64.u8 	%rd150, %rs30;
	bfi.b64 	%rd151, %rd150, %rd149, 8, 8;
	ld.global.nc.u8 	%rs31, [%rd89+30];
	cvt.u64.u8 	%rd152, %rs31;
	ld.global.nc.u8 	%rs32, [%rd89+31];
	cvt.u64.u8 	%rd153, %rs32;
	bfi.b64 	%rd154, %rd153, %rd152, 8, 8;
	bfi.b64 	%rd155, %rd154, %rd151, 16, 16;
	bfi.b64 	%rd321, %rd155, %rd148, 32, 32;
	mov.u64 	%rd328, -9223372036854775808;
	mov.u64 	%rd320, 1;
	mov.u64 	%rd311, %rd71;
	mov.u64 	%rd313, %rd312;
	mov.u64 	%rd314, %rd312;
	mov.u64 	%rd315, %rd312;
	mov.u64 	%rd317, %rd312;
	mov.u64 	%rd318, %rd312;
	mov.u64 	%rd319, %rd312;
	mov.u64 	%rd322, %rd312;
	mov.u64 	%rd323, %rd312;
	mov.u64 	%rd324, %rd312;
	mov.u64 	%rd327, %rd312;
	mov.u64 	%rd329, %rd312;
	mov.u64 	%rd332, %rd312;
	mov.u64 	%rd333, %rd312;
	mov.u64 	%rd334, %rd312;

$L__BB1_4:
	xor.b64  	%rd214, %rd335, %rd336;
	xor.b64  	%rd215, %rd214, %rd334;
	xor.b64  	%rd216, %rd215, %rd333;
	xor.b64  	%rd165, %rd216, %rd332;
	xor.b64  	%rd217, %rd330, %rd331;
	xor.b64  	%rd218, %rd217, %rd329;
	xor.b64  	%rd219, %rd218, %rd328;
	xor.b64  	%rd157, %rd219, %rd327;
	xor.b64  	%rd220, %rd325, %rd326;
	xor.b64  	%rd221, %rd220, %rd324;
	xor.b64  	%rd222, %rd221, %rd323;
	xor.b64  	%rd159, %rd222, %rd322;
	xor.b64  	%rd223, %rd320, %rd321;
	xor.b64  	%rd224, %rd223, %rd319;
	xor.b64  	%rd225, %rd224, %rd318;
	xor.b64  	%rd161, %rd225, %rd317;
	xor.b64  	%rd226, %rd315, %rd316;
	xor.b64  	%rd227, %rd226, %rd314;
	xor.b64  	%rd228, %rd227, %rd313;
	xor.b64  	%rd163, %rd228, %rd312;
	mov.u32 	%r19, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd157;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd156, {vl,vh};
	@p  mov.b64 %rd156, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd229, %rd156, %rd163;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd159;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd158, {vl,vh};
	@p  mov.b64 %rd158, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd230, %rd158, %rd165;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd161;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd160, {vl,vh};
	@p  mov.b64 %rd160, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd231, %rd160, %rd157;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd163;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd162, {vl,vh};
	@p  mov.b64 %rd162, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd232, %rd162, %rd159;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd165;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd164, {vl,vh};
	@p  mov.b64 %rd164, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd233, %rd164, %rd161;
	xor.b64  	%rd234, %rd336, %rd229;
	xor.b64  	%rd201, %rd335, %rd229;
	xor.b64  	%rd213, %rd334, %rd229;
	xor.b64  	%rd189, %rd333, %rd229;
	xor.b64  	%rd177, %rd332, %rd229;
	xor.b64  	%rd167, %rd331, %rd230;
	xor.b64  	%rd169, %rd330, %rd230;
	xor.b64  	%rd209, %rd329, %rd230;
	xor.b64  	%rd199, %rd328, %rd230;
	xor.b64  	%rd195, %rd327, %rd230;
	xor.b64  	%rd179, %rd326, %rd231;
	xor.b64  	%rd211, %rd325, %rd231;
	xor.b64  	%rd181, %rd324, %rd231;
	xor.b64  	%rd207, %rd323, %rd231;
	xor.b64  	%rd173, %rd322, %rd231;
	xor.b64  	%rd203, %rd321, %rd232;
	xor.b64  	%rd197, %rd320, %rd232;
	xor.b64  	%rd183, %rd319, %rd232;
	xor.b64  	%rd205, %rd318, %rd232;
	xor.b64  	%rd187, %rd317, %rd232;
	xor.b64  	%rd191, %rd316, %rd233;
	xor.b64  	%rd171, %rd315, %rd233;
	xor.b64  	%rd175, %rd314, %rd233;
	xor.b64  	%rd185, %rd313, %rd233;
	xor.b64  	%rd193, %rd312, %rd233;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd167;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd166, {vl,vh};
	@p  mov.b64 %rd166, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r20, 44;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd169;
	shf.l.wrap.b32 vl, tl, th, %r20;
	shf.l.wrap.b32 vh, th, tl, %r20;
	setp.lt.u32 p, %r20, 32;
	@!p mov.b64 %rd168, {vl,vh};
	@p  mov.b64 %rd168, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r21, 20;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd171;
	shf.l.wrap.b32 vl, tl, th, %r21;
	shf.l.wrap.b32 vh, th, tl, %r21;
	setp.lt.u32 p, %r21, 32;
	@!p mov.b64 %rd170, {vl,vh};
	@p  mov.b64 %rd170, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r22, 61;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd173;
	shf.l.wrap.b32 vl, tl, th, %r22;
	shf.l.wrap.b32 vh, th, tl, %r22;
	setp.lt.u32 p, %r22, 32;
	@!p mov.b64 %rd172, {vl,vh};
	@p  mov.b64 %rd172, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r23, 39;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd175;
	shf.l.wrap.b32 vl, tl, th, %r23;
	shf.l.wrap.b32 vh, th, tl, %r23;
	setp.lt.u32 p, %r23, 32;
	@!p mov.b64 %rd174, {vl,vh};
	@p  mov.b64 %rd174, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r24, 18;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd177;
	shf.l.wrap.b32 vl, tl, th, %r24;
	shf.l.wrap.b32 vh, th, tl, %r24;
	setp.lt.u32 p, %r24, 32;
	@!p mov.b64 %rd176, {vl,vh};
	@p  mov.b64 %rd176, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r25, 62;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd179;
	shf.l.wrap.b32 vl, tl, th, %r25;
	shf.l.wrap.b32 vh, th, tl, %r25;
	setp.lt.u32 p, %r25, 32;
	@!p mov.b64 %rd178, {vl,vh};
	@p  mov.b64 %rd178, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r26, 43;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd181;
	shf.l.wrap.b32 vl, tl, th, %r26;
	shf.l.wrap.b32 vh, th, tl, %r26;
	setp.lt.u32 p, %r26, 32;
	@!p mov.b64 %rd180, {vl,vh};
	@p  mov.b64 %rd180, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r27, 25;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd183;
	shf.l.wrap.b32 vl, tl, th, %r27;
	shf.l.wrap.b32 vh, th, tl, %r27;
	setp.lt.u32 p, %r27, 32;
	@!p mov.b64 %rd182, {vl,vh};
	@p  mov.b64 %rd182, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r28, 8;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd185;
	shf.l.wrap.b32 vl, tl, th, %r28;
	shf.l.wrap.b32 vh, th, tl, %r28;
	setp.lt.u32 p, %r28, 32;
	@!p mov.b64 %rd184, {vl,vh};
	@p  mov.b64 %rd184, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r29, 56;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd187;
	shf.l.wrap.b32 vl, tl, th, %r29;
	shf.l.wrap.b32 vh, th, tl, %r29;
	setp.lt.u32 p, %r29, 32;
	@!p mov.b64 %rd186, {vl,vh};
	@p  mov.b64 %rd186, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r30, 41;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd189;
	shf.l.wrap.b32 vl, tl, th, %r30;
	shf.l.wrap.b32 vh, th, tl, %r30;
	setp.lt.u32 p, %r30, 32;
	@!p mov.b64 %rd188, {vl,vh};
	@p  mov.b64 %rd188, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r31, 27;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd191;
	shf.l.wrap.b32 vl, tl, th, %r31;
	shf.l.wrap.b32 vh, th, tl, %r31;
	setp.lt.u32 p, %r31, 32;
	@!p mov.b64 %rd190, {vl,vh};
	@p  mov.b64 %rd190, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r32, 14;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd193;
	shf.l.wrap.b32 vl, tl, th, %r32;
	shf.l.wrap.b32 vh, th, tl, %r32;
	setp.lt.u32 p, %r32, 32;
	@!p mov.b64 %rd192, {vl,vh};
	@p  mov.b64 %rd192, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r33, 2;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd195;
	shf.l.wrap.b32 vl, tl, th, %r33;
	shf.l.wrap.b32 vh, th, tl, %r33;
	setp.lt.u32 p, %r33, 32;
	@!p mov.b64 %rd194, {vl,vh};
	@p  mov.b64 %rd194, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r34, 55;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd197;
	shf.l.wrap.b32 vl, tl, th, %r34;
	shf.l.wrap.b32 vh, th, tl, %r34;
	setp.lt.u32 p, %r34, 32;
	@!p mov.b64 %rd196, {vl,vh};
	@p  mov.b64 %rd196, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r35, 45;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd199;
	shf.l.wrap.b32 vl, tl, th, %r35;
	shf.l.wrap.b32 vh, th, tl, %r35;
	setp.lt.u32 p, %r35, 32;
	@!p mov.b64 %rd198, {vl,vh};
	@p  mov.b64 %rd198, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r36, 36;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd201;
	shf.l.wrap.b32 vl, tl, th, %r36;
	shf.l.wrap.b32 vh, th, tl, %r36;
	setp.lt.u32 p, %r36, 32;
	@!p mov.b64 %rd200, {vl,vh};
	@p  mov.b64 %rd200, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r37, 28;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd203;
	shf.l.wrap.b32 vl, tl, th, %r37;
	shf.l.wrap.b32 vh, th, tl, %r37;
	setp.lt.u32 p, %r37, 32;
	@!p mov.b64 %rd202, {vl,vh};
	@p  mov.b64 %rd202, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r38, 21;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd205;
	shf.l.wrap.b32 vl, tl, th, %r38;
	shf.l.wrap.b32 vh, th, tl, %r38;
	setp.lt.u32 p, %r38, 32;
	@!p mov.b64 %rd204, {vl,vh};
	@p  mov.b64 %rd204, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r39, 15;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd207;
	shf.l.wrap.b32 vl, tl, th, %r39;
	shf.l.wrap.b32 vh, th, tl, %r39;
	setp.lt.u32 p, %r39, 32;
	@!p mov.b64 %rd206, {vl,vh};
	@p  mov.b64 %rd206, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r40, 10;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd209;
	shf.l.wrap.b32 vl, tl, th, %r40;
	shf.l.wrap.b32 vh, th, tl, %r40;
	setp.lt.u32 p, %r40, 32;
	@!p mov.b64 %rd208, {vl,vh};
	@p  mov.b64 %rd208, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r41, 6;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd211;
	shf.l.wrap.b32 vl, tl, th, %r41;
	shf.l.wrap.b32 vh, th, tl, %r41;
	setp.lt.u32 p, %r41, 32;
	@!p mov.b64 %rd210, {vl,vh};
	@p  mov.b64 %rd210, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r42, 3;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd213;
	shf.l.wrap.b32 vl, tl, th, %r42;
	shf.l.wrap.b32 vh, th, tl, %r42;
	setp.lt.u32 p, %r42, 32;
	@!p mov.b64 %rd212, {vl,vh};
	@p  mov.b64 %rd212, {vh,vl};
	}

	// end inline asm
	not.b64 	%rd235, %rd168;
	and.b64  	%rd236, %rd180, %rd235;
	xor.b64  	%rd237, %rd236, %rd234;
	not.b64 	%rd238, %rd180;
	and.b64  	%rd239, %rd204, %rd238;
	xor.b64  	%rd331, %rd239, %rd168;
	not.b64 	%rd240, %rd204;
	and.b64  	%rd241, %rd192, %rd240;
	xor.b64  	%rd326, %rd180, %rd241;
	not.b64 	%rd242, %rd192;
	and.b64  	%rd243, %rd234, %rd242;
	xor.b64  	%rd321, %rd204, %rd243;
	not.b64 	%rd244, %rd234;
	and.b64  	%rd245, %rd168, %rd244;
	xor.b64  	%rd316, %rd192, %rd245;
	not.b64 	%rd246, %rd170;
	and.b64  	%rd247, %rd212, %rd246;
	xor.b64  	%rd335, %rd247, %rd202;
	not.b64 	%rd248, %rd212;
	and.b64  	%rd249, %rd198, %rd248;
	xor.b64  	%rd330, %rd249, %rd170;
	not.b64 	%rd250, %rd198;
	and.b64  	%rd251, %rd172, %rd250;
	xor.b64  	%rd325, %rd212, %rd251;
	not.b64 	%rd252, %rd172;
	and.b64  	%rd253, %rd202, %rd252;
	xor.b64  	%rd320, %rd198, %rd253;
	not.b64 	%rd254, %rd202;
	and.b64  	%rd255, %rd170, %rd254;
	xor.b64  	%rd315, %rd172, %rd255;
	not.b64 	%rd256, %rd210;
	and.b64  	%rd257, %rd182, %rd256;
	xor.b64  	%rd334, %rd257, %rd166;
	not.b64 	%rd258, %rd182;
	and.b64  	%rd259, %rd184, %rd258;
	xor.b64  	%rd329, %rd259, %rd210;
	not.b64 	%rd260, %rd184;
	and.b64  	%rd261, %rd176, %rd260;
	xor.b64  	%rd324, %rd182, %rd261;
	not.b64 	%rd262, %rd176;
	and.b64  	%rd263, %rd166, %rd262;
	xor.b64  	%rd319, %rd184, %rd263;
	not.b64 	%rd264, %rd166;
	and.b64  	%rd265, %rd210, %rd264;
	xor.b64  	%rd314, %rd176, %rd265;
	not.b64 	%rd266, %rd200;
	and.b64  	%rd267, %rd208, %rd266;
	xor.b64  	%rd333, %rd267, %rd190;
	not.b64 	%rd268, %rd208;
	and.b64  	%rd269, %rd206, %rd268;
	xor.b64  	%rd328, %rd269, %rd200;
	not.b64 	%rd270, %rd206;
	and.b64  	%rd271, %rd186, %rd270;
	xor.b64  	%rd323, %rd208, %rd271;
	not.b64 	%rd272, %rd186;
	and.b64  	%rd273, %rd190, %rd272;
	xor.b64  	%rd318, %rd206, %rd273;
	not.b64 	%rd274, %rd190;
	and.b64  	%rd275, %rd200, %rd274;
	xor.b64  	%rd313, %rd186, %rd275;
	not.b64 	%rd276, %rd196;
	and.b64  	%rd277, %rd174, %rd276;
	xor.b64  	%rd332, %rd277, %rd178;
	not.b64 	%rd278, %rd174;
	and.b64  	%rd279, %rd188, %rd278;
	xor.b64  	%rd327, %rd279, %rd196;
	not.b64 	%rd280, %rd188;
	and.b64  	%rd281, %rd194, %rd280;
	xor.b64  	%rd322, %rd174, %rd281;
	not.b64 	%rd282, %rd194;
	and.b64  	%rd283, %rd178, %rd282;
	xor.b64  	%rd317, %rd188, %rd283;
	not.b64 	%rd284, %rd178;
	and.b64  	%rd285, %rd196, %rd284;
	xor.b64  	%rd312, %rd194, %rd285;
	ld.const.u64 	%rd286, [%rd311];
	xor.b64  	%rd336, %rd237, %rd286;
	add.s64 	%rd311, %rd311, 8;
	add.s32 	%r85, %r85, 1;
	setp.ne.s32 	%p6, %r85, 24;
	@%p6 bra 	$L__BB1_4;

	shr.u64 	%rd287, %rd336, 16;
	cvt.u32.u64 	%r43, %rd336;
	shr.u64 	%rd288, %rd336, 32;
	shr.u64 	%rd289, %rd336, 40;
	cvt.u32.u64 	%r44, %rd289;
	shr.u64 	%rd290, %rd336, 48;
	shr.u64 	%rd291, %rd336, 56;
	shr.u64 	%rd292, %rd331, 16;
	cvt.u32.u64 	%r45, %rd331;
	shr.u64 	%rd293, %rd331, 32;
	shr.u64 	%rd294, %rd331, 40;
	cvt.u32.u64 	%r46, %rd294;
	shr.u64 	%rd295, %rd331, 48;
	shr.u64 	%rd296, %rd331, 56;
	shr.u64 	%rd297, %rd326, 16;
	cvt.u32.u64 	%r47, %rd326;
	shr.u64 	%rd298, %rd326, 32;
	shr.u64 	%rd299, %rd326, 40;
	cvt.u32.u64 	%r48, %rd299;
	shr.u64 	%rd300, %rd326, 48;
	shr.u64 	%rd301, %rd326, 56;
	shr.u64 	%rd302, %rd321, 56;
	shr.u64 	%rd303, %rd321, 48;
	shr.u64 	%rd304, %rd321, 40;
	cvt.u32.u64 	%r49, %rd304;
	shr.u64 	%rd305, %rd321, 32;
	cvt.u32.u64 	%r50, %rd321;
	shr.u64 	%rd306, %rd321, 16;
	cvt.u16.u64 	%rs33, %rd302;
	cvt.u16.u64 	%rs34, %rd303;
	shl.b16 	%rs35, %rs34, 8;
	or.b16  	%rs36, %rs33, %rs35;
	cvt.u32.u64 	%r51, %rd305;
	and.b32  	%r52, %r49, 255;
	prmt.b32 	%r53, %r51, %r52, 30212;
	cvt.u16.u32 	%rs37, %r53;
	cvt.u16.u64 	%rs38, %rd301;
	cvt.u16.u64 	%rs39, %rd300;
	shl.b16 	%rs40, %rs39, 8;
	or.b16  	%rs41, %rs38, %rs40;
	cvt.u32.u64 	%r54, %rd298;
	and.b32  	%r55, %r48, 255;
	prmt.b32 	%r56, %r54, %r55, 30212;
	cvt.u16.u32 	%rs42, %r56;
	cvt.u16.u64 	%rs43, %rd321;
	shl.b16 	%rs44, %rs43, 8;
	shr.u16 	%rs45, %rs43, 8;
	or.b16  	%rs46, %rs45, %rs44;
	shr.u32 	%r57, %r50, 24;
	cvt.u32.u64 	%r58, %rd306;
	prmt.b32 	%r59, %r58, %r57, 30212;
	cvt.u16.u32 	%rs47, %r59;
	cvt.u16.u64 	%rs48, %rd326;
	shl.b16 	%rs49, %rs48, 8;
	shr.u16 	%rs50, %rs48, 8;
	or.b16  	%rs51, %rs50, %rs49;
	shr.u32 	%r60, %r47, 24;
	cvt.u32.u64 	%r61, %rd297;
	prmt.b32 	%r62, %r61, %r60, 30212;
	cvt.u16.u32 	%rs52, %r62;
	mov.b32 	%r63, {%rs52, %rs51};
	mov.b32 	%r64, {%rs47, %rs46};
	mov.b32 	%r65, {%rs41, %rs42};
	mov.b32 	%r66, {%rs36, %rs37};
	st.local.v4.u32 	[%rd2], {%r66, %r64, %r65, %r63};
	cvt.u16.u64 	%rs53, %rd296;
	cvt.u16.u64 	%rs54, %rd295;
	shl.b16 	%rs55, %rs54, 8;
	or.b16  	%rs56, %rs53, %rs55;
	cvt.u32.u64 	%r67, %rd293;
	and.b32  	%r68, %r46, 255;
	prmt.b32 	%r69, %r67, %r68, 30212;
	cvt.u16.u32 	%rs57, %r69;
	cvt.u16.u64 	%rs58, %rd291;
	cvt.u16.u64 	%rs59, %rd290;
	shl.b16 	%rs60, %rs59, 8;
	or.b16  	%rs61, %rs58, %rs60;
	cvt.u32.u64 	%r70, %rd288;
	and.b32  	%r71, %r44, 255;
	prmt.b32 	%r72, %r70, %r71, 30212;
	cvt.u16.u32 	%rs62, %r72;
	cvt.u16.u64 	%rs63, %rd331;
	shl.b16 	%rs64, %rs63, 8;
	shr.u16 	%rs65, %rs63, 8;
	or.b16  	%rs66, %rs65, %rs64;
	shr.u32 	%r73, %r45, 24;
	cvt.u32.u64 	%r74, %rd292;
	prmt.b32 	%r75, %r74, %r73, 30212;
	cvt.u16.u32 	%rs67, %r75;
	cvt.u16.u64 	%rs68, %rd336;
	shl.b16 	%rs69, %rs68, 8;
	shr.u16 	%rs70, %rs68, 8;
	or.b16  	%rs71, %rs70, %rs69;
	shr.u32 	%r76, %r43, 24;
	cvt.u32.u64 	%r77, %rd287;
	prmt.b32 	%r78, %r77, %r76, 30212;
	cvt.u16.u32 	%rs72, %r78;
	mov.b32 	%r79, {%rs72, %rs71};
	mov.b32 	%r80, {%rs67, %rs66};
	mov.b32 	%r81, {%rs61, %rs62};
	mov.b32 	%r82, {%rs56, %rs57};
	st.local.v4.u32 	[%rd2+16], {%r82, %r80, %r81, %r79};
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd68;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r83, [retval0+0];
	} // callseq 1
	cvt.u16.u32 	%rs73, %r83;
	setp.eq.s16 	%p7, %rs73, 0;
	@%p7 bra 	$L__BB1_8;

	mov.u64 	%rd337, 0;

$L__BB1_7:
	add.s64 	%rd309, %rd3, %rd337;
	ld.u8 	%rs74, [%rd309];
	add.s64 	%rd310, %rd1, %rd337;
	st.global.u8 	[%rd310], %rs74;
	add.s64 	%rd337, %rd337, 1;
	setp.lt.u64 	%p8, %rd337, 32;
	@%p8 bra 	$L__BB1_7;

$L__BB1_8:
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 2
	add.s32 	%r84, %r84, 1;
	setp.lt.u32 	%p9, %r84, %r3;
	@%p9 bra 	$L__BB1_3;

$L__BB1_9:
	ret;

}
	// .globl	kernel_lilypad_pow_debug
.visible .entry kernel_lilypad_pow_debug(
	.param .u64 kernel_lilypad_pow_debug_param_0,
	.param .u64 kernel_lilypad_pow_debug_param_1,
	.param .u64 kernel_lilypad_pow_debug_param_2,
	.param .u32 kernel_lilypad_pow_debug_param_3,
	.param .u32 kernel_lilypad_pow_debug_param_4,
	.param .u64 kernel_lilypad_pow_debug_param_5,
	.param .u64 kernel_lilypad_pow_debug_param_6,
	.param .u64 kernel_lilypad_pow_debug_param_7
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot2[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<110>;
	.reg .b32 	%r<164>;
	.reg .b64 	%rd<428>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd73, [kernel_lilypad_pow_debug_param_0];
	ld.param.u64 	%rd74, [kernel_lilypad_pow_debug_param_1];
	ld.param.u32 	%r8, [kernel_lilypad_pow_debug_param_3];
	ld.param.u32 	%r7, [kernel_lilypad_pow_debug_param_4];
	ld.param.u64 	%rd76, [kernel_lilypad_pow_debug_param_5];
	ld.param.u64 	%rd77, [kernel_lilypad_pow_debug_param_6];
	ld.param.u64 	%rd78, [kernel_lilypad_pow_debug_param_7];
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r10, %r9, %r11;
	setp.ge.u32 	%p1, %r1, %r8;
	@%p1 bra 	$L__BB2_11;

	mul.lo.s32 	%r162, %r1, %r7;
	add.s32 	%r12, %r162, %r7;
	setp.ge.u32 	%p2, %r162, %r12;
	@%p2 bra 	$L__BB2_11;

	cvta.to.global.u64 	%rd96, %rd73;
	cvta.to.global.u64 	%rd98, %rd74;
	cvta.to.global.u64 	%rd67, %rd77;
	cvta.to.global.u64 	%rd339, %rd78;
	cvta.to.global.u64 	%rd70, %rd76;

$L__BB2_3:
	mov.u64 	%rd97, 32;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd97;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd1, [retval0+0];
	} // callseq 3
	mov.u32 	%r163, 0;
	ld.global.nc.u64 	%rd99, [%rd98];
	cvt.s64.s32 	%rd100, %r162;
	add.s64 	%rd2, %rd99, %rd100;
	mov.u64 	%rd401, 0;
	st.u64 	[%rd1], %rd2;
	setp.lt.u64 	%p3, %rd2, %rd99;
	selp.u64 	%rd101, 1, 0, %p3;
	ld.global.nc.u64 	%rd102, [%rd98+8];
	add.s64 	%rd3, %rd102, %rd101;
	st.u64 	[%rd1+8], %rd3;
	setp.lt.u64 	%p4, %rd3, %rd102;
	selp.u64 	%rd103, 1, 0, %p4;
	ld.global.nc.u64 	%rd104, [%rd98+16];
	add.s64 	%rd4, %rd104, %rd103;
	st.u64 	[%rd1+16], %rd4;
	setp.lt.u64 	%p5, %rd4, %rd104;
	selp.u64 	%rd105, 1, 0, %p5;
	ld.global.nc.u64 	%rd106, [%rd98+24];
	add.s64 	%rd414, %rd106, %rd105;
	st.u64 	[%rd1+24], %rd414;
	ld.global.nc.u8 	%rs41, [%rd96];
	cvt.u64.u8 	%rd107, %rs41;
	ld.global.nc.u8 	%rs42, [%rd96+1];
	cvt.u64.u8 	%rd108, %rs42;
	bfi.b64 	%rd109, %rd108, %rd107, 8, 8;
	ld.global.nc.u8 	%rs43, [%rd96+2];
	cvt.u64.u8 	%rd110, %rs43;
	ld.global.nc.u8 	%rs44, [%rd96+3];
	cvt.u64.u8 	%rd111, %rs44;
	bfi.b64 	%rd112, %rd111, %rd110, 8, 8;
	bfi.b64 	%rd113, %rd112, %rd109, 16, 16;
	ld.global.nc.u8 	%rs45, [%rd96+4];
	cvt.u64.u8 	%rd114, %rs45;
	ld.global.nc.u8 	%rs46, [%rd96+5];
	cvt.u64.u8 	%rd115, %rs46;
	bfi.b64 	%rd116, %rd115, %rd114, 8, 8;
	ld.global.nc.u8 	%rs47, [%rd96+6];
	cvt.u64.u8 	%rd117, %rs47;
	ld.global.nc.u8 	%rs48, [%rd96+7];
	cvt.u64.u8 	%rd118, %rs48;
	bfi.b64 	%rd119, %rd118, %rd117, 8, 8;
	bfi.b64 	%rd120, %rd119, %rd116, 16, 16;
	bfi.b64 	%rd10, %rd120, %rd113, 32, 32;
	ld.global.nc.u8 	%rs49, [%rd96+8];
	cvt.u64.u8 	%rd121, %rs49;
	ld.global.nc.u8 	%rs50, [%rd96+9];
	cvt.u64.u8 	%rd122, %rs50;
	bfi.b64 	%rd123, %rd122, %rd121, 8, 8;
	ld.global.nc.u8 	%rs51, [%rd96+10];
	cvt.u64.u8 	%rd124, %rs51;
	ld.global.nc.u8 	%rs52, [%rd96+11];
	cvt.u64.u8 	%rd125, %rs52;
	bfi.b64 	%rd126, %rd125, %rd124, 8, 8;
	bfi.b64 	%rd127, %rd126, %rd123, 16, 16;
	ld.global.nc.u8 	%rs53, [%rd96+12];
	cvt.u64.u8 	%rd128, %rs53;
	ld.global.nc.u8 	%rs54, [%rd96+13];
	cvt.u64.u8 	%rd129, %rs54;
	bfi.b64 	%rd130, %rd129, %rd128, 8, 8;
	ld.global.nc.u8 	%rs55, [%rd96+14];
	cvt.u64.u8 	%rd131, %rs55;
	ld.global.nc.u8 	%rs56, [%rd96+15];
	cvt.u64.u8 	%rd132, %rs56;
	bfi.b64 	%rd133, %rd132, %rd131, 8, 8;
	bfi.b64 	%rd134, %rd133, %rd130, 16, 16;
	bfi.b64 	%rd12, %rd134, %rd127, 32, 32;
	ld.global.nc.u8 	%rs57, [%rd96+16];
	cvt.u64.u8 	%rd135, %rs57;
	ld.global.nc.u8 	%rs58, [%rd96+17];
	cvt.u64.u8 	%rd136, %rs58;
	bfi.b64 	%rd137, %rd136, %rd135, 8, 8;
	ld.global.nc.u8 	%rs59, [%rd96+18];
	cvt.u64.u8 	%rd138, %rs59;
	ld.global.nc.u8 	%rs60, [%rd96+19];
	cvt.u64.u8 	%rd139, %rs60;
	bfi.b64 	%rd140, %rd139, %rd138, 8, 8;
	bfi.b64 	%rd141, %rd140, %rd137, 16, 16;
	ld.global.nc.u8 	%rs61, [%rd96+20];
	cvt.u64.u8 	%rd142, %rs61;
	ld.global.nc.u8 	%rs62, [%rd96+21];
	cvt.u64.u8 	%rd143, %rs62;
	bfi.b64 	%rd144, %rd143, %rd142, 8, 8;
	ld.global.nc.u8 	%rs63, [%rd96+22];
	cvt.u64.u8 	%rd145, %rs63;
	ld.global.nc.u8 	%rs64, [%rd96+23];
	cvt.u64.u8 	%rd146, %rs64;
	bfi.b64 	%rd147, %rd146, %rd145, 8, 8;
	bfi.b64 	%rd148, %rd147, %rd144, 16, 16;
	bfi.b64 	%rd14, %rd148, %rd141, 32, 32;
	ld.global.nc.u8 	%rs65, [%rd96+24];
	cvt.u64.u8 	%rd149, %rs65;
	ld.global.nc.u8 	%rs66, [%rd96+25];
	cvt.u64.u8 	%rd150, %rs66;
	bfi.b64 	%rd151, %rd150, %rd149, 8, 8;
	ld.global.nc.u8 	%rs67, [%rd96+26];
	cvt.u64.u8 	%rd152, %rs67;
	ld.global.nc.u8 	%rs68, [%rd96+27];
	cvt.u64.u8 	%rd153, %rs68;
	bfi.b64 	%rd154, %rd153, %rd152, 8, 8;
	bfi.b64 	%rd155, %rd154, %rd151, 16, 16;
	ld.global.nc.u8 	%rs69, [%rd96+28];
	cvt.u64.u8 	%rd156, %rs69;
	ld.global.nc.u8 	%rs70, [%rd96+29];
	cvt.u64.u8 	%rd157, %rs70;
	bfi.b64 	%rd158, %rd157, %rd156, 8, 8;
	ld.global.nc.u8 	%rs71, [%rd96+30];
	cvt.u64.u8 	%rd159, %rs71;
	ld.global.nc.u8 	%rs72, [%rd96+31];
	cvt.u64.u8 	%rd160, %rs72;
	bfi.b64 	%rd161, %rd160, %rd159, 8, 8;
	bfi.b64 	%rd162, %rd161, %rd158, 16, 16;
	bfi.b64 	%rd9, %rd162, %rd155, 32, 32;
	cvt.u16.u64 	%rs1, %rd414;
	shr.u64 	%rd163, %rd414, 8;
	cvt.u16.u64 	%rs2, %rd163;
	shr.u64 	%rd164, %rd414, 16;
	cvt.u16.u64 	%rs3, %rd164;
	shr.u64 	%rd165, %rd414, 24;
	cvt.u16.u64 	%rs4, %rd165;
	shr.u64 	%rd166, %rd414, 32;
	cvt.u16.u64 	%rs5, %rd166;
	shr.u64 	%rd167, %rd414, 40;
	cvt.u16.u64 	%rs6, %rd167;
	shr.u64 	%rd168, %rd414, 48;
	cvt.u16.u64 	%rs7, %rd168;
	shr.u64 	%rd169, %rd414, 56;
	cvt.u16.u64 	%rs8, %rd169;
	mov.u64 	%rd417, -9223372036854775808;
	mov.u64 	%rd409, 1;
	mov.u64 	%rd402, %rd401;
	mov.u64 	%rd403, %rd401;
	mov.u64 	%rd404, %rd401;
	mov.u64 	%rd405, %rd2;
	mov.u64 	%rd406, %rd401;
	mov.u64 	%rd407, %rd401;
	mov.u64 	%rd408, %rd401;
	mov.u64 	%rd410, %rd9;
	mov.u64 	%rd411, %rd401;
	mov.u64 	%rd412, %rd401;
	mov.u64 	%rd413, %rd401;
	mov.u64 	%rd415, %rd14;
	mov.u64 	%rd416, %rd401;
	mov.u64 	%rd418, %rd401;
	mov.u64 	%rd419, %rd4;
	mov.u64 	%rd420, %rd12;
	mov.u64 	%rd421, %rd401;
	mov.u64 	%rd422, %rd401;
	mov.u64 	%rd423, %rd401;
	mov.u64 	%rd424, %rd3;
	mov.u64 	%rd425, %rd10;

$L__BB2_4:
	xor.b64  	%rd228, %rd424, %rd425;
	xor.b64  	%rd229, %rd228, %rd423;
	xor.b64  	%rd230, %rd229, %rd422;
	xor.b64  	%rd179, %rd230, %rd421;
	xor.b64  	%rd231, %rd419, %rd420;
	xor.b64  	%rd232, %rd231, %rd418;
	xor.b64  	%rd233, %rd232, %rd417;
	xor.b64  	%rd171, %rd233, %rd416;
	xor.b64  	%rd234, %rd414, %rd415;
	xor.b64  	%rd235, %rd234, %rd413;
	xor.b64  	%rd236, %rd235, %rd412;
	xor.b64  	%rd173, %rd236, %rd411;
	xor.b64  	%rd237, %rd409, %rd410;
	xor.b64  	%rd238, %rd237, %rd408;
	xor.b64  	%rd239, %rd238, %rd407;
	xor.b64  	%rd175, %rd239, %rd406;
	xor.b64  	%rd240, %rd404, %rd405;
	xor.b64  	%rd241, %rd240, %rd403;
	xor.b64  	%rd242, %rd241, %rd402;
	xor.b64  	%rd177, %rd242, %rd401;
	mov.u32 	%r19, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd171;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd170, {vl,vh};
	@p  mov.b64 %rd170, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd243, %rd170, %rd177;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd173;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd172, {vl,vh};
	@p  mov.b64 %rd172, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd244, %rd172, %rd179;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd175;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd174, {vl,vh};
	@p  mov.b64 %rd174, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd245, %rd174, %rd171;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd177;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd176, {vl,vh};
	@p  mov.b64 %rd176, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd246, %rd176, %rd173;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd179;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd178, {vl,vh};
	@p  mov.b64 %rd178, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd247, %rd178, %rd175;
	xor.b64  	%rd248, %rd425, %rd243;
	xor.b64  	%rd215, %rd424, %rd243;
	xor.b64  	%rd227, %rd423, %rd243;
	xor.b64  	%rd203, %rd422, %rd243;
	xor.b64  	%rd191, %rd421, %rd243;
	xor.b64  	%rd181, %rd420, %rd244;
	xor.b64  	%rd183, %rd419, %rd244;
	xor.b64  	%rd223, %rd418, %rd244;
	xor.b64  	%rd213, %rd417, %rd244;
	xor.b64  	%rd209, %rd416, %rd244;
	xor.b64  	%rd193, %rd415, %rd245;
	xor.b64  	%rd225, %rd414, %rd245;
	xor.b64  	%rd195, %rd413, %rd245;
	xor.b64  	%rd221, %rd412, %rd245;
	xor.b64  	%rd187, %rd411, %rd245;
	xor.b64  	%rd217, %rd410, %rd246;
	xor.b64  	%rd211, %rd409, %rd246;
	xor.b64  	%rd197, %rd408, %rd246;
	xor.b64  	%rd219, %rd407, %rd246;
	xor.b64  	%rd201, %rd406, %rd246;
	xor.b64  	%rd205, %rd405, %rd247;
	xor.b64  	%rd185, %rd404, %rd247;
	xor.b64  	%rd189, %rd403, %rd247;
	xor.b64  	%rd199, %rd402, %rd247;
	xor.b64  	%rd207, %rd401, %rd247;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd181;
	shf.l.wrap.b32 vl, tl, th, %r19;
	shf.l.wrap.b32 vh, th, tl, %r19;
	setp.lt.u32 p, %r19, 32;
	@!p mov.b64 %rd180, {vl,vh};
	@p  mov.b64 %rd180, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r20, 44;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd183;
	shf.l.wrap.b32 vl, tl, th, %r20;
	shf.l.wrap.b32 vh, th, tl, %r20;
	setp.lt.u32 p, %r20, 32;
	@!p mov.b64 %rd182, {vl,vh};
	@p  mov.b64 %rd182, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r21, 20;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd185;
	shf.l.wrap.b32 vl, tl, th, %r21;
	shf.l.wrap.b32 vh, th, tl, %r21;
	setp.lt.u32 p, %r21, 32;
	@!p mov.b64 %rd184, {vl,vh};
	@p  mov.b64 %rd184, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r22, 61;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd187;
	shf.l.wrap.b32 vl, tl, th, %r22;
	shf.l.wrap.b32 vh, th, tl, %r22;
	setp.lt.u32 p, %r22, 32;
	@!p mov.b64 %rd186, {vl,vh};
	@p  mov.b64 %rd186, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r23, 39;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd189;
	shf.l.wrap.b32 vl, tl, th, %r23;
	shf.l.wrap.b32 vh, th, tl, %r23;
	setp.lt.u32 p, %r23, 32;
	@!p mov.b64 %rd188, {vl,vh};
	@p  mov.b64 %rd188, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r24, 18;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd191;
	shf.l.wrap.b32 vl, tl, th, %r24;
	shf.l.wrap.b32 vh, th, tl, %r24;
	setp.lt.u32 p, %r24, 32;
	@!p mov.b64 %rd190, {vl,vh};
	@p  mov.b64 %rd190, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r25, 62;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd193;
	shf.l.wrap.b32 vl, tl, th, %r25;
	shf.l.wrap.b32 vh, th, tl, %r25;
	setp.lt.u32 p, %r25, 32;
	@!p mov.b64 %rd192, {vl,vh};
	@p  mov.b64 %rd192, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r26, 43;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd195;
	shf.l.wrap.b32 vl, tl, th, %r26;
	shf.l.wrap.b32 vh, th, tl, %r26;
	setp.lt.u32 p, %r26, 32;
	@!p mov.b64 %rd194, {vl,vh};
	@p  mov.b64 %rd194, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r27, 25;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd197;
	shf.l.wrap.b32 vl, tl, th, %r27;
	shf.l.wrap.b32 vh, th, tl, %r27;
	setp.lt.u32 p, %r27, 32;
	@!p mov.b64 %rd196, {vl,vh};
	@p  mov.b64 %rd196, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r28, 8;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd199;
	shf.l.wrap.b32 vl, tl, th, %r28;
	shf.l.wrap.b32 vh, th, tl, %r28;
	setp.lt.u32 p, %r28, 32;
	@!p mov.b64 %rd198, {vl,vh};
	@p  mov.b64 %rd198, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r29, 56;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd201;
	shf.l.wrap.b32 vl, tl, th, %r29;
	shf.l.wrap.b32 vh, th, tl, %r29;
	setp.lt.u32 p, %r29, 32;
	@!p mov.b64 %rd200, {vl,vh};
	@p  mov.b64 %rd200, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r30, 41;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd203;
	shf.l.wrap.b32 vl, tl, th, %r30;
	shf.l.wrap.b32 vh, th, tl, %r30;
	setp.lt.u32 p, %r30, 32;
	@!p mov.b64 %rd202, {vl,vh};
	@p  mov.b64 %rd202, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r31, 27;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd205;
	shf.l.wrap.b32 vl, tl, th, %r31;
	shf.l.wrap.b32 vh, th, tl, %r31;
	setp.lt.u32 p, %r31, 32;
	@!p mov.b64 %rd204, {vl,vh};
	@p  mov.b64 %rd204, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r32, 14;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd207;
	shf.l.wrap.b32 vl, tl, th, %r32;
	shf.l.wrap.b32 vh, th, tl, %r32;
	setp.lt.u32 p, %r32, 32;
	@!p mov.b64 %rd206, {vl,vh};
	@p  mov.b64 %rd206, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r33, 2;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd209;
	shf.l.wrap.b32 vl, tl, th, %r33;
	shf.l.wrap.b32 vh, th, tl, %r33;
	setp.lt.u32 p, %r33, 32;
	@!p mov.b64 %rd208, {vl,vh};
	@p  mov.b64 %rd208, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r34, 55;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd211;
	shf.l.wrap.b32 vl, tl, th, %r34;
	shf.l.wrap.b32 vh, th, tl, %r34;
	setp.lt.u32 p, %r34, 32;
	@!p mov.b64 %rd210, {vl,vh};
	@p  mov.b64 %rd210, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r35, 45;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd213;
	shf.l.wrap.b32 vl, tl, th, %r35;
	shf.l.wrap.b32 vh, th, tl, %r35;
	setp.lt.u32 p, %r35, 32;
	@!p mov.b64 %rd212, {vl,vh};
	@p  mov.b64 %rd212, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r36, 36;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd215;
	shf.l.wrap.b32 vl, tl, th, %r36;
	shf.l.wrap.b32 vh, th, tl, %r36;
	setp.lt.u32 p, %r36, 32;
	@!p mov.b64 %rd214, {vl,vh};
	@p  mov.b64 %rd214, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r37, 28;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd217;
	shf.l.wrap.b32 vl, tl, th, %r37;
	shf.l.wrap.b32 vh, th, tl, %r37;
	setp.lt.u32 p, %r37, 32;
	@!p mov.b64 %rd216, {vl,vh};
	@p  mov.b64 %rd216, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r38, 21;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd219;
	shf.l.wrap.b32 vl, tl, th, %r38;
	shf.l.wrap.b32 vh, th, tl, %r38;
	setp.lt.u32 p, %r38, 32;
	@!p mov.b64 %rd218, {vl,vh};
	@p  mov.b64 %rd218, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r39, 15;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd221;
	shf.l.wrap.b32 vl, tl, th, %r39;
	shf.l.wrap.b32 vh, th, tl, %r39;
	setp.lt.u32 p, %r39, 32;
	@!p mov.b64 %rd220, {vl,vh};
	@p  mov.b64 %rd220, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r40, 10;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd223;
	shf.l.wrap.b32 vl, tl, th, %r40;
	shf.l.wrap.b32 vh, th, tl, %r40;
	setp.lt.u32 p, %r40, 32;
	@!p mov.b64 %rd222, {vl,vh};
	@p  mov.b64 %rd222, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r41, 6;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd225;
	shf.l.wrap.b32 vl, tl, th, %r41;
	shf.l.wrap.b32 vh, th, tl, %r41;
	setp.lt.u32 p, %r41, 32;
	@!p mov.b64 %rd224, {vl,vh};
	@p  mov.b64 %rd224, {vh,vl};
	}

	// end inline asm
	mov.u32 	%r42, 3;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd227;
	shf.l.wrap.b32 vl, tl, th, %r42;
	shf.l.wrap.b32 vh, th, tl, %r42;
	setp.lt.u32 p, %r42, 32;
	@!p mov.b64 %rd226, {vl,vh};
	@p  mov.b64 %rd226, {vh,vl};
	}

	// end inline asm
	not.b64 	%rd249, %rd182;
	and.b64  	%rd250, %rd194, %rd249;
	xor.b64  	%rd251, %rd250, %rd248;
	not.b64 	%rd252, %rd194;
	and.b64  	%rd253, %rd218, %rd252;
	xor.b64  	%rd420, %rd253, %rd182;
	not.b64 	%rd254, %rd218;
	and.b64  	%rd255, %rd206, %rd254;
	xor.b64  	%rd415, %rd194, %rd255;
	not.b64 	%rd256, %rd206;
	and.b64  	%rd257, %rd248, %rd256;
	xor.b64  	%rd410, %rd218, %rd257;
	not.b64 	%rd258, %rd248;
	and.b64  	%rd259, %rd182, %rd258;
	xor.b64  	%rd405, %rd206, %rd259;
	not.b64 	%rd260, %rd184;
	and.b64  	%rd261, %rd226, %rd260;
	xor.b64  	%rd424, %rd261, %rd216;
	not.b64 	%rd262, %rd226;
	and.b64  	%rd263, %rd212, %rd262;
	xor.b64  	%rd419, %rd263, %rd184;
	not.b64 	%rd264, %rd212;
	and.b64  	%rd265, %rd186, %rd264;
	xor.b64  	%rd414, %rd226, %rd265;
	not.b64 	%rd266, %rd186;
	and.b64  	%rd267, %rd216, %rd266;
	xor.b64  	%rd409, %rd212, %rd267;
	not.b64 	%rd268, %rd216;
	and.b64  	%rd269, %rd184, %rd268;
	xor.b64  	%rd404, %rd186, %rd269;
	not.b64 	%rd270, %rd224;
	and.b64  	%rd271, %rd196, %rd270;
	xor.b64  	%rd423, %rd271, %rd180;
	not.b64 	%rd272, %rd196;
	and.b64  	%rd273, %rd198, %rd272;
	xor.b64  	%rd418, %rd273, %rd224;
	not.b64 	%rd274, %rd198;
	and.b64  	%rd275, %rd190, %rd274;
	xor.b64  	%rd413, %rd196, %rd275;
	not.b64 	%rd276, %rd190;
	and.b64  	%rd277, %rd180, %rd276;
	xor.b64  	%rd408, %rd198, %rd277;
	not.b64 	%rd278, %rd180;
	and.b64  	%rd279, %rd224, %rd278;
	xor.b64  	%rd403, %rd190, %rd279;
	not.b64 	%rd280, %rd214;
	and.b64  	%rd281, %rd222, %rd280;
	xor.b64  	%rd422, %rd281, %rd204;
	not.b64 	%rd282, %rd222;
	and.b64  	%rd283, %rd220, %rd282;
	xor.b64  	%rd417, %rd283, %rd214;
	not.b64 	%rd284, %rd220;
	and.b64  	%rd285, %rd200, %rd284;
	xor.b64  	%rd412, %rd222, %rd285;
	not.b64 	%rd286, %rd200;
	and.b64  	%rd287, %rd204, %rd286;
	xor.b64  	%rd407, %rd220, %rd287;
	not.b64 	%rd288, %rd204;
	and.b64  	%rd289, %rd214, %rd288;
	xor.b64  	%rd402, %rd200, %rd289;
	not.b64 	%rd290, %rd210;
	and.b64  	%rd291, %rd188, %rd290;
	xor.b64  	%rd421, %rd291, %rd192;
	not.b64 	%rd292, %rd188;
	and.b64  	%rd293, %rd202, %rd292;
	xor.b64  	%rd416, %rd293, %rd210;
	not.b64 	%rd294, %rd202;
	and.b64  	%rd295, %rd208, %rd294;
	xor.b64  	%rd411, %rd188, %rd295;
	not.b64 	%rd296, %rd208;
	and.b64  	%rd297, %rd192, %rd296;
	xor.b64  	%rd406, %rd202, %rd297;
	not.b64 	%rd298, %rd192;
	and.b64  	%rd299, %rd210, %rd298;
	xor.b64  	%rd401, %rd208, %rd299;
	mul.wide.s32 	%rd300, %r163, 8;
	mov.u64 	%rd301, CUDA_KECCAK_CONSTS;
	add.s64 	%rd302, %rd301, %rd300;
	ld.const.u64 	%rd303, [%rd302];
	xor.b64  	%rd425, %rd251, %rd303;
	add.s32 	%r163, %r163, 1;
	setp.ne.s32 	%p6, %r163, 24;
	@%p6 bra 	$L__BB2_4;

	ld.param.u64 	%rd400, [kernel_lilypad_pow_debug_param_2];
	shr.u64 	%rd304, %rd425, 8;
	cvt.u32.u64 	%r43, %rd304;
	cvt.u16.u64 	%rs9, %rd304;
	shr.u64 	%rd305, %rd425, 16;
	cvt.u16.u64 	%rs10, %rd305;
	shr.u64 	%rd306, %rd425, 24;
	cvt.u32.u64 	%r44, %rd306;
	cvt.u16.u64 	%rs11, %rd306;
	shr.u64 	%rd307, %rd425, 32;
	cvt.u16.u64 	%rs12, %rd307;
	shr.u64 	%rd308, %rd425, 40;
	cvt.u32.u64 	%r45, %rd308;
	cvt.u16.u64 	%rs13, %rd308;
	shr.u64 	%rd309, %rd425, 48;
	cvt.u16.u64 	%rs14, %rd309;
	shr.u64 	%rd310, %rd425, 56;
	cvt.u16.u64 	%rs15, %rd310;
	shr.u64 	%rd311, %rd420, 8;
	cvt.u32.u64 	%r46, %rd311;
	shr.u64 	%rd312, %rd420, 16;
	shr.u64 	%rd313, %rd420, 24;
	cvt.u32.u64 	%r47, %rd313;
	shr.u64 	%rd314, %rd420, 32;
	shr.u64 	%rd315, %rd420, 40;
	cvt.u32.u64 	%r48, %rd315;
	shr.u64 	%rd316, %rd420, 48;
	cvt.u16.u64 	%rs21, %rd316;
	shr.u64 	%rd317, %rd420, 56;
	cvt.u16.u64 	%rs22, %rd317;
	shr.u64 	%rd318, %rd415, 8;
	cvt.u32.u64 	%r49, %rd318;
	cvt.u16.u64 	%rs23, %rd318;
	shr.u64 	%rd319, %rd415, 16;
	cvt.u16.u64 	%rs24, %rd319;
	shr.u64 	%rd320, %rd415, 24;
	cvt.u32.u64 	%r50, %rd320;
	cvt.u16.u64 	%rs25, %rd320;
	shr.u64 	%rd321, %rd415, 32;
	cvt.u16.u64 	%rs26, %rd321;
	shr.u64 	%rd322, %rd415, 40;
	cvt.u32.u64 	%r51, %rd322;
	cvt.u16.u64 	%rs27, %rd322;
	shr.u64 	%rd323, %rd415, 48;
	cvt.u16.u64 	%rs28, %rd323;
	shr.u64 	%rd324, %rd415, 56;
	cvt.u16.u64 	%rs29, %rd324;
	shr.u64 	%rd325, %rd410, 56;
	cvt.u16.u64 	%rs30, %rd325;
	shr.u64 	%rd326, %rd410, 48;
	cvt.u16.u64 	%rs31, %rd326;
	shr.u64 	%rd327, %rd410, 40;
	cvt.u32.u64 	%r52, %rd327;
	cvt.u16.u64 	%rs32, %rd327;
	shr.u64 	%rd328, %rd410, 32;
	cvt.u16.u64 	%rs33, %rd328;
	shr.u64 	%rd329, %rd410, 24;
	cvt.u32.u64 	%r53, %rd329;
	cvt.u16.u64 	%rs34, %rd329;
	shr.u64 	%rd330, %rd410, 16;
	cvt.u16.u64 	%rs35, %rd330;
	shr.u64 	%rd331, %rd410, 8;
	cvt.u32.u64 	%r54, %rd331;
	cvt.u16.u64 	%rs36, %rd331;
	cvt.u16.u64 	%rs37, %rd410;
	cvt.u16.u64 	%rs38, %rd415;
	shl.b16 	%rs73, %rs31, 8;
	or.b16  	%rs74, %rs30, %rs73;
	cvt.u32.u64 	%r55, %rd328;
	and.b32  	%r56, %r52, 255;
	prmt.b32 	%r57, %r55, %r56, 30212;
	cvt.u16.u32 	%rs75, %r57;
	shl.b16 	%rs76, %rs28, 8;
	or.b16  	%rs77, %rs29, %rs76;
	cvt.u32.u64 	%r58, %rd321;
	and.b32  	%r59, %r51, 255;
	prmt.b32 	%r60, %r58, %r59, 30212;
	cvt.u16.u32 	%rs78, %r60;
	cvt.u32.u64 	%r61, %rd330;
	and.b32  	%r62, %r53, 255;
	prmt.b32 	%r63, %r61, %r62, 30212;
	cvt.u32.u64 	%r64, %rd410;
	and.b32  	%r65, %r54, 255;
	prmt.b32 	%r66, %r64, %r65, 30212;
	cvt.u32.u64 	%r67, %rd319;
	and.b32  	%r68, %r50, 255;
	prmt.b32 	%r69, %r67, %r68, 30212;
	cvt.u32.u64 	%r70, %rd415;
	and.b32  	%r71, %r49, 255;
	prmt.b32 	%r72, %r70, %r71, 30212;
	add.u64 	%rd332, %SP, 0;
	add.u64 	%rd333, %SPL, 0;
	prmt.b32 	%r73, %r72, %r69, 4180;
	prmt.b32 	%r74, %r66, %r63, 4180;
	mov.b32 	%r75, {%rs77, %rs78};
	mov.b32 	%r76, {%rs74, %rs75};
	st.local.v4.u32 	[%rd333], {%r76, %r74, %r75, %r73};
	cvt.u16.u64 	%rs40, %rd425;
	shl.b16 	%rs79, %rs21, 8;
	or.b16  	%rs80, %rs22, %rs79;
	cvt.u32.u64 	%r77, %rd314;
	and.b32  	%r78, %r48, 255;
	prmt.b32 	%r79, %r77, %r78, 30212;
	cvt.u16.u32 	%rs81, %r79;
	shl.b16 	%rs82, %rs14, 8;
	or.b16  	%rs83, %rs15, %rs82;
	cvt.u32.u64 	%r80, %rd307;
	and.b32  	%r81, %r45, 255;
	prmt.b32 	%r82, %r80, %r81, 30212;
	cvt.u16.u32 	%rs84, %r82;
	cvt.u32.u64 	%r83, %rd312;
	and.b32  	%r84, %r47, 255;
	prmt.b32 	%r85, %r83, %r84, 30212;
	cvt.u32.u64 	%r86, %rd420;
	and.b32  	%r87, %r46, 255;
	prmt.b32 	%r88, %r86, %r87, 30212;
	cvt.u32.u64 	%r89, %rd305;
	and.b32  	%r90, %r44, 255;
	prmt.b32 	%r91, %r89, %r90, 30212;
	cvt.u32.u64 	%r92, %rd425;
	and.b32  	%r93, %r43, 255;
	prmt.b32 	%r94, %r92, %r93, 30212;
	prmt.b32 	%r95, %r94, %r91, 4180;
	prmt.b32 	%r96, %r88, %r85, 4180;
	mov.b32 	%r97, {%rs83, %rs84};
	mov.b32 	%r98, {%rs80, %rs81};
	st.local.v4.u32 	[%rd333+16], {%r98, %r96, %r97, %r95};
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd332;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd400;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r99, [retval0+0];
	} // callseq 4
	cvt.u16.u32 	%rs85, %r99;
	setp.eq.s16 	%p7, %rs85, 0;
	@%p7 bra 	$L__BB2_10;

	shr.u64 	%rd397, %rd420, 32;
	cvt.u16.u64 	%rs109, %rd397;
	shr.u64 	%rd396, %rd420, 40;
	cvt.u16.u64 	%rs108, %rd396;
	shr.u64 	%rd395, %rd420, 16;
	cvt.u16.u64 	%rs107, %rd395;
	shr.u64 	%rd394, %rd420, 24;
	cvt.u16.u64 	%rs106, %rd394;
	cvt.u16.u64 	%rs105, %rd420;
	shr.u64 	%rd393, %rd420, 8;
	cvt.u16.u64 	%rs104, %rd393;
	add.u64 	%rd391, %SPL, 0;
	mov.u64 	%rd426, 0;
	cvt.u32.u16 	%r100, %rs9;
	and.b16  	%rs86, %rs40, 255;
	cvt.u32.u16 	%r101, %rs86;
	prmt.b32 	%r102, %r100, %r101, 30212;
	cvt.u32.u16 	%r103, %rs11;
	and.b16  	%rs87, %rs10, 255;
	cvt.u32.u16 	%r104, %rs87;
	prmt.b32 	%r105, %r103, %r104, 30212;
	cvt.u32.u16 	%r106, %rs13;
	and.b16  	%rs88, %rs12, 255;
	cvt.u32.u16 	%r107, %rs88;
	prmt.b32 	%r108, %r106, %r107, 30212;
	cvt.u32.u16 	%r109, %rs15;
	and.b16  	%rs89, %rs14, 255;
	cvt.u32.u16 	%r110, %rs89;
	prmt.b32 	%r111, %r109, %r110, 30212;
	cvt.u32.u16 	%r112, %rs104;
	and.b16  	%rs90, %rs105, 255;
	cvt.u32.u16 	%r113, %rs90;
	prmt.b32 	%r114, %r112, %r113, 30212;
	cvt.u32.u16 	%r115, %rs106;
	and.b16  	%rs91, %rs107, 255;
	cvt.u32.u16 	%r116, %rs91;
	prmt.b32 	%r117, %r115, %r116, 30212;
	cvt.u32.u16 	%r118, %rs108;
	and.b16  	%rs92, %rs109, 255;
	cvt.u32.u16 	%r119, %rs92;
	prmt.b32 	%r120, %r118, %r119, 30212;
	cvt.u32.u16 	%r121, %rs22;
	and.b16  	%rs93, %rs21, 255;
	cvt.u32.u16 	%r122, %rs93;
	prmt.b32 	%r123, %r121, %r122, 30212;
	prmt.b32 	%r124, %r123, %r120, 4180;
	prmt.b32 	%r125, %r117, %r114, 4180;
	prmt.b32 	%r126, %r111, %r108, 4180;
	prmt.b32 	%r127, %r105, %r102, 4180;
	st.local.v4.u32 	[%rd391], {%r127, %r126, %r125, %r124};
	cvt.u32.u16 	%r128, %rs23;
	and.b16  	%rs94, %rs38, 255;
	cvt.u32.u16 	%r129, %rs94;
	prmt.b32 	%r130, %r128, %r129, 30212;
	cvt.u32.u16 	%r131, %rs25;
	and.b16  	%rs95, %rs24, 255;
	cvt.u32.u16 	%r132, %rs95;
	prmt.b32 	%r133, %r131, %r132, 30212;
	cvt.u32.u16 	%r134, %rs27;
	and.b16  	%rs96, %rs26, 255;
	cvt.u32.u16 	%r135, %rs96;
	prmt.b32 	%r136, %r134, %r135, 30212;
	cvt.u32.u16 	%r137, %rs29;
	and.b16  	%rs97, %rs28, 255;
	cvt.u32.u16 	%r138, %rs97;
	prmt.b32 	%r139, %r137, %r138, 30212;
	cvt.u32.u16 	%r140, %rs36;
	and.b16  	%rs98, %rs37, 255;
	cvt.u32.u16 	%r141, %rs98;
	prmt.b32 	%r142, %r140, %r141, 30212;
	cvt.u32.u16 	%r143, %rs34;
	and.b16  	%rs99, %rs35, 255;
	cvt.u32.u16 	%r144, %rs99;
	prmt.b32 	%r145, %r143, %r144, 30212;
	cvt.u32.u16 	%r146, %rs32;
	and.b16  	%rs100, %rs33, 255;
	cvt.u32.u16 	%r147, %rs100;
	prmt.b32 	%r148, %r146, %r147, 30212;
	cvt.u32.u16 	%r149, %rs30;
	and.b16  	%rs101, %rs31, 255;
	cvt.u32.u16 	%r150, %rs101;
	prmt.b32 	%r151, %r149, %r150, 30212;
	prmt.b32 	%r152, %r151, %r148, 4180;
	prmt.b32 	%r153, %r145, %r142, 4180;
	prmt.b32 	%r154, %r139, %r136, 4180;
	prmt.b32 	%r155, %r133, %r130, 4180;
	st.local.v4.u32 	[%rd391+16], {%r155, %r154, %r153, %r152};

$L__BB2_7:
	add.u64 	%rd398, %SPL, 0;
	add.s64 	%rd336, %rd398, %rd426;
	ld.local.u8 	%rs102, [%rd336];
	add.s64 	%rd337, %rd67, %rd426;
	st.global.u8 	[%rd337], %rs102;
	add.s64 	%rd426, %rd426, 1;
	setp.lt.u64 	%p8, %rd426, 32;
	@%p8 bra 	$L__BB2_7;

	mov.u64 	%rd427, 0;
	st.global.u8 	[%rd339], %rd10;
	shr.u64 	%rd340, %rd10, 8;
	st.global.u8 	[%rd339+1], %rd340;
	shr.u64 	%rd341, %rd10, 16;
	st.global.u8 	[%rd339+2], %rd341;
	shr.u64 	%rd342, %rd10, 24;
	st.global.u8 	[%rd339+3], %rd342;
	shr.u64 	%rd343, %rd10, 32;
	st.global.u8 	[%rd339+4], %rd343;
	shr.u64 	%rd344, %rd10, 40;
	st.global.u8 	[%rd339+5], %rd344;
	shr.u64 	%rd345, %rd10, 48;
	st.global.u8 	[%rd339+6], %rd345;
	shr.u64 	%rd346, %rd10, 56;
	st.global.u8 	[%rd339+7], %rd346;
	st.global.u8 	[%rd339+8], %rd12;
	shr.u64 	%rd347, %rd12, 8;
	st.global.u8 	[%rd339+9], %rd347;
	shr.u64 	%rd348, %rd12, 16;
	st.global.u8 	[%rd339+10], %rd348;
	shr.u64 	%rd349, %rd12, 24;
	st.global.u8 	[%rd339+11], %rd349;
	shr.u64 	%rd350, %rd12, 32;
	st.global.u8 	[%rd339+12], %rd350;
	shr.u64 	%rd351, %rd12, 40;
	st.global.u8 	[%rd339+13], %rd351;
	shr.u64 	%rd352, %rd12, 48;
	st.global.u8 	[%rd339+14], %rd352;
	shr.u64 	%rd353, %rd12, 56;
	st.global.u8 	[%rd339+15], %rd353;
	st.global.u8 	[%rd339+16], %rd14;
	shr.u64 	%rd354, %rd14, 8;
	st.global.u8 	[%rd339+17], %rd354;
	shr.u64 	%rd355, %rd14, 16;
	st.global.u8 	[%rd339+18], %rd355;
	shr.u64 	%rd356, %rd14, 24;
	st.global.u8 	[%rd339+19], %rd356;
	shr.u64 	%rd357, %rd14, 32;
	st.global.u8 	[%rd339+20], %rd357;
	shr.u64 	%rd358, %rd14, 40;
	st.global.u8 	[%rd339+21], %rd358;
	shr.u64 	%rd359, %rd14, 48;
	st.global.u8 	[%rd339+22], %rd359;
	shr.u64 	%rd360, %rd14, 56;
	st.global.u8 	[%rd339+23], %rd360;
	st.global.u8 	[%rd339+24], %rd9;
	shr.u64 	%rd361, %rd9, 8;
	st.global.u8 	[%rd339+25], %rd361;
	shr.u64 	%rd362, %rd9, 16;
	st.global.u8 	[%rd339+26], %rd362;
	shr.u64 	%rd363, %rd9, 24;
	st.global.u8 	[%rd339+27], %rd363;
	shr.u64 	%rd364, %rd9, 32;
	st.global.u8 	[%rd339+28], %rd364;
	shr.u64 	%rd365, %rd9, 40;
	st.global.u8 	[%rd339+29], %rd365;
	shr.u64 	%rd366, %rd9, 48;
	st.global.u8 	[%rd339+30], %rd366;
	shr.u64 	%rd367, %rd9, 56;
	st.global.u8 	[%rd339+31], %rd367;
	st.global.u8 	[%rd339+32], %rd2;
	shr.u64 	%rd368, %rd2, 8;
	st.global.u8 	[%rd339+33], %rd368;
	shr.u64 	%rd369, %rd2, 16;
	st.global.u8 	[%rd339+34], %rd369;
	shr.u64 	%rd370, %rd2, 24;
	st.global.u8 	[%rd339+35], %rd370;
	shr.u64 	%rd371, %rd2, 32;
	st.global.u8 	[%rd339+36], %rd371;
	shr.u64 	%rd372, %rd2, 40;
	st.global.u8 	[%rd339+37], %rd372;
	shr.u64 	%rd373, %rd2, 48;
	st.global.u8 	[%rd339+38], %rd373;
	shr.u64 	%rd374, %rd2, 56;
	st.global.u8 	[%rd339+39], %rd374;
	st.global.u8 	[%rd339+40], %rd3;
	shr.u64 	%rd375, %rd3, 8;
	st.global.u8 	[%rd339+41], %rd375;
	shr.u64 	%rd376, %rd3, 16;
	st.global.u8 	[%rd339+42], %rd376;
	shr.u64 	%rd377, %rd3, 24;
	st.global.u8 	[%rd339+43], %rd377;
	shr.u64 	%rd378, %rd3, 32;
	st.global.u8 	[%rd339+44], %rd378;
	shr.u64 	%rd379, %rd3, 40;
	st.global.u8 	[%rd339+45], %rd379;
	shr.u64 	%rd380, %rd3, 48;
	st.global.u8 	[%rd339+46], %rd380;
	shr.u64 	%rd381, %rd3, 56;
	st.global.u8 	[%rd339+47], %rd381;
	st.global.u8 	[%rd339+48], %rd4;
	shr.u64 	%rd382, %rd4, 8;
	st.global.u8 	[%rd339+49], %rd382;
	shr.u64 	%rd383, %rd4, 16;
	st.global.u8 	[%rd339+50], %rd383;
	shr.u64 	%rd384, %rd4, 24;
	st.global.u8 	[%rd339+51], %rd384;
	shr.u64 	%rd385, %rd4, 32;
	st.global.u8 	[%rd339+52], %rd385;
	shr.u64 	%rd386, %rd4, 40;
	st.global.u8 	[%rd339+53], %rd386;
	shr.u64 	%rd387, %rd4, 48;
	st.global.u8 	[%rd339+54], %rd387;
	shr.u64 	%rd388, %rd4, 56;
	st.global.u8 	[%rd339+55], %rd388;
	st.global.u8 	[%rd339+56], %rs1;
	st.global.u8 	[%rd339+57], %rs2;
	st.global.u8 	[%rd339+58], %rs3;
	st.global.u8 	[%rd339+59], %rs4;
	st.global.u8 	[%rd339+60], %rs5;
	st.global.u8 	[%rd339+61], %rs6;
	st.global.u8 	[%rd339+62], %rs7;
	st.global.u8 	[%rd339+63], %rs8;

$L__BB2_9:
	add.s64 	%rd389, %rd1, %rd427;
	ld.u8 	%rs103, [%rd389];
	add.s64 	%rd390, %rd70, %rd427;
	st.global.u8 	[%rd390], %rs103;
	add.s64 	%rd427, %rd427, 1;
	setp.lt.u64 	%p9, %rd427, 32;
	@%p9 bra 	$L__BB2_9;

$L__BB2_10:
	ld.param.u32 	%r161, [kernel_lilypad_pow_debug_param_4];
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 5
	mad.lo.s32 	%r160, %r1, %r161, %r161;
	add.s32 	%r162, %r162, 1;
	setp.lt.u32 	%p10, %r162, %r160;
	@%p10 bra 	$L__BB2_3;

$L__BB2_11:
	ret;

}

