//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_86
.address_size 64

.extern .func  (.param .b64 func_retval0) malloc
(
	.param .b64 malloc_param_0
)
;
.extern .func free
(
	.param .b64 free_param_0
)
;
.const .align 8 .b8 CUDA_KECCAK_CONSTS[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};
.const .align 4 .b8 r[96] = {1, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 10, 0, 0, 0, 15, 0, 0, 0, 21, 0, 0, 0, 28, 0, 0, 0, 36, 0, 0, 0, 45, 0, 0, 0, 55, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, 27, 0, 0, 0, 41, 0, 0, 0, 56, 0, 0, 0, 8, 0, 0, 0, 25, 0, 0, 0, 43, 0, 0, 0, 62, 0, 0, 0, 18, 0, 0, 0, 39, 0, 0, 0, 61, 0, 0, 0, 20, 0, 0, 0, 44, 0, 0, 0};
.const .align 4 .b8 piln[96] = {10, 0, 0, 0, 7, 0, 0, 0, 11, 0, 0, 0, 17, 0, 0, 0, 18, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 21, 0, 0, 0, 24, 0, 0, 0, 4, 0, 0, 0, 15, 0, 0, 0, 23, 0, 0, 0, 19, 0, 0, 0, 13, 0, 0, 0, 12, 0, 0, 0, 2, 0, 0, 0, 20, 0, 0, 0, 14, 0, 0, 0, 22, 0, 0, 0, 9, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0};
.shared .align 8 .b8 _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE1C[40];
.shared .align 8 .b8 _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE4temp[40];

.func  (.param .b32 func_retval0) _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_(
	.param .b64 _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0,
	.param .b64 _ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<10>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd9, [_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_0];
	ld.param.u64 	%rd10, [_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1__param_1];
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.local.u64 	%rd1, %rd9;
	ld.global.u64 	%rd3, [%rd2+24];
	ld.local.u64 	%rd4, [%rd1+24];
	setp.gt.u64 	%p1, %rd4, %rd3;
	mov.u16 	%rs3, 0;
	mov.u16 	%rs9, %rs3;
	@%p1 bra 	$L__BB0_7;

	setp.lt.u64 	%p2, %rd4, %rd3;
	mov.u16 	%rs4, 1;
	mov.u16 	%rs9, %rs4;
	@%p2 bra 	$L__BB0_7;

	ld.global.u64 	%rd5, [%rd2+16];
	ld.local.u64 	%rd6, [%rd1+16];
	setp.gt.u64 	%p3, %rd6, %rd5;
	mov.u16 	%rs9, %rs3;
	@%p3 bra 	$L__BB0_7;

	setp.lt.u64 	%p4, %rd6, %rd5;
	mov.u16 	%rs9, %rs4;
	@%p4 bra 	$L__BB0_7;

	ld.global.u64 	%rd7, [%rd2+8];
	ld.local.u64 	%rd8, [%rd1+8];
	setp.gt.u64 	%p5, %rd8, %rd7;
	mov.u16 	%rs9, %rs3;
	@%p5 bra 	$L__BB0_7;

	setp.lt.u64 	%p6, %rd8, %rd7;
	mov.u16 	%rs9, %rs4;
	@%p6 bra 	$L__BB0_7;

	ld.local.u64 	%rd11, [%rd1];
	ld.global.u64 	%rd12, [%rd2];
	setp.le.u64 	%p7, %rd11, %rd12;
	selp.u16 	%rs9, 1, 0, %p7;

$L__BB0_7:
	cvt.u32.u16 	%r1, %rs9;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	kernel_lilypad_pow
.visible .entry kernel_lilypad_pow(
	.param .u64 kernel_lilypad_pow_param_0,
	.param .u64 kernel_lilypad_pow_param_1,
	.param .u64 kernel_lilypad_pow_param_2,
	.param .u32 kernel_lilypad_pow_param_3,
	.param .u32 kernel_lilypad_pow_param_4,
	.param .u64 kernel_lilypad_pow_param_5
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot1[240];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<107>;
	.reg .b16 	%rs<109>;
	.reg .b32 	%r<129>;
	.reg .b64 	%rd<1120>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd159, [kernel_lilypad_pow_param_0];
	ld.param.u64 	%rd160, [kernel_lilypad_pow_param_1];
	ld.param.u32 	%r16, [kernel_lilypad_pow_param_3];
	ld.param.u32 	%r15, [kernel_lilypad_pow_param_4];
	ld.param.u64 	%rd162, [kernel_lilypad_pow_param_5];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r18, %r17, %r1;
	setp.ge.u32 	%p1, %r2, %r16;
	@%p1 bra 	$L__BB1_204;

	mul.lo.s32 	%r127, %r2, %r15;
	add.s32 	%r19, %r127, %r15;
	setp.ge.u32 	%p2, %r127, %r19;
	@%p2 bra 	$L__BB1_204;

	mul.wide.s32 	%rd164, %r1, 8;
	add.s64 	%rd2, %rd1, %rd164;
	shl.b32 	%r20, %r1, 3;
	mov.u32 	%r21, _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE1C;
	add.s32 	%r4, %r21, %r20;
	add.s32 	%r22, %r1, 4;
	mul.hi.s32 	%r23, %r22, 1717986919;
	shr.u32 	%r24, %r23, 31;
	shr.u32 	%r25, %r23, 1;
	add.s32 	%r26, %r25, %r24;
	mul.lo.s32 	%r27, %r26, 5;
	sub.s32 	%r28, %r22, %r27;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r5, %r21, %r29;
	add.s32 	%r30, %r1, 1;
	mul.hi.s32 	%r31, %r30, 1717986919;
	shr.u32 	%r32, %r31, 31;
	shr.u32 	%r33, %r31, 1;
	add.s32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, 5;
	sub.s32 	%r36, %r30, %r35;
	shl.b32 	%r37, %r36, 3;
	add.s32 	%r6, %r21, %r37;
	mov.u32 	%r38, _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE4temp;
	add.s32 	%r7, %r38, %r20;
	mul.hi.s32 	%r39, %r1, 1717986919;
	shr.u32 	%r40, %r39, 31;
	shr.u32 	%r41, %r39, 1;
	add.s32 	%r42, %r41, %r40;
	mul.lo.s32 	%r43, %r42, 5;
	sub.s32 	%r44, %r1, %r43;
	shl.b32 	%r45, %r44, 3;
	add.s32 	%r8, %r38, %r45;
	mul.wide.s32 	%rd165, %r1, 4;
	mov.u64 	%rd166, piln;
	add.s64 	%rd3, %rd166, %rd165;
	mov.u64 	%rd167, r;
	add.s64 	%rd4, %rd167, %rd165;
	add.s32 	%r46, %r1, 2;
	mul.hi.s32 	%r47, %r46, 1717986919;
	shr.u32 	%r48, %r47, 31;
	shr.u32 	%r49, %r47, 1;
	add.s32 	%r50, %r49, %r48;
	mul.lo.s32 	%r51, %r50, 5;
	sub.s32 	%r52, %r46, %r51;
	shl.b32 	%r53, %r52, 3;
	add.s32 	%r9, %r21, %r53;
	cvta.to.global.u64 	%rd170, %rd160;
	cvta.to.global.u64 	%rd183, %rd159;
	ld.const.u32 	%r57, [%rd3];
	ld.const.u32 	%r13, [%rd4];
	cvta.to.global.u64 	%rd156, %rd162;

$L__BB1_3:
	mov.u64 	%rd169, 32;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd169;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd29, [retval0+0];
	} // callseq 0
	ld.global.nc.u64 	%rd171, [%rd170];
	cvt.u64.u32 	%rd172, %r127;
	add.s64 	%rd173, %rd171, %rd172;
	setp.lt.u64 	%p3, %rd173, %rd171;
	mov.u64 	%rd1117, 0;
	st.u64 	[%rd29], %rd173;
	selp.u64 	%rd174, 1, 0, %p3;
	ld.global.nc.u64 	%rd175, [%rd170+8];
	add.s64 	%rd176, %rd175, %rd174;
	setp.lt.u64 	%p4, %rd176, %rd175;
	st.u64 	[%rd29+8], %rd176;
	selp.u64 	%rd177, 1, 0, %p4;
	ld.global.nc.u64 	%rd178, [%rd170+16];
	add.s64 	%rd179, %rd178, %rd177;
	setp.lt.u64 	%p5, %rd179, %rd178;
	st.u64 	[%rd29+16], %rd179;
	selp.u64 	%rd180, 1, 0, %p5;
	ld.global.nc.u64 	%rd181, [%rd170+24];
	add.s64 	%rd182, %rd181, %rd180;
	st.u64 	[%rd29+24], %rd182;
	mov.u32 	%r128, 0;
	st.local.v2.u32 	[%rd1+72], {%r128, %r128};
	st.local.v2.u32 	[%rd1+80], {%r128, %r128};
	st.local.v2.u32 	[%rd1+88], {%r128, %r128};
	st.local.v2.u32 	[%rd1+96], {%r128, %r128};
	st.local.v2.u32 	[%rd1+104], {%r128, %r128};
	st.local.v2.u32 	[%rd1+112], {%r128, %r128};
	st.local.v2.u32 	[%rd1+120], {%r128, %r128};
	st.local.v2.u32 	[%rd1+128], {%r128, %r128};
	st.local.v2.u32 	[%rd1+136], {%r128, %r128};
	st.local.v2.u32 	[%rd1+144], {%r128, %r128};
	st.local.v2.u32 	[%rd1+152], {%r128, %r128};
	st.local.v2.u32 	[%rd1+160], {%r128, %r128};
	st.local.v2.u32 	[%rd1+168], {%r128, %r128};
	st.local.v2.u32 	[%rd1+176], {%r128, %r128};
	st.local.v2.u32 	[%rd1+184], {%r128, %r128};
	st.local.v2.u32 	[%rd1+192], {%r128, %r128};

$L__BB1_4:
	add.s64 	%rd184, %rd183, %rd1117;
	ld.global.nc.u8 	%rs25, [%rd184];
	add.s64 	%rd185, %rd1, %rd1117;
	st.local.u8 	[%rd185], %rs25;
	add.s64 	%rd1117, %rd1117, 1;
	add.s32 	%r128, %r128, 1;
	setp.lt.u32 	%p6, %r128, 32;
	@%p6 bra 	$L__BB1_4;

	mov.u64 	%rd1118, 0;

$L__BB1_6:
	add.s64 	%rd187, %rd1, %rd1118;
	add.s64 	%rd188, %rd29, %rd1118;
	ld.u8 	%rs26, [%rd188];
	st.local.u8 	[%rd187+32], %rs26;
	add.s64 	%rd1118, %rd1118, 1;
	setp.lt.u64 	%p7, %rd1118, 32;
	@%p7 bra 	$L__BB1_6;

	setp.gt.s32 	%p8, %r1, 4;
	mov.u64 	%rd189, 1;
	st.local.u64 	[%rd1+64], %rd189;
	mov.u64 	%rd190, -9223372036854775808;
	st.local.u64 	[%rd1+128], %rd190;
	@%p8 bra 	$L__BB1_9;

	ld.local.u64 	%rd191, [%rd2];
	ld.local.u64 	%rd192, [%rd2+40];
	xor.b64  	%rd193, %rd192, %rd191;
	ld.local.u64 	%rd194, [%rd2+80];
	xor.b64  	%rd195, %rd193, %rd194;
	ld.local.u64 	%rd196, [%rd2+120];
	xor.b64  	%rd197, %rd195, %rd196;
	ld.local.u64 	%rd198, [%rd2+160];
	xor.b64  	%rd199, %rd197, %rd198;
	st.shared.u64 	[%r4], %rd199;

$L__BB1_9:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_11;

	ld.shared.u64 	%rd202, [%r5];
	ld.shared.u64 	%rd201, [%r6];
	mov.u32 	%r55, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd201;
	shf.l.wrap.b32 vl, tl, th, %r55;
	shf.l.wrap.b32 vh, th, tl, %r55;
	setp.lt.u32 p, %r55, 32;
	@!p mov.b64 %rd200, {vl,vh};
	@p  mov.b64 %rd200, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd203, %rd200, %rd202;
	st.shared.u64 	[%r7], %rd203;

$L__BB1_11:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd206, [%rd2];
	ld.shared.u64 	%rd207, [%r8];
	xor.b64  	%rd205, %rd206, %rd207;
	st.local.u64 	[%rd2], %rd205;
	ld.local.u64 	%rd208, [%rd2+40];
	xor.b64  	%rd209, %rd208, %rd207;
	st.local.u64 	[%rd2+40], %rd209;
	ld.local.u64 	%rd210, [%rd2+80];
	xor.b64  	%rd211, %rd210, %rd207;
	st.local.u64 	[%rd2+80], %rd211;
	ld.local.u64 	%rd212, [%rd2+120];
	xor.b64  	%rd213, %rd212, %rd207;
	st.local.u64 	[%rd2+120], %rd213;
	ld.local.u64 	%rd214, [%rd2+160];
	xor.b64  	%rd215, %rd214, %rd207;
	st.local.u64 	[%rd2+160], %rd215;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd205;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd204, {vl,vh};
	@p  mov.b64 %rd204, {vh,vl};
	}

	// end inline asm
	mul.wide.s32 	%rd216, %r57, 8;
	add.s64 	%rd35, %rd1, %rd216;
	st.local.u64 	[%rd35], %rd204;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd36, [%rd2];
	ld.local.u64 	%rd37, [%rd2+40];
	ld.local.u64 	%rd38, [%rd2+80];
	ld.local.u64 	%rd39, [%rd2+120];
	ld.local.u64 	%rd40, [%rd2+160];
	@%p8 bra 	$L__BB1_13;

	st.shared.u64 	[%r4], %rd40;
	ld.shared.u64 	%rd217, [%r6];
	not.b64 	%rd218, %rd217;
	ld.shared.u64 	%rd219, [%r9];
	and.b64  	%rd220, %rd219, %rd218;
	st.shared.u64 	[%r7], %rd220;

$L__BB1_13:
	setp.ne.s32 	%p11, %r1, 0;
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd221, [%r8];
	xor.b64  	%rd222, %rd36, %rd221;
	st.local.u64 	[%rd2], %rd222;
	xor.b64  	%rd223, %rd37, %rd221;
	st.local.u64 	[%rd2+40], %rd223;
	xor.b64  	%rd224, %rd38, %rd221;
	st.local.u64 	[%rd2+80], %rd224;
	xor.b64  	%rd225, %rd39, %rd221;
	st.local.u64 	[%rd2+120], %rd225;
	xor.b64  	%rd226, %rd40, %rd221;
	st.local.u64 	[%rd2+160], %rd226;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_15;

	ld.const.u64 	%rd1092, [CUDA_KECCAK_CONSTS];
	ld.local.u64 	%rd227, [%rd1];
	xor.b64  	%rd228, %rd227, %rd1092;
	st.local.u64 	[%rd1], %rd228;

$L__BB1_15:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_17;

	ld.local.u64 	%rd229, [%rd2];
	ld.local.u64 	%rd230, [%rd2+40];
	xor.b64  	%rd231, %rd230, %rd229;
	ld.local.u64 	%rd232, [%rd2+80];
	xor.b64  	%rd233, %rd231, %rd232;
	ld.local.u64 	%rd234, [%rd2+120];
	xor.b64  	%rd235, %rd233, %rd234;
	ld.local.u64 	%rd236, [%rd2+160];
	xor.b64  	%rd237, %rd235, %rd236;
	st.shared.u64 	[%r4], %rd237;

$L__BB1_17:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_19;

	ld.shared.u64 	%rd240, [%r5];
	ld.shared.u64 	%rd239, [%r6];
	mov.u32 	%r58, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd239;
	shf.l.wrap.b32 vl, tl, th, %r58;
	shf.l.wrap.b32 vh, th, tl, %r58;
	setp.lt.u32 p, %r58, 32;
	@!p mov.b64 %rd238, {vl,vh};
	@p  mov.b64 %rd238, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd241, %rd238, %rd240;
	st.shared.u64 	[%r7], %rd241;

$L__BB1_19:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd244, [%rd2];
	ld.shared.u64 	%rd245, [%r8];
	xor.b64  	%rd243, %rd244, %rd245;
	st.local.u64 	[%rd2], %rd243;
	ld.local.u64 	%rd246, [%rd2+40];
	xor.b64  	%rd247, %rd246, %rd245;
	st.local.u64 	[%rd2+40], %rd247;
	ld.local.u64 	%rd248, [%rd2+80];
	xor.b64  	%rd249, %rd248, %rd245;
	st.local.u64 	[%rd2+80], %rd249;
	ld.local.u64 	%rd250, [%rd2+120];
	xor.b64  	%rd251, %rd250, %rd245;
	st.local.u64 	[%rd2+120], %rd251;
	ld.local.u64 	%rd252, [%rd2+160];
	xor.b64  	%rd253, %rd252, %rd245;
	st.local.u64 	[%rd2+160], %rd253;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd243;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd242, {vl,vh};
	@p  mov.b64 %rd242, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd242;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd41, [%rd2];
	ld.local.u64 	%rd42, [%rd2+40];
	ld.local.u64 	%rd43, [%rd2+80];
	ld.local.u64 	%rd44, [%rd2+120];
	ld.local.u64 	%rd45, [%rd2+160];
	@%p8 bra 	$L__BB1_21;

	st.shared.u64 	[%r4], %rd45;
	ld.shared.u64 	%rd254, [%r6];
	not.b64 	%rd255, %rd254;
	ld.shared.u64 	%rd256, [%r9];
	and.b64  	%rd257, %rd256, %rd255;
	st.shared.u64 	[%r7], %rd257;

$L__BB1_21:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd258, [%r8];
	xor.b64  	%rd259, %rd41, %rd258;
	st.local.u64 	[%rd2], %rd259;
	xor.b64  	%rd260, %rd42, %rd258;
	st.local.u64 	[%rd2+40], %rd260;
	xor.b64  	%rd261, %rd43, %rd258;
	st.local.u64 	[%rd2+80], %rd261;
	xor.b64  	%rd262, %rd44, %rd258;
	st.local.u64 	[%rd2+120], %rd262;
	xor.b64  	%rd263, %rd45, %rd258;
	st.local.u64 	[%rd2+160], %rd263;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_23;

	ld.const.u64 	%rd1105, [CUDA_KECCAK_CONSTS+8];
	ld.local.u64 	%rd264, [%rd1];
	xor.b64  	%rd265, %rd264, %rd1105;
	st.local.u64 	[%rd1], %rd265;

$L__BB1_23:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_25;

	ld.local.u64 	%rd266, [%rd2];
	ld.local.u64 	%rd267, [%rd2+40];
	xor.b64  	%rd268, %rd267, %rd266;
	ld.local.u64 	%rd269, [%rd2+80];
	xor.b64  	%rd270, %rd268, %rd269;
	ld.local.u64 	%rd271, [%rd2+120];
	xor.b64  	%rd272, %rd270, %rd271;
	ld.local.u64 	%rd273, [%rd2+160];
	xor.b64  	%rd274, %rd272, %rd273;
	st.shared.u64 	[%r4], %rd274;

$L__BB1_25:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_27;

	ld.shared.u64 	%rd277, [%r5];
	ld.shared.u64 	%rd276, [%r6];
	mov.u32 	%r60, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd276;
	shf.l.wrap.b32 vl, tl, th, %r60;
	shf.l.wrap.b32 vh, th, tl, %r60;
	setp.lt.u32 p, %r60, 32;
	@!p mov.b64 %rd275, {vl,vh};
	@p  mov.b64 %rd275, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd278, %rd275, %rd277;
	st.shared.u64 	[%r7], %rd278;

$L__BB1_27:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd281, [%rd2];
	ld.shared.u64 	%rd282, [%r8];
	xor.b64  	%rd280, %rd281, %rd282;
	st.local.u64 	[%rd2], %rd280;
	ld.local.u64 	%rd283, [%rd2+40];
	xor.b64  	%rd284, %rd283, %rd282;
	st.local.u64 	[%rd2+40], %rd284;
	ld.local.u64 	%rd285, [%rd2+80];
	xor.b64  	%rd286, %rd285, %rd282;
	st.local.u64 	[%rd2+80], %rd286;
	ld.local.u64 	%rd287, [%rd2+120];
	xor.b64  	%rd288, %rd287, %rd282;
	st.local.u64 	[%rd2+120], %rd288;
	ld.local.u64 	%rd289, [%rd2+160];
	xor.b64  	%rd290, %rd289, %rd282;
	st.local.u64 	[%rd2+160], %rd290;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd280;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd279, {vl,vh};
	@p  mov.b64 %rd279, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd279;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd46, [%rd2];
	ld.local.u64 	%rd47, [%rd2+40];
	ld.local.u64 	%rd48, [%rd2+80];
	ld.local.u64 	%rd49, [%rd2+120];
	ld.local.u64 	%rd50, [%rd2+160];
	@%p8 bra 	$L__BB1_29;

	st.shared.u64 	[%r4], %rd50;
	ld.shared.u64 	%rd291, [%r6];
	not.b64 	%rd292, %rd291;
	ld.shared.u64 	%rd293, [%r9];
	and.b64  	%rd294, %rd293, %rd292;
	st.shared.u64 	[%r7], %rd294;

$L__BB1_29:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd295, [%r8];
	xor.b64  	%rd296, %rd46, %rd295;
	st.local.u64 	[%rd2], %rd296;
	xor.b64  	%rd297, %rd47, %rd295;
	st.local.u64 	[%rd2+40], %rd297;
	xor.b64  	%rd298, %rd48, %rd295;
	st.local.u64 	[%rd2+80], %rd298;
	xor.b64  	%rd299, %rd49, %rd295;
	st.local.u64 	[%rd2+120], %rd299;
	xor.b64  	%rd300, %rd50, %rd295;
	st.local.u64 	[%rd2+160], %rd300;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_31;

	ld.const.u64 	%rd1104, [CUDA_KECCAK_CONSTS+16];
	ld.local.u64 	%rd301, [%rd1];
	xor.b64  	%rd302, %rd301, %rd1104;
	st.local.u64 	[%rd1], %rd302;

$L__BB1_31:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_33;

	ld.local.u64 	%rd303, [%rd2];
	ld.local.u64 	%rd304, [%rd2+40];
	xor.b64  	%rd305, %rd304, %rd303;
	ld.local.u64 	%rd306, [%rd2+80];
	xor.b64  	%rd307, %rd305, %rd306;
	ld.local.u64 	%rd308, [%rd2+120];
	xor.b64  	%rd309, %rd307, %rd308;
	ld.local.u64 	%rd310, [%rd2+160];
	xor.b64  	%rd311, %rd309, %rd310;
	st.shared.u64 	[%r4], %rd311;

$L__BB1_33:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_35;

	ld.shared.u64 	%rd314, [%r5];
	ld.shared.u64 	%rd313, [%r6];
	mov.u32 	%r62, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd313;
	shf.l.wrap.b32 vl, tl, th, %r62;
	shf.l.wrap.b32 vh, th, tl, %r62;
	setp.lt.u32 p, %r62, 32;
	@!p mov.b64 %rd312, {vl,vh};
	@p  mov.b64 %rd312, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd315, %rd312, %rd314;
	st.shared.u64 	[%r7], %rd315;

$L__BB1_35:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd318, [%rd2];
	ld.shared.u64 	%rd319, [%r8];
	xor.b64  	%rd317, %rd318, %rd319;
	st.local.u64 	[%rd2], %rd317;
	ld.local.u64 	%rd320, [%rd2+40];
	xor.b64  	%rd321, %rd320, %rd319;
	st.local.u64 	[%rd2+40], %rd321;
	ld.local.u64 	%rd322, [%rd2+80];
	xor.b64  	%rd323, %rd322, %rd319;
	st.local.u64 	[%rd2+80], %rd323;
	ld.local.u64 	%rd324, [%rd2+120];
	xor.b64  	%rd325, %rd324, %rd319;
	st.local.u64 	[%rd2+120], %rd325;
	ld.local.u64 	%rd326, [%rd2+160];
	xor.b64  	%rd327, %rd326, %rd319;
	st.local.u64 	[%rd2+160], %rd327;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd317;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd316, {vl,vh};
	@p  mov.b64 %rd316, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd316;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd51, [%rd2];
	ld.local.u64 	%rd52, [%rd2+40];
	ld.local.u64 	%rd53, [%rd2+80];
	ld.local.u64 	%rd54, [%rd2+120];
	ld.local.u64 	%rd55, [%rd2+160];
	@%p8 bra 	$L__BB1_37;

	st.shared.u64 	[%r4], %rd55;
	ld.shared.u64 	%rd328, [%r6];
	not.b64 	%rd329, %rd328;
	ld.shared.u64 	%rd330, [%r9];
	and.b64  	%rd331, %rd330, %rd329;
	st.shared.u64 	[%r7], %rd331;

$L__BB1_37:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd332, [%r8];
	xor.b64  	%rd333, %rd51, %rd332;
	st.local.u64 	[%rd2], %rd333;
	xor.b64  	%rd334, %rd52, %rd332;
	st.local.u64 	[%rd2+40], %rd334;
	xor.b64  	%rd335, %rd53, %rd332;
	st.local.u64 	[%rd2+80], %rd335;
	xor.b64  	%rd336, %rd54, %rd332;
	st.local.u64 	[%rd2+120], %rd336;
	xor.b64  	%rd337, %rd55, %rd332;
	st.local.u64 	[%rd2+160], %rd337;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_39;

	ld.const.u64 	%rd1103, [CUDA_KECCAK_CONSTS+24];
	ld.local.u64 	%rd338, [%rd1];
	xor.b64  	%rd339, %rd338, %rd1103;
	st.local.u64 	[%rd1], %rd339;

$L__BB1_39:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_41;

	ld.local.u64 	%rd340, [%rd2];
	ld.local.u64 	%rd341, [%rd2+40];
	xor.b64  	%rd342, %rd341, %rd340;
	ld.local.u64 	%rd343, [%rd2+80];
	xor.b64  	%rd344, %rd342, %rd343;
	ld.local.u64 	%rd345, [%rd2+120];
	xor.b64  	%rd346, %rd344, %rd345;
	ld.local.u64 	%rd347, [%rd2+160];
	xor.b64  	%rd348, %rd346, %rd347;
	st.shared.u64 	[%r4], %rd348;

$L__BB1_41:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_43;

	ld.shared.u64 	%rd351, [%r5];
	ld.shared.u64 	%rd350, [%r6];
	mov.u32 	%r64, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd350;
	shf.l.wrap.b32 vl, tl, th, %r64;
	shf.l.wrap.b32 vh, th, tl, %r64;
	setp.lt.u32 p, %r64, 32;
	@!p mov.b64 %rd349, {vl,vh};
	@p  mov.b64 %rd349, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd352, %rd349, %rd351;
	st.shared.u64 	[%r7], %rd352;

$L__BB1_43:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd355, [%rd2];
	ld.shared.u64 	%rd356, [%r8];
	xor.b64  	%rd354, %rd355, %rd356;
	st.local.u64 	[%rd2], %rd354;
	ld.local.u64 	%rd357, [%rd2+40];
	xor.b64  	%rd358, %rd357, %rd356;
	st.local.u64 	[%rd2+40], %rd358;
	ld.local.u64 	%rd359, [%rd2+80];
	xor.b64  	%rd360, %rd359, %rd356;
	st.local.u64 	[%rd2+80], %rd360;
	ld.local.u64 	%rd361, [%rd2+120];
	xor.b64  	%rd362, %rd361, %rd356;
	st.local.u64 	[%rd2+120], %rd362;
	ld.local.u64 	%rd363, [%rd2+160];
	xor.b64  	%rd364, %rd363, %rd356;
	st.local.u64 	[%rd2+160], %rd364;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd354;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd353, {vl,vh};
	@p  mov.b64 %rd353, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd353;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd56, [%rd2];
	ld.local.u64 	%rd57, [%rd2+40];
	ld.local.u64 	%rd58, [%rd2+80];
	ld.local.u64 	%rd59, [%rd2+120];
	ld.local.u64 	%rd60, [%rd2+160];
	@%p8 bra 	$L__BB1_45;

	st.shared.u64 	[%r4], %rd60;
	ld.shared.u64 	%rd365, [%r6];
	not.b64 	%rd366, %rd365;
	ld.shared.u64 	%rd367, [%r9];
	and.b64  	%rd368, %rd367, %rd366;
	st.shared.u64 	[%r7], %rd368;

$L__BB1_45:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd369, [%r8];
	xor.b64  	%rd370, %rd56, %rd369;
	st.local.u64 	[%rd2], %rd370;
	xor.b64  	%rd371, %rd57, %rd369;
	st.local.u64 	[%rd2+40], %rd371;
	xor.b64  	%rd372, %rd58, %rd369;
	st.local.u64 	[%rd2+80], %rd372;
	xor.b64  	%rd373, %rd59, %rd369;
	st.local.u64 	[%rd2+120], %rd373;
	xor.b64  	%rd374, %rd60, %rd369;
	st.local.u64 	[%rd2+160], %rd374;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_47;

	ld.const.u64 	%rd1102, [CUDA_KECCAK_CONSTS+32];
	ld.local.u64 	%rd375, [%rd1];
	xor.b64  	%rd376, %rd375, %rd1102;
	st.local.u64 	[%rd1], %rd376;

$L__BB1_47:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_49;

	ld.local.u64 	%rd377, [%rd2];
	ld.local.u64 	%rd378, [%rd2+40];
	xor.b64  	%rd379, %rd378, %rd377;
	ld.local.u64 	%rd380, [%rd2+80];
	xor.b64  	%rd381, %rd379, %rd380;
	ld.local.u64 	%rd382, [%rd2+120];
	xor.b64  	%rd383, %rd381, %rd382;
	ld.local.u64 	%rd384, [%rd2+160];
	xor.b64  	%rd385, %rd383, %rd384;
	st.shared.u64 	[%r4], %rd385;

$L__BB1_49:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_51;

	ld.shared.u64 	%rd388, [%r5];
	ld.shared.u64 	%rd387, [%r6];
	mov.u32 	%r66, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd387;
	shf.l.wrap.b32 vl, tl, th, %r66;
	shf.l.wrap.b32 vh, th, tl, %r66;
	setp.lt.u32 p, %r66, 32;
	@!p mov.b64 %rd386, {vl,vh};
	@p  mov.b64 %rd386, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd389, %rd386, %rd388;
	st.shared.u64 	[%r7], %rd389;

$L__BB1_51:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd392, [%rd2];
	ld.shared.u64 	%rd393, [%r8];
	xor.b64  	%rd391, %rd392, %rd393;
	st.local.u64 	[%rd2], %rd391;
	ld.local.u64 	%rd394, [%rd2+40];
	xor.b64  	%rd395, %rd394, %rd393;
	st.local.u64 	[%rd2+40], %rd395;
	ld.local.u64 	%rd396, [%rd2+80];
	xor.b64  	%rd397, %rd396, %rd393;
	st.local.u64 	[%rd2+80], %rd397;
	ld.local.u64 	%rd398, [%rd2+120];
	xor.b64  	%rd399, %rd398, %rd393;
	st.local.u64 	[%rd2+120], %rd399;
	ld.local.u64 	%rd400, [%rd2+160];
	xor.b64  	%rd401, %rd400, %rd393;
	st.local.u64 	[%rd2+160], %rd401;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd391;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd390, {vl,vh};
	@p  mov.b64 %rd390, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd390;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd61, [%rd2];
	ld.local.u64 	%rd62, [%rd2+40];
	ld.local.u64 	%rd63, [%rd2+80];
	ld.local.u64 	%rd64, [%rd2+120];
	ld.local.u64 	%rd65, [%rd2+160];
	@%p8 bra 	$L__BB1_53;

	st.shared.u64 	[%r4], %rd65;
	ld.shared.u64 	%rd402, [%r6];
	not.b64 	%rd403, %rd402;
	ld.shared.u64 	%rd404, [%r9];
	and.b64  	%rd405, %rd404, %rd403;
	st.shared.u64 	[%r7], %rd405;

$L__BB1_53:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd406, [%r8];
	xor.b64  	%rd407, %rd61, %rd406;
	st.local.u64 	[%rd2], %rd407;
	xor.b64  	%rd408, %rd62, %rd406;
	st.local.u64 	[%rd2+40], %rd408;
	xor.b64  	%rd409, %rd63, %rd406;
	st.local.u64 	[%rd2+80], %rd409;
	xor.b64  	%rd410, %rd64, %rd406;
	st.local.u64 	[%rd2+120], %rd410;
	xor.b64  	%rd411, %rd65, %rd406;
	st.local.u64 	[%rd2+160], %rd411;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_55;

	ld.const.u64 	%rd1101, [CUDA_KECCAK_CONSTS+40];
	ld.local.u64 	%rd412, [%rd1];
	xor.b64  	%rd413, %rd412, %rd1101;
	st.local.u64 	[%rd1], %rd413;

$L__BB1_55:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_57;

	ld.local.u64 	%rd414, [%rd2];
	ld.local.u64 	%rd415, [%rd2+40];
	xor.b64  	%rd416, %rd415, %rd414;
	ld.local.u64 	%rd417, [%rd2+80];
	xor.b64  	%rd418, %rd416, %rd417;
	ld.local.u64 	%rd419, [%rd2+120];
	xor.b64  	%rd420, %rd418, %rd419;
	ld.local.u64 	%rd421, [%rd2+160];
	xor.b64  	%rd422, %rd420, %rd421;
	st.shared.u64 	[%r4], %rd422;

$L__BB1_57:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_59;

	ld.shared.u64 	%rd425, [%r5];
	ld.shared.u64 	%rd424, [%r6];
	mov.u32 	%r68, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd424;
	shf.l.wrap.b32 vl, tl, th, %r68;
	shf.l.wrap.b32 vh, th, tl, %r68;
	setp.lt.u32 p, %r68, 32;
	@!p mov.b64 %rd423, {vl,vh};
	@p  mov.b64 %rd423, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd426, %rd423, %rd425;
	st.shared.u64 	[%r7], %rd426;

$L__BB1_59:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd429, [%rd2];
	ld.shared.u64 	%rd430, [%r8];
	xor.b64  	%rd428, %rd429, %rd430;
	st.local.u64 	[%rd2], %rd428;
	ld.local.u64 	%rd431, [%rd2+40];
	xor.b64  	%rd432, %rd431, %rd430;
	st.local.u64 	[%rd2+40], %rd432;
	ld.local.u64 	%rd433, [%rd2+80];
	xor.b64  	%rd434, %rd433, %rd430;
	st.local.u64 	[%rd2+80], %rd434;
	ld.local.u64 	%rd435, [%rd2+120];
	xor.b64  	%rd436, %rd435, %rd430;
	st.local.u64 	[%rd2+120], %rd436;
	ld.local.u64 	%rd437, [%rd2+160];
	xor.b64  	%rd438, %rd437, %rd430;
	st.local.u64 	[%rd2+160], %rd438;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd428;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd427, {vl,vh};
	@p  mov.b64 %rd427, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd427;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd66, [%rd2];
	ld.local.u64 	%rd67, [%rd2+40];
	ld.local.u64 	%rd68, [%rd2+80];
	ld.local.u64 	%rd69, [%rd2+120];
	ld.local.u64 	%rd70, [%rd2+160];
	@%p8 bra 	$L__BB1_61;

	st.shared.u64 	[%r4], %rd70;
	ld.shared.u64 	%rd439, [%r6];
	not.b64 	%rd440, %rd439;
	ld.shared.u64 	%rd441, [%r9];
	and.b64  	%rd442, %rd441, %rd440;
	st.shared.u64 	[%r7], %rd442;

$L__BB1_61:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd443, [%r8];
	xor.b64  	%rd444, %rd66, %rd443;
	st.local.u64 	[%rd2], %rd444;
	xor.b64  	%rd445, %rd67, %rd443;
	st.local.u64 	[%rd2+40], %rd445;
	xor.b64  	%rd446, %rd68, %rd443;
	st.local.u64 	[%rd2+80], %rd446;
	xor.b64  	%rd447, %rd69, %rd443;
	st.local.u64 	[%rd2+120], %rd447;
	xor.b64  	%rd448, %rd70, %rd443;
	st.local.u64 	[%rd2+160], %rd448;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_63;

	ld.const.u64 	%rd1100, [CUDA_KECCAK_CONSTS+48];
	ld.local.u64 	%rd449, [%rd1];
	xor.b64  	%rd450, %rd449, %rd1100;
	st.local.u64 	[%rd1], %rd450;

$L__BB1_63:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_65;

	ld.local.u64 	%rd451, [%rd2];
	ld.local.u64 	%rd452, [%rd2+40];
	xor.b64  	%rd453, %rd452, %rd451;
	ld.local.u64 	%rd454, [%rd2+80];
	xor.b64  	%rd455, %rd453, %rd454;
	ld.local.u64 	%rd456, [%rd2+120];
	xor.b64  	%rd457, %rd455, %rd456;
	ld.local.u64 	%rd458, [%rd2+160];
	xor.b64  	%rd459, %rd457, %rd458;
	st.shared.u64 	[%r4], %rd459;

$L__BB1_65:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_67;

	ld.shared.u64 	%rd462, [%r5];
	ld.shared.u64 	%rd461, [%r6];
	mov.u32 	%r70, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd461;
	shf.l.wrap.b32 vl, tl, th, %r70;
	shf.l.wrap.b32 vh, th, tl, %r70;
	setp.lt.u32 p, %r70, 32;
	@!p mov.b64 %rd460, {vl,vh};
	@p  mov.b64 %rd460, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd463, %rd460, %rd462;
	st.shared.u64 	[%r7], %rd463;

$L__BB1_67:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd466, [%rd2];
	ld.shared.u64 	%rd467, [%r8];
	xor.b64  	%rd465, %rd466, %rd467;
	st.local.u64 	[%rd2], %rd465;
	ld.local.u64 	%rd468, [%rd2+40];
	xor.b64  	%rd469, %rd468, %rd467;
	st.local.u64 	[%rd2+40], %rd469;
	ld.local.u64 	%rd470, [%rd2+80];
	xor.b64  	%rd471, %rd470, %rd467;
	st.local.u64 	[%rd2+80], %rd471;
	ld.local.u64 	%rd472, [%rd2+120];
	xor.b64  	%rd473, %rd472, %rd467;
	st.local.u64 	[%rd2+120], %rd473;
	ld.local.u64 	%rd474, [%rd2+160];
	xor.b64  	%rd475, %rd474, %rd467;
	st.local.u64 	[%rd2+160], %rd475;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd465;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd464, {vl,vh};
	@p  mov.b64 %rd464, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd464;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd71, [%rd2];
	ld.local.u64 	%rd72, [%rd2+40];
	ld.local.u64 	%rd73, [%rd2+80];
	ld.local.u64 	%rd74, [%rd2+120];
	ld.local.u64 	%rd75, [%rd2+160];
	@%p8 bra 	$L__BB1_69;

	st.shared.u64 	[%r4], %rd75;
	ld.shared.u64 	%rd476, [%r6];
	not.b64 	%rd477, %rd476;
	ld.shared.u64 	%rd478, [%r9];
	and.b64  	%rd479, %rd478, %rd477;
	st.shared.u64 	[%r7], %rd479;

$L__BB1_69:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd480, [%r8];
	xor.b64  	%rd481, %rd71, %rd480;
	st.local.u64 	[%rd2], %rd481;
	xor.b64  	%rd482, %rd72, %rd480;
	st.local.u64 	[%rd2+40], %rd482;
	xor.b64  	%rd483, %rd73, %rd480;
	st.local.u64 	[%rd2+80], %rd483;
	xor.b64  	%rd484, %rd74, %rd480;
	st.local.u64 	[%rd2+120], %rd484;
	xor.b64  	%rd485, %rd75, %rd480;
	st.local.u64 	[%rd2+160], %rd485;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_71;

	ld.const.u64 	%rd1099, [CUDA_KECCAK_CONSTS+56];
	ld.local.u64 	%rd486, [%rd1];
	xor.b64  	%rd487, %rd486, %rd1099;
	st.local.u64 	[%rd1], %rd487;

$L__BB1_71:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_73;

	ld.local.u64 	%rd488, [%rd2];
	ld.local.u64 	%rd489, [%rd2+40];
	xor.b64  	%rd490, %rd489, %rd488;
	ld.local.u64 	%rd491, [%rd2+80];
	xor.b64  	%rd492, %rd490, %rd491;
	ld.local.u64 	%rd493, [%rd2+120];
	xor.b64  	%rd494, %rd492, %rd493;
	ld.local.u64 	%rd495, [%rd2+160];
	xor.b64  	%rd496, %rd494, %rd495;
	st.shared.u64 	[%r4], %rd496;

$L__BB1_73:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_75;

	ld.shared.u64 	%rd499, [%r5];
	ld.shared.u64 	%rd498, [%r6];
	mov.u32 	%r72, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd498;
	shf.l.wrap.b32 vl, tl, th, %r72;
	shf.l.wrap.b32 vh, th, tl, %r72;
	setp.lt.u32 p, %r72, 32;
	@!p mov.b64 %rd497, {vl,vh};
	@p  mov.b64 %rd497, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd500, %rd497, %rd499;
	st.shared.u64 	[%r7], %rd500;

$L__BB1_75:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd503, [%rd2];
	ld.shared.u64 	%rd504, [%r8];
	xor.b64  	%rd502, %rd503, %rd504;
	st.local.u64 	[%rd2], %rd502;
	ld.local.u64 	%rd505, [%rd2+40];
	xor.b64  	%rd506, %rd505, %rd504;
	st.local.u64 	[%rd2+40], %rd506;
	ld.local.u64 	%rd507, [%rd2+80];
	xor.b64  	%rd508, %rd507, %rd504;
	st.local.u64 	[%rd2+80], %rd508;
	ld.local.u64 	%rd509, [%rd2+120];
	xor.b64  	%rd510, %rd509, %rd504;
	st.local.u64 	[%rd2+120], %rd510;
	ld.local.u64 	%rd511, [%rd2+160];
	xor.b64  	%rd512, %rd511, %rd504;
	st.local.u64 	[%rd2+160], %rd512;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd502;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd501, {vl,vh};
	@p  mov.b64 %rd501, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd501;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd76, [%rd2];
	ld.local.u64 	%rd77, [%rd2+40];
	ld.local.u64 	%rd78, [%rd2+80];
	ld.local.u64 	%rd79, [%rd2+120];
	ld.local.u64 	%rd80, [%rd2+160];
	@%p8 bra 	$L__BB1_77;

	st.shared.u64 	[%r4], %rd80;
	ld.shared.u64 	%rd513, [%r6];
	not.b64 	%rd514, %rd513;
	ld.shared.u64 	%rd515, [%r9];
	and.b64  	%rd516, %rd515, %rd514;
	st.shared.u64 	[%r7], %rd516;

$L__BB1_77:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd517, [%r8];
	xor.b64  	%rd518, %rd76, %rd517;
	st.local.u64 	[%rd2], %rd518;
	xor.b64  	%rd519, %rd77, %rd517;
	st.local.u64 	[%rd2+40], %rd519;
	xor.b64  	%rd520, %rd78, %rd517;
	st.local.u64 	[%rd2+80], %rd520;
	xor.b64  	%rd521, %rd79, %rd517;
	st.local.u64 	[%rd2+120], %rd521;
	xor.b64  	%rd522, %rd80, %rd517;
	st.local.u64 	[%rd2+160], %rd522;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_79;

	ld.const.u64 	%rd1098, [CUDA_KECCAK_CONSTS+64];
	ld.local.u64 	%rd523, [%rd1];
	xor.b64  	%rd524, %rd523, %rd1098;
	st.local.u64 	[%rd1], %rd524;

$L__BB1_79:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_81;

	ld.local.u64 	%rd525, [%rd2];
	ld.local.u64 	%rd526, [%rd2+40];
	xor.b64  	%rd527, %rd526, %rd525;
	ld.local.u64 	%rd528, [%rd2+80];
	xor.b64  	%rd529, %rd527, %rd528;
	ld.local.u64 	%rd530, [%rd2+120];
	xor.b64  	%rd531, %rd529, %rd530;
	ld.local.u64 	%rd532, [%rd2+160];
	xor.b64  	%rd533, %rd531, %rd532;
	st.shared.u64 	[%r4], %rd533;

$L__BB1_81:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_83;

	ld.shared.u64 	%rd536, [%r5];
	ld.shared.u64 	%rd535, [%r6];
	mov.u32 	%r74, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd535;
	shf.l.wrap.b32 vl, tl, th, %r74;
	shf.l.wrap.b32 vh, th, tl, %r74;
	setp.lt.u32 p, %r74, 32;
	@!p mov.b64 %rd534, {vl,vh};
	@p  mov.b64 %rd534, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd537, %rd534, %rd536;
	st.shared.u64 	[%r7], %rd537;

$L__BB1_83:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd540, [%rd2];
	ld.shared.u64 	%rd541, [%r8];
	xor.b64  	%rd539, %rd540, %rd541;
	st.local.u64 	[%rd2], %rd539;
	ld.local.u64 	%rd542, [%rd2+40];
	xor.b64  	%rd543, %rd542, %rd541;
	st.local.u64 	[%rd2+40], %rd543;
	ld.local.u64 	%rd544, [%rd2+80];
	xor.b64  	%rd545, %rd544, %rd541;
	st.local.u64 	[%rd2+80], %rd545;
	ld.local.u64 	%rd546, [%rd2+120];
	xor.b64  	%rd547, %rd546, %rd541;
	st.local.u64 	[%rd2+120], %rd547;
	ld.local.u64 	%rd548, [%rd2+160];
	xor.b64  	%rd549, %rd548, %rd541;
	st.local.u64 	[%rd2+160], %rd549;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd539;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd538, {vl,vh};
	@p  mov.b64 %rd538, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd538;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd81, [%rd2];
	ld.local.u64 	%rd82, [%rd2+40];
	ld.local.u64 	%rd83, [%rd2+80];
	ld.local.u64 	%rd84, [%rd2+120];
	ld.local.u64 	%rd85, [%rd2+160];
	@%p8 bra 	$L__BB1_85;

	st.shared.u64 	[%r4], %rd85;
	ld.shared.u64 	%rd550, [%r6];
	not.b64 	%rd551, %rd550;
	ld.shared.u64 	%rd552, [%r9];
	and.b64  	%rd553, %rd552, %rd551;
	st.shared.u64 	[%r7], %rd553;

$L__BB1_85:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd554, [%r8];
	xor.b64  	%rd555, %rd81, %rd554;
	st.local.u64 	[%rd2], %rd555;
	xor.b64  	%rd556, %rd82, %rd554;
	st.local.u64 	[%rd2+40], %rd556;
	xor.b64  	%rd557, %rd83, %rd554;
	st.local.u64 	[%rd2+80], %rd557;
	xor.b64  	%rd558, %rd84, %rd554;
	st.local.u64 	[%rd2+120], %rd558;
	xor.b64  	%rd559, %rd85, %rd554;
	st.local.u64 	[%rd2+160], %rd559;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_87;

	ld.const.u64 	%rd1097, [CUDA_KECCAK_CONSTS+72];
	ld.local.u64 	%rd560, [%rd1];
	xor.b64  	%rd561, %rd560, %rd1097;
	st.local.u64 	[%rd1], %rd561;

$L__BB1_87:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_89;

	ld.local.u64 	%rd562, [%rd2];
	ld.local.u64 	%rd563, [%rd2+40];
	xor.b64  	%rd564, %rd563, %rd562;
	ld.local.u64 	%rd565, [%rd2+80];
	xor.b64  	%rd566, %rd564, %rd565;
	ld.local.u64 	%rd567, [%rd2+120];
	xor.b64  	%rd568, %rd566, %rd567;
	ld.local.u64 	%rd569, [%rd2+160];
	xor.b64  	%rd570, %rd568, %rd569;
	st.shared.u64 	[%r4], %rd570;

$L__BB1_89:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_91;

	ld.shared.u64 	%rd573, [%r5];
	ld.shared.u64 	%rd572, [%r6];
	mov.u32 	%r76, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd572;
	shf.l.wrap.b32 vl, tl, th, %r76;
	shf.l.wrap.b32 vh, th, tl, %r76;
	setp.lt.u32 p, %r76, 32;
	@!p mov.b64 %rd571, {vl,vh};
	@p  mov.b64 %rd571, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd574, %rd571, %rd573;
	st.shared.u64 	[%r7], %rd574;

$L__BB1_91:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd577, [%rd2];
	ld.shared.u64 	%rd578, [%r8];
	xor.b64  	%rd576, %rd577, %rd578;
	st.local.u64 	[%rd2], %rd576;
	ld.local.u64 	%rd579, [%rd2+40];
	xor.b64  	%rd580, %rd579, %rd578;
	st.local.u64 	[%rd2+40], %rd580;
	ld.local.u64 	%rd581, [%rd2+80];
	xor.b64  	%rd582, %rd581, %rd578;
	st.local.u64 	[%rd2+80], %rd582;
	ld.local.u64 	%rd583, [%rd2+120];
	xor.b64  	%rd584, %rd583, %rd578;
	st.local.u64 	[%rd2+120], %rd584;
	ld.local.u64 	%rd585, [%rd2+160];
	xor.b64  	%rd586, %rd585, %rd578;
	st.local.u64 	[%rd2+160], %rd586;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd576;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd575, {vl,vh};
	@p  mov.b64 %rd575, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd575;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd86, [%rd2];
	ld.local.u64 	%rd87, [%rd2+40];
	ld.local.u64 	%rd88, [%rd2+80];
	ld.local.u64 	%rd89, [%rd2+120];
	ld.local.u64 	%rd90, [%rd2+160];
	@%p8 bra 	$L__BB1_93;

	st.shared.u64 	[%r4], %rd90;
	ld.shared.u64 	%rd587, [%r6];
	not.b64 	%rd588, %rd587;
	ld.shared.u64 	%rd589, [%r9];
	and.b64  	%rd590, %rd589, %rd588;
	st.shared.u64 	[%r7], %rd590;

$L__BB1_93:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd591, [%r8];
	xor.b64  	%rd592, %rd86, %rd591;
	st.local.u64 	[%rd2], %rd592;
	xor.b64  	%rd593, %rd87, %rd591;
	st.local.u64 	[%rd2+40], %rd593;
	xor.b64  	%rd594, %rd88, %rd591;
	st.local.u64 	[%rd2+80], %rd594;
	xor.b64  	%rd595, %rd89, %rd591;
	st.local.u64 	[%rd2+120], %rd595;
	xor.b64  	%rd596, %rd90, %rd591;
	st.local.u64 	[%rd2+160], %rd596;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_95;

	ld.const.u64 	%rd1096, [CUDA_KECCAK_CONSTS+80];
	ld.local.u64 	%rd597, [%rd1];
	xor.b64  	%rd598, %rd597, %rd1096;
	st.local.u64 	[%rd1], %rd598;

$L__BB1_95:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_97;

	ld.local.u64 	%rd599, [%rd2];
	ld.local.u64 	%rd600, [%rd2+40];
	xor.b64  	%rd601, %rd600, %rd599;
	ld.local.u64 	%rd602, [%rd2+80];
	xor.b64  	%rd603, %rd601, %rd602;
	ld.local.u64 	%rd604, [%rd2+120];
	xor.b64  	%rd605, %rd603, %rd604;
	ld.local.u64 	%rd606, [%rd2+160];
	xor.b64  	%rd607, %rd605, %rd606;
	st.shared.u64 	[%r4], %rd607;

$L__BB1_97:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_99;

	ld.shared.u64 	%rd610, [%r5];
	ld.shared.u64 	%rd609, [%r6];
	mov.u32 	%r78, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd609;
	shf.l.wrap.b32 vl, tl, th, %r78;
	shf.l.wrap.b32 vh, th, tl, %r78;
	setp.lt.u32 p, %r78, 32;
	@!p mov.b64 %rd608, {vl,vh};
	@p  mov.b64 %rd608, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd611, %rd608, %rd610;
	st.shared.u64 	[%r7], %rd611;

$L__BB1_99:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd614, [%rd2];
	ld.shared.u64 	%rd615, [%r8];
	xor.b64  	%rd613, %rd614, %rd615;
	st.local.u64 	[%rd2], %rd613;
	ld.local.u64 	%rd616, [%rd2+40];
	xor.b64  	%rd617, %rd616, %rd615;
	st.local.u64 	[%rd2+40], %rd617;
	ld.local.u64 	%rd618, [%rd2+80];
	xor.b64  	%rd619, %rd618, %rd615;
	st.local.u64 	[%rd2+80], %rd619;
	ld.local.u64 	%rd620, [%rd2+120];
	xor.b64  	%rd621, %rd620, %rd615;
	st.local.u64 	[%rd2+120], %rd621;
	ld.local.u64 	%rd622, [%rd2+160];
	xor.b64  	%rd623, %rd622, %rd615;
	st.local.u64 	[%rd2+160], %rd623;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd613;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd612, {vl,vh};
	@p  mov.b64 %rd612, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd612;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd91, [%rd2];
	ld.local.u64 	%rd92, [%rd2+40];
	ld.local.u64 	%rd93, [%rd2+80];
	ld.local.u64 	%rd94, [%rd2+120];
	ld.local.u64 	%rd95, [%rd2+160];
	@%p8 bra 	$L__BB1_101;

	st.shared.u64 	[%r4], %rd95;
	ld.shared.u64 	%rd624, [%r6];
	not.b64 	%rd625, %rd624;
	ld.shared.u64 	%rd626, [%r9];
	and.b64  	%rd627, %rd626, %rd625;
	st.shared.u64 	[%r7], %rd627;

$L__BB1_101:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd628, [%r8];
	xor.b64  	%rd629, %rd91, %rd628;
	st.local.u64 	[%rd2], %rd629;
	xor.b64  	%rd630, %rd92, %rd628;
	st.local.u64 	[%rd2+40], %rd630;
	xor.b64  	%rd631, %rd93, %rd628;
	st.local.u64 	[%rd2+80], %rd631;
	xor.b64  	%rd632, %rd94, %rd628;
	st.local.u64 	[%rd2+120], %rd632;
	xor.b64  	%rd633, %rd95, %rd628;
	st.local.u64 	[%rd2+160], %rd633;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_103;

	ld.const.u64 	%rd1095, [CUDA_KECCAK_CONSTS+88];
	ld.local.u64 	%rd634, [%rd1];
	xor.b64  	%rd635, %rd634, %rd1095;
	st.local.u64 	[%rd1], %rd635;

$L__BB1_103:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_105;

	ld.local.u64 	%rd636, [%rd2];
	ld.local.u64 	%rd637, [%rd2+40];
	xor.b64  	%rd638, %rd637, %rd636;
	ld.local.u64 	%rd639, [%rd2+80];
	xor.b64  	%rd640, %rd638, %rd639;
	ld.local.u64 	%rd641, [%rd2+120];
	xor.b64  	%rd642, %rd640, %rd641;
	ld.local.u64 	%rd643, [%rd2+160];
	xor.b64  	%rd644, %rd642, %rd643;
	st.shared.u64 	[%r4], %rd644;

$L__BB1_105:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_107;

	ld.shared.u64 	%rd647, [%r5];
	ld.shared.u64 	%rd646, [%r6];
	mov.u32 	%r80, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd646;
	shf.l.wrap.b32 vl, tl, th, %r80;
	shf.l.wrap.b32 vh, th, tl, %r80;
	setp.lt.u32 p, %r80, 32;
	@!p mov.b64 %rd645, {vl,vh};
	@p  mov.b64 %rd645, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd648, %rd645, %rd647;
	st.shared.u64 	[%r7], %rd648;

$L__BB1_107:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd651, [%rd2];
	ld.shared.u64 	%rd652, [%r8];
	xor.b64  	%rd650, %rd651, %rd652;
	st.local.u64 	[%rd2], %rd650;
	ld.local.u64 	%rd653, [%rd2+40];
	xor.b64  	%rd654, %rd653, %rd652;
	st.local.u64 	[%rd2+40], %rd654;
	ld.local.u64 	%rd655, [%rd2+80];
	xor.b64  	%rd656, %rd655, %rd652;
	st.local.u64 	[%rd2+80], %rd656;
	ld.local.u64 	%rd657, [%rd2+120];
	xor.b64  	%rd658, %rd657, %rd652;
	st.local.u64 	[%rd2+120], %rd658;
	ld.local.u64 	%rd659, [%rd2+160];
	xor.b64  	%rd660, %rd659, %rd652;
	st.local.u64 	[%rd2+160], %rd660;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd650;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd649, {vl,vh};
	@p  mov.b64 %rd649, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd649;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd96, [%rd2];
	ld.local.u64 	%rd97, [%rd2+40];
	ld.local.u64 	%rd98, [%rd2+80];
	ld.local.u64 	%rd99, [%rd2+120];
	ld.local.u64 	%rd100, [%rd2+160];
	@%p8 bra 	$L__BB1_109;

	st.shared.u64 	[%r4], %rd100;
	ld.shared.u64 	%rd661, [%r6];
	not.b64 	%rd662, %rd661;
	ld.shared.u64 	%rd663, [%r9];
	and.b64  	%rd664, %rd663, %rd662;
	st.shared.u64 	[%r7], %rd664;

$L__BB1_109:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd665, [%r8];
	xor.b64  	%rd666, %rd96, %rd665;
	st.local.u64 	[%rd2], %rd666;
	xor.b64  	%rd667, %rd97, %rd665;
	st.local.u64 	[%rd2+40], %rd667;
	xor.b64  	%rd668, %rd98, %rd665;
	st.local.u64 	[%rd2+80], %rd668;
	xor.b64  	%rd669, %rd99, %rd665;
	st.local.u64 	[%rd2+120], %rd669;
	xor.b64  	%rd670, %rd100, %rd665;
	st.local.u64 	[%rd2+160], %rd670;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_111;

	ld.const.u64 	%rd1094, [CUDA_KECCAK_CONSTS+96];
	ld.local.u64 	%rd671, [%rd1];
	xor.b64  	%rd672, %rd671, %rd1094;
	st.local.u64 	[%rd1], %rd672;

$L__BB1_111:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_113;

	ld.local.u64 	%rd673, [%rd2];
	ld.local.u64 	%rd674, [%rd2+40];
	xor.b64  	%rd675, %rd674, %rd673;
	ld.local.u64 	%rd676, [%rd2+80];
	xor.b64  	%rd677, %rd675, %rd676;
	ld.local.u64 	%rd678, [%rd2+120];
	xor.b64  	%rd679, %rd677, %rd678;
	ld.local.u64 	%rd680, [%rd2+160];
	xor.b64  	%rd681, %rd679, %rd680;
	st.shared.u64 	[%r4], %rd681;

$L__BB1_113:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_115;

	ld.shared.u64 	%rd684, [%r5];
	ld.shared.u64 	%rd683, [%r6];
	mov.u32 	%r82, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd683;
	shf.l.wrap.b32 vl, tl, th, %r82;
	shf.l.wrap.b32 vh, th, tl, %r82;
	setp.lt.u32 p, %r82, 32;
	@!p mov.b64 %rd682, {vl,vh};
	@p  mov.b64 %rd682, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd685, %rd682, %rd684;
	st.shared.u64 	[%r7], %rd685;

$L__BB1_115:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd688, [%rd2];
	ld.shared.u64 	%rd689, [%r8];
	xor.b64  	%rd687, %rd688, %rd689;
	st.local.u64 	[%rd2], %rd687;
	ld.local.u64 	%rd690, [%rd2+40];
	xor.b64  	%rd691, %rd690, %rd689;
	st.local.u64 	[%rd2+40], %rd691;
	ld.local.u64 	%rd692, [%rd2+80];
	xor.b64  	%rd693, %rd692, %rd689;
	st.local.u64 	[%rd2+80], %rd693;
	ld.local.u64 	%rd694, [%rd2+120];
	xor.b64  	%rd695, %rd694, %rd689;
	st.local.u64 	[%rd2+120], %rd695;
	ld.local.u64 	%rd696, [%rd2+160];
	xor.b64  	%rd697, %rd696, %rd689;
	st.local.u64 	[%rd2+160], %rd697;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd687;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd686, {vl,vh};
	@p  mov.b64 %rd686, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd686;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd101, [%rd2];
	ld.local.u64 	%rd102, [%rd2+40];
	ld.local.u64 	%rd103, [%rd2+80];
	ld.local.u64 	%rd104, [%rd2+120];
	ld.local.u64 	%rd105, [%rd2+160];
	@%p8 bra 	$L__BB1_117;

	st.shared.u64 	[%r4], %rd105;
	ld.shared.u64 	%rd698, [%r6];
	not.b64 	%rd699, %rd698;
	ld.shared.u64 	%rd700, [%r9];
	and.b64  	%rd701, %rd700, %rd699;
	st.shared.u64 	[%r7], %rd701;

$L__BB1_117:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd702, [%r8];
	xor.b64  	%rd703, %rd101, %rd702;
	st.local.u64 	[%rd2], %rd703;
	xor.b64  	%rd704, %rd102, %rd702;
	st.local.u64 	[%rd2+40], %rd704;
	xor.b64  	%rd705, %rd103, %rd702;
	st.local.u64 	[%rd2+80], %rd705;
	xor.b64  	%rd706, %rd104, %rd702;
	st.local.u64 	[%rd2+120], %rd706;
	xor.b64  	%rd707, %rd105, %rd702;
	st.local.u64 	[%rd2+160], %rd707;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_119;

	ld.const.u64 	%rd1093, [CUDA_KECCAK_CONSTS+104];
	ld.local.u64 	%rd708, [%rd1];
	xor.b64  	%rd709, %rd708, %rd1093;
	st.local.u64 	[%rd1], %rd709;

$L__BB1_119:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_121;

	ld.local.u64 	%rd710, [%rd2];
	ld.local.u64 	%rd711, [%rd2+40];
	xor.b64  	%rd712, %rd711, %rd710;
	ld.local.u64 	%rd713, [%rd2+80];
	xor.b64  	%rd714, %rd712, %rd713;
	ld.local.u64 	%rd715, [%rd2+120];
	xor.b64  	%rd716, %rd714, %rd715;
	ld.local.u64 	%rd717, [%rd2+160];
	xor.b64  	%rd718, %rd716, %rd717;
	st.shared.u64 	[%r4], %rd718;

$L__BB1_121:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_123;

	ld.shared.u64 	%rd721, [%r5];
	ld.shared.u64 	%rd720, [%r6];
	mov.u32 	%r84, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd720;
	shf.l.wrap.b32 vl, tl, th, %r84;
	shf.l.wrap.b32 vh, th, tl, %r84;
	setp.lt.u32 p, %r84, 32;
	@!p mov.b64 %rd719, {vl,vh};
	@p  mov.b64 %rd719, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd722, %rd719, %rd721;
	st.shared.u64 	[%r7], %rd722;

$L__BB1_123:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd725, [%rd2];
	ld.shared.u64 	%rd726, [%r8];
	xor.b64  	%rd724, %rd725, %rd726;
	st.local.u64 	[%rd2], %rd724;
	ld.local.u64 	%rd727, [%rd2+40];
	xor.b64  	%rd728, %rd727, %rd726;
	st.local.u64 	[%rd2+40], %rd728;
	ld.local.u64 	%rd729, [%rd2+80];
	xor.b64  	%rd730, %rd729, %rd726;
	st.local.u64 	[%rd2+80], %rd730;
	ld.local.u64 	%rd731, [%rd2+120];
	xor.b64  	%rd732, %rd731, %rd726;
	st.local.u64 	[%rd2+120], %rd732;
	ld.local.u64 	%rd733, [%rd2+160];
	xor.b64  	%rd734, %rd733, %rd726;
	st.local.u64 	[%rd2+160], %rd734;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd724;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd723, {vl,vh};
	@p  mov.b64 %rd723, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd723;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd106, [%rd2];
	ld.local.u64 	%rd107, [%rd2+40];
	ld.local.u64 	%rd108, [%rd2+80];
	ld.local.u64 	%rd109, [%rd2+120];
	ld.local.u64 	%rd110, [%rd2+160];
	@%p8 bra 	$L__BB1_125;

	st.shared.u64 	[%r4], %rd110;
	ld.shared.u64 	%rd735, [%r6];
	not.b64 	%rd736, %rd735;
	ld.shared.u64 	%rd737, [%r9];
	and.b64  	%rd738, %rd737, %rd736;
	st.shared.u64 	[%r7], %rd738;

$L__BB1_125:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd739, [%r8];
	xor.b64  	%rd740, %rd106, %rd739;
	st.local.u64 	[%rd2], %rd740;
	xor.b64  	%rd741, %rd107, %rd739;
	st.local.u64 	[%rd2+40], %rd741;
	xor.b64  	%rd742, %rd108, %rd739;
	st.local.u64 	[%rd2+80], %rd742;
	xor.b64  	%rd743, %rd109, %rd739;
	st.local.u64 	[%rd2+120], %rd743;
	xor.b64  	%rd744, %rd110, %rd739;
	st.local.u64 	[%rd2+160], %rd744;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_127;

	ld.const.u64 	%rd1112, [CUDA_KECCAK_CONSTS+112];
	ld.local.u64 	%rd745, [%rd1];
	xor.b64  	%rd746, %rd745, %rd1112;
	st.local.u64 	[%rd1], %rd746;

$L__BB1_127:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_129;

	ld.local.u64 	%rd747, [%rd2];
	ld.local.u64 	%rd748, [%rd2+40];
	xor.b64  	%rd749, %rd748, %rd747;
	ld.local.u64 	%rd750, [%rd2+80];
	xor.b64  	%rd751, %rd749, %rd750;
	ld.local.u64 	%rd752, [%rd2+120];
	xor.b64  	%rd753, %rd751, %rd752;
	ld.local.u64 	%rd754, [%rd2+160];
	xor.b64  	%rd755, %rd753, %rd754;
	st.shared.u64 	[%r4], %rd755;

$L__BB1_129:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_131;

	ld.shared.u64 	%rd758, [%r5];
	ld.shared.u64 	%rd757, [%r6];
	mov.u32 	%r86, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd757;
	shf.l.wrap.b32 vl, tl, th, %r86;
	shf.l.wrap.b32 vh, th, tl, %r86;
	setp.lt.u32 p, %r86, 32;
	@!p mov.b64 %rd756, {vl,vh};
	@p  mov.b64 %rd756, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd759, %rd756, %rd758;
	st.shared.u64 	[%r7], %rd759;

$L__BB1_131:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd762, [%rd2];
	ld.shared.u64 	%rd763, [%r8];
	xor.b64  	%rd761, %rd762, %rd763;
	st.local.u64 	[%rd2], %rd761;
	ld.local.u64 	%rd764, [%rd2+40];
	xor.b64  	%rd765, %rd764, %rd763;
	st.local.u64 	[%rd2+40], %rd765;
	ld.local.u64 	%rd766, [%rd2+80];
	xor.b64  	%rd767, %rd766, %rd763;
	st.local.u64 	[%rd2+80], %rd767;
	ld.local.u64 	%rd768, [%rd2+120];
	xor.b64  	%rd769, %rd768, %rd763;
	st.local.u64 	[%rd2+120], %rd769;
	ld.local.u64 	%rd770, [%rd2+160];
	xor.b64  	%rd771, %rd770, %rd763;
	st.local.u64 	[%rd2+160], %rd771;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd761;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd760, {vl,vh};
	@p  mov.b64 %rd760, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd760;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd111, [%rd2];
	ld.local.u64 	%rd112, [%rd2+40];
	ld.local.u64 	%rd113, [%rd2+80];
	ld.local.u64 	%rd114, [%rd2+120];
	ld.local.u64 	%rd115, [%rd2+160];
	@%p8 bra 	$L__BB1_133;

	st.shared.u64 	[%r4], %rd115;
	ld.shared.u64 	%rd772, [%r6];
	not.b64 	%rd773, %rd772;
	ld.shared.u64 	%rd774, [%r9];
	and.b64  	%rd775, %rd774, %rd773;
	st.shared.u64 	[%r7], %rd775;

$L__BB1_133:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd776, [%r8];
	xor.b64  	%rd777, %rd111, %rd776;
	st.local.u64 	[%rd2], %rd777;
	xor.b64  	%rd778, %rd112, %rd776;
	st.local.u64 	[%rd2+40], %rd778;
	xor.b64  	%rd779, %rd113, %rd776;
	st.local.u64 	[%rd2+80], %rd779;
	xor.b64  	%rd780, %rd114, %rd776;
	st.local.u64 	[%rd2+120], %rd780;
	xor.b64  	%rd781, %rd115, %rd776;
	st.local.u64 	[%rd2+160], %rd781;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_135;

	ld.const.u64 	%rd1111, [CUDA_KECCAK_CONSTS+120];
	ld.local.u64 	%rd782, [%rd1];
	xor.b64  	%rd783, %rd782, %rd1111;
	st.local.u64 	[%rd1], %rd783;

$L__BB1_135:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_137;

	ld.local.u64 	%rd784, [%rd2];
	ld.local.u64 	%rd785, [%rd2+40];
	xor.b64  	%rd786, %rd785, %rd784;
	ld.local.u64 	%rd787, [%rd2+80];
	xor.b64  	%rd788, %rd786, %rd787;
	ld.local.u64 	%rd789, [%rd2+120];
	xor.b64  	%rd790, %rd788, %rd789;
	ld.local.u64 	%rd791, [%rd2+160];
	xor.b64  	%rd792, %rd790, %rd791;
	st.shared.u64 	[%r4], %rd792;

$L__BB1_137:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_139;

	ld.shared.u64 	%rd795, [%r5];
	ld.shared.u64 	%rd794, [%r6];
	mov.u32 	%r88, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd794;
	shf.l.wrap.b32 vl, tl, th, %r88;
	shf.l.wrap.b32 vh, th, tl, %r88;
	setp.lt.u32 p, %r88, 32;
	@!p mov.b64 %rd793, {vl,vh};
	@p  mov.b64 %rd793, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd796, %rd793, %rd795;
	st.shared.u64 	[%r7], %rd796;

$L__BB1_139:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd799, [%rd2];
	ld.shared.u64 	%rd800, [%r8];
	xor.b64  	%rd798, %rd799, %rd800;
	st.local.u64 	[%rd2], %rd798;
	ld.local.u64 	%rd801, [%rd2+40];
	xor.b64  	%rd802, %rd801, %rd800;
	st.local.u64 	[%rd2+40], %rd802;
	ld.local.u64 	%rd803, [%rd2+80];
	xor.b64  	%rd804, %rd803, %rd800;
	st.local.u64 	[%rd2+80], %rd804;
	ld.local.u64 	%rd805, [%rd2+120];
	xor.b64  	%rd806, %rd805, %rd800;
	st.local.u64 	[%rd2+120], %rd806;
	ld.local.u64 	%rd807, [%rd2+160];
	xor.b64  	%rd808, %rd807, %rd800;
	st.local.u64 	[%rd2+160], %rd808;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd798;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd797, {vl,vh};
	@p  mov.b64 %rd797, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd797;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd116, [%rd2];
	ld.local.u64 	%rd117, [%rd2+40];
	ld.local.u64 	%rd118, [%rd2+80];
	ld.local.u64 	%rd119, [%rd2+120];
	ld.local.u64 	%rd120, [%rd2+160];
	@%p8 bra 	$L__BB1_141;

	st.shared.u64 	[%r4], %rd120;
	ld.shared.u64 	%rd809, [%r6];
	not.b64 	%rd810, %rd809;
	ld.shared.u64 	%rd811, [%r9];
	and.b64  	%rd812, %rd811, %rd810;
	st.shared.u64 	[%r7], %rd812;

$L__BB1_141:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd813, [%r8];
	xor.b64  	%rd814, %rd116, %rd813;
	st.local.u64 	[%rd2], %rd814;
	xor.b64  	%rd815, %rd117, %rd813;
	st.local.u64 	[%rd2+40], %rd815;
	xor.b64  	%rd816, %rd118, %rd813;
	st.local.u64 	[%rd2+80], %rd816;
	xor.b64  	%rd817, %rd119, %rd813;
	st.local.u64 	[%rd2+120], %rd817;
	xor.b64  	%rd818, %rd120, %rd813;
	st.local.u64 	[%rd2+160], %rd818;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_143;

	ld.const.u64 	%rd1110, [CUDA_KECCAK_CONSTS+128];
	ld.local.u64 	%rd819, [%rd1];
	xor.b64  	%rd820, %rd819, %rd1110;
	st.local.u64 	[%rd1], %rd820;

$L__BB1_143:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_145;

	ld.local.u64 	%rd821, [%rd2];
	ld.local.u64 	%rd822, [%rd2+40];
	xor.b64  	%rd823, %rd822, %rd821;
	ld.local.u64 	%rd824, [%rd2+80];
	xor.b64  	%rd825, %rd823, %rd824;
	ld.local.u64 	%rd826, [%rd2+120];
	xor.b64  	%rd827, %rd825, %rd826;
	ld.local.u64 	%rd828, [%rd2+160];
	xor.b64  	%rd829, %rd827, %rd828;
	st.shared.u64 	[%r4], %rd829;

$L__BB1_145:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_147;

	ld.shared.u64 	%rd832, [%r5];
	ld.shared.u64 	%rd831, [%r6];
	mov.u32 	%r90, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd831;
	shf.l.wrap.b32 vl, tl, th, %r90;
	shf.l.wrap.b32 vh, th, tl, %r90;
	setp.lt.u32 p, %r90, 32;
	@!p mov.b64 %rd830, {vl,vh};
	@p  mov.b64 %rd830, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd833, %rd830, %rd832;
	st.shared.u64 	[%r7], %rd833;

$L__BB1_147:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd836, [%rd2];
	ld.shared.u64 	%rd837, [%r8];
	xor.b64  	%rd835, %rd836, %rd837;
	st.local.u64 	[%rd2], %rd835;
	ld.local.u64 	%rd838, [%rd2+40];
	xor.b64  	%rd839, %rd838, %rd837;
	st.local.u64 	[%rd2+40], %rd839;
	ld.local.u64 	%rd840, [%rd2+80];
	xor.b64  	%rd841, %rd840, %rd837;
	st.local.u64 	[%rd2+80], %rd841;
	ld.local.u64 	%rd842, [%rd2+120];
	xor.b64  	%rd843, %rd842, %rd837;
	st.local.u64 	[%rd2+120], %rd843;
	ld.local.u64 	%rd844, [%rd2+160];
	xor.b64  	%rd845, %rd844, %rd837;
	st.local.u64 	[%rd2+160], %rd845;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd835;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd834, {vl,vh};
	@p  mov.b64 %rd834, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd834;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd121, [%rd2];
	ld.local.u64 	%rd122, [%rd2+40];
	ld.local.u64 	%rd123, [%rd2+80];
	ld.local.u64 	%rd124, [%rd2+120];
	ld.local.u64 	%rd125, [%rd2+160];
	@%p8 bra 	$L__BB1_149;

	st.shared.u64 	[%r4], %rd125;
	ld.shared.u64 	%rd846, [%r6];
	not.b64 	%rd847, %rd846;
	ld.shared.u64 	%rd848, [%r9];
	and.b64  	%rd849, %rd848, %rd847;
	st.shared.u64 	[%r7], %rd849;

$L__BB1_149:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd850, [%r8];
	xor.b64  	%rd851, %rd121, %rd850;
	st.local.u64 	[%rd2], %rd851;
	xor.b64  	%rd852, %rd122, %rd850;
	st.local.u64 	[%rd2+40], %rd852;
	xor.b64  	%rd853, %rd123, %rd850;
	st.local.u64 	[%rd2+80], %rd853;
	xor.b64  	%rd854, %rd124, %rd850;
	st.local.u64 	[%rd2+120], %rd854;
	xor.b64  	%rd855, %rd125, %rd850;
	st.local.u64 	[%rd2+160], %rd855;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_151;

	ld.const.u64 	%rd1109, [CUDA_KECCAK_CONSTS+136];
	ld.local.u64 	%rd856, [%rd1];
	xor.b64  	%rd857, %rd856, %rd1109;
	st.local.u64 	[%rd1], %rd857;

$L__BB1_151:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_153;

	ld.local.u64 	%rd858, [%rd2];
	ld.local.u64 	%rd859, [%rd2+40];
	xor.b64  	%rd860, %rd859, %rd858;
	ld.local.u64 	%rd861, [%rd2+80];
	xor.b64  	%rd862, %rd860, %rd861;
	ld.local.u64 	%rd863, [%rd2+120];
	xor.b64  	%rd864, %rd862, %rd863;
	ld.local.u64 	%rd865, [%rd2+160];
	xor.b64  	%rd866, %rd864, %rd865;
	st.shared.u64 	[%r4], %rd866;

$L__BB1_153:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_155;

	ld.shared.u64 	%rd869, [%r5];
	ld.shared.u64 	%rd868, [%r6];
	mov.u32 	%r92, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd868;
	shf.l.wrap.b32 vl, tl, th, %r92;
	shf.l.wrap.b32 vh, th, tl, %r92;
	setp.lt.u32 p, %r92, 32;
	@!p mov.b64 %rd867, {vl,vh};
	@p  mov.b64 %rd867, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd870, %rd867, %rd869;
	st.shared.u64 	[%r7], %rd870;

$L__BB1_155:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd873, [%rd2];
	ld.shared.u64 	%rd874, [%r8];
	xor.b64  	%rd872, %rd873, %rd874;
	st.local.u64 	[%rd2], %rd872;
	ld.local.u64 	%rd875, [%rd2+40];
	xor.b64  	%rd876, %rd875, %rd874;
	st.local.u64 	[%rd2+40], %rd876;
	ld.local.u64 	%rd877, [%rd2+80];
	xor.b64  	%rd878, %rd877, %rd874;
	st.local.u64 	[%rd2+80], %rd878;
	ld.local.u64 	%rd879, [%rd2+120];
	xor.b64  	%rd880, %rd879, %rd874;
	st.local.u64 	[%rd2+120], %rd880;
	ld.local.u64 	%rd881, [%rd2+160];
	xor.b64  	%rd882, %rd881, %rd874;
	st.local.u64 	[%rd2+160], %rd882;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd872;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd871, {vl,vh};
	@p  mov.b64 %rd871, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd871;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd126, [%rd2];
	ld.local.u64 	%rd127, [%rd2+40];
	ld.local.u64 	%rd128, [%rd2+80];
	ld.local.u64 	%rd129, [%rd2+120];
	ld.local.u64 	%rd130, [%rd2+160];
	@%p8 bra 	$L__BB1_157;

	st.shared.u64 	[%r4], %rd130;
	ld.shared.u64 	%rd883, [%r6];
	not.b64 	%rd884, %rd883;
	ld.shared.u64 	%rd885, [%r9];
	and.b64  	%rd886, %rd885, %rd884;
	st.shared.u64 	[%r7], %rd886;

$L__BB1_157:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd887, [%r8];
	xor.b64  	%rd888, %rd126, %rd887;
	st.local.u64 	[%rd2], %rd888;
	xor.b64  	%rd889, %rd127, %rd887;
	st.local.u64 	[%rd2+40], %rd889;
	xor.b64  	%rd890, %rd128, %rd887;
	st.local.u64 	[%rd2+80], %rd890;
	xor.b64  	%rd891, %rd129, %rd887;
	st.local.u64 	[%rd2+120], %rd891;
	xor.b64  	%rd892, %rd130, %rd887;
	st.local.u64 	[%rd2+160], %rd892;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_159;

	ld.const.u64 	%rd1108, [CUDA_KECCAK_CONSTS+144];
	ld.local.u64 	%rd893, [%rd1];
	xor.b64  	%rd894, %rd893, %rd1108;
	st.local.u64 	[%rd1], %rd894;

$L__BB1_159:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_161;

	ld.local.u64 	%rd895, [%rd2];
	ld.local.u64 	%rd896, [%rd2+40];
	xor.b64  	%rd897, %rd896, %rd895;
	ld.local.u64 	%rd898, [%rd2+80];
	xor.b64  	%rd899, %rd897, %rd898;
	ld.local.u64 	%rd900, [%rd2+120];
	xor.b64  	%rd901, %rd899, %rd900;
	ld.local.u64 	%rd902, [%rd2+160];
	xor.b64  	%rd903, %rd901, %rd902;
	st.shared.u64 	[%r4], %rd903;

$L__BB1_161:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_163;

	ld.shared.u64 	%rd906, [%r5];
	ld.shared.u64 	%rd905, [%r6];
	mov.u32 	%r94, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd905;
	shf.l.wrap.b32 vl, tl, th, %r94;
	shf.l.wrap.b32 vh, th, tl, %r94;
	setp.lt.u32 p, %r94, 32;
	@!p mov.b64 %rd904, {vl,vh};
	@p  mov.b64 %rd904, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd907, %rd904, %rd906;
	st.shared.u64 	[%r7], %rd907;

$L__BB1_163:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd910, [%rd2];
	ld.shared.u64 	%rd911, [%r8];
	xor.b64  	%rd909, %rd910, %rd911;
	st.local.u64 	[%rd2], %rd909;
	ld.local.u64 	%rd912, [%rd2+40];
	xor.b64  	%rd913, %rd912, %rd911;
	st.local.u64 	[%rd2+40], %rd913;
	ld.local.u64 	%rd914, [%rd2+80];
	xor.b64  	%rd915, %rd914, %rd911;
	st.local.u64 	[%rd2+80], %rd915;
	ld.local.u64 	%rd916, [%rd2+120];
	xor.b64  	%rd917, %rd916, %rd911;
	st.local.u64 	[%rd2+120], %rd917;
	ld.local.u64 	%rd918, [%rd2+160];
	xor.b64  	%rd919, %rd918, %rd911;
	st.local.u64 	[%rd2+160], %rd919;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd909;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd908, {vl,vh};
	@p  mov.b64 %rd908, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd908;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd131, [%rd2];
	ld.local.u64 	%rd132, [%rd2+40];
	ld.local.u64 	%rd133, [%rd2+80];
	ld.local.u64 	%rd134, [%rd2+120];
	ld.local.u64 	%rd135, [%rd2+160];
	@%p8 bra 	$L__BB1_165;

	st.shared.u64 	[%r4], %rd135;
	ld.shared.u64 	%rd920, [%r6];
	not.b64 	%rd921, %rd920;
	ld.shared.u64 	%rd922, [%r9];
	and.b64  	%rd923, %rd922, %rd921;
	st.shared.u64 	[%r7], %rd923;

$L__BB1_165:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd924, [%r8];
	xor.b64  	%rd925, %rd131, %rd924;
	st.local.u64 	[%rd2], %rd925;
	xor.b64  	%rd926, %rd132, %rd924;
	st.local.u64 	[%rd2+40], %rd926;
	xor.b64  	%rd927, %rd133, %rd924;
	st.local.u64 	[%rd2+80], %rd927;
	xor.b64  	%rd928, %rd134, %rd924;
	st.local.u64 	[%rd2+120], %rd928;
	xor.b64  	%rd929, %rd135, %rd924;
	st.local.u64 	[%rd2+160], %rd929;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_167;

	ld.const.u64 	%rd1107, [CUDA_KECCAK_CONSTS+152];
	ld.local.u64 	%rd930, [%rd1];
	xor.b64  	%rd931, %rd930, %rd1107;
	st.local.u64 	[%rd1], %rd931;

$L__BB1_167:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_169;

	ld.local.u64 	%rd932, [%rd2];
	ld.local.u64 	%rd933, [%rd2+40];
	xor.b64  	%rd934, %rd933, %rd932;
	ld.local.u64 	%rd935, [%rd2+80];
	xor.b64  	%rd936, %rd934, %rd935;
	ld.local.u64 	%rd937, [%rd2+120];
	xor.b64  	%rd938, %rd936, %rd937;
	ld.local.u64 	%rd939, [%rd2+160];
	xor.b64  	%rd940, %rd938, %rd939;
	st.shared.u64 	[%r4], %rd940;

$L__BB1_169:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_171;

	ld.shared.u64 	%rd943, [%r5];
	ld.shared.u64 	%rd942, [%r6];
	mov.u32 	%r96, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd942;
	shf.l.wrap.b32 vl, tl, th, %r96;
	shf.l.wrap.b32 vh, th, tl, %r96;
	setp.lt.u32 p, %r96, 32;
	@!p mov.b64 %rd941, {vl,vh};
	@p  mov.b64 %rd941, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd944, %rd941, %rd943;
	st.shared.u64 	[%r7], %rd944;

$L__BB1_171:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd947, [%rd2];
	ld.shared.u64 	%rd948, [%r8];
	xor.b64  	%rd946, %rd947, %rd948;
	st.local.u64 	[%rd2], %rd946;
	ld.local.u64 	%rd949, [%rd2+40];
	xor.b64  	%rd950, %rd949, %rd948;
	st.local.u64 	[%rd2+40], %rd950;
	ld.local.u64 	%rd951, [%rd2+80];
	xor.b64  	%rd952, %rd951, %rd948;
	st.local.u64 	[%rd2+80], %rd952;
	ld.local.u64 	%rd953, [%rd2+120];
	xor.b64  	%rd954, %rd953, %rd948;
	st.local.u64 	[%rd2+120], %rd954;
	ld.local.u64 	%rd955, [%rd2+160];
	xor.b64  	%rd956, %rd955, %rd948;
	st.local.u64 	[%rd2+160], %rd956;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd946;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd945, {vl,vh};
	@p  mov.b64 %rd945, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd945;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd136, [%rd2];
	ld.local.u64 	%rd137, [%rd2+40];
	ld.local.u64 	%rd138, [%rd2+80];
	ld.local.u64 	%rd139, [%rd2+120];
	ld.local.u64 	%rd140, [%rd2+160];
	@%p8 bra 	$L__BB1_173;

	st.shared.u64 	[%r4], %rd140;
	ld.shared.u64 	%rd957, [%r6];
	not.b64 	%rd958, %rd957;
	ld.shared.u64 	%rd959, [%r9];
	and.b64  	%rd960, %rd959, %rd958;
	st.shared.u64 	[%r7], %rd960;

$L__BB1_173:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd961, [%r8];
	xor.b64  	%rd962, %rd136, %rd961;
	st.local.u64 	[%rd2], %rd962;
	xor.b64  	%rd963, %rd137, %rd961;
	st.local.u64 	[%rd2+40], %rd963;
	xor.b64  	%rd964, %rd138, %rd961;
	st.local.u64 	[%rd2+80], %rd964;
	xor.b64  	%rd965, %rd139, %rd961;
	st.local.u64 	[%rd2+120], %rd965;
	xor.b64  	%rd966, %rd140, %rd961;
	st.local.u64 	[%rd2+160], %rd966;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_175;

	ld.const.u64 	%rd1106, [CUDA_KECCAK_CONSTS+160];
	ld.local.u64 	%rd967, [%rd1];
	xor.b64  	%rd968, %rd967, %rd1106;
	st.local.u64 	[%rd1], %rd968;

$L__BB1_175:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_177;

	ld.local.u64 	%rd969, [%rd2];
	ld.local.u64 	%rd970, [%rd2+40];
	xor.b64  	%rd971, %rd970, %rd969;
	ld.local.u64 	%rd972, [%rd2+80];
	xor.b64  	%rd973, %rd971, %rd972;
	ld.local.u64 	%rd974, [%rd2+120];
	xor.b64  	%rd975, %rd973, %rd974;
	ld.local.u64 	%rd976, [%rd2+160];
	xor.b64  	%rd977, %rd975, %rd976;
	st.shared.u64 	[%r4], %rd977;

$L__BB1_177:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_179;

	ld.shared.u64 	%rd980, [%r5];
	ld.shared.u64 	%rd979, [%r6];
	mov.u32 	%r98, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd979;
	shf.l.wrap.b32 vl, tl, th, %r98;
	shf.l.wrap.b32 vh, th, tl, %r98;
	setp.lt.u32 p, %r98, 32;
	@!p mov.b64 %rd978, {vl,vh};
	@p  mov.b64 %rd978, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd981, %rd978, %rd980;
	st.shared.u64 	[%r7], %rd981;

$L__BB1_179:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd984, [%rd2];
	ld.shared.u64 	%rd985, [%r8];
	xor.b64  	%rd983, %rd984, %rd985;
	st.local.u64 	[%rd2], %rd983;
	ld.local.u64 	%rd986, [%rd2+40];
	xor.b64  	%rd987, %rd986, %rd985;
	st.local.u64 	[%rd2+40], %rd987;
	ld.local.u64 	%rd988, [%rd2+80];
	xor.b64  	%rd989, %rd988, %rd985;
	st.local.u64 	[%rd2+80], %rd989;
	ld.local.u64 	%rd990, [%rd2+120];
	xor.b64  	%rd991, %rd990, %rd985;
	st.local.u64 	[%rd2+120], %rd991;
	ld.local.u64 	%rd992, [%rd2+160];
	xor.b64  	%rd993, %rd992, %rd985;
	st.local.u64 	[%rd2+160], %rd993;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd983;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd982, {vl,vh};
	@p  mov.b64 %rd982, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd982;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd141, [%rd2];
	ld.local.u64 	%rd142, [%rd2+40];
	ld.local.u64 	%rd143, [%rd2+80];
	ld.local.u64 	%rd144, [%rd2+120];
	ld.local.u64 	%rd145, [%rd2+160];
	@%p8 bra 	$L__BB1_181;

	st.shared.u64 	[%r4], %rd145;
	ld.shared.u64 	%rd994, [%r6];
	not.b64 	%rd995, %rd994;
	ld.shared.u64 	%rd996, [%r9];
	and.b64  	%rd997, %rd996, %rd995;
	st.shared.u64 	[%r7], %rd997;

$L__BB1_181:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd998, [%r8];
	xor.b64  	%rd999, %rd141, %rd998;
	st.local.u64 	[%rd2], %rd999;
	xor.b64  	%rd1000, %rd142, %rd998;
	st.local.u64 	[%rd2+40], %rd1000;
	xor.b64  	%rd1001, %rd143, %rd998;
	st.local.u64 	[%rd2+80], %rd1001;
	xor.b64  	%rd1002, %rd144, %rd998;
	st.local.u64 	[%rd2+120], %rd1002;
	xor.b64  	%rd1003, %rd145, %rd998;
	st.local.u64 	[%rd2+160], %rd1003;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_183;

	ld.const.u64 	%rd1116, [CUDA_KECCAK_CONSTS+168];
	ld.local.u64 	%rd1004, [%rd1];
	xor.b64  	%rd1005, %rd1004, %rd1116;
	st.local.u64 	[%rd1], %rd1005;

$L__BB1_183:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_185;

	ld.local.u64 	%rd1006, [%rd2];
	ld.local.u64 	%rd1007, [%rd2+40];
	xor.b64  	%rd1008, %rd1007, %rd1006;
	ld.local.u64 	%rd1009, [%rd2+80];
	xor.b64  	%rd1010, %rd1008, %rd1009;
	ld.local.u64 	%rd1011, [%rd2+120];
	xor.b64  	%rd1012, %rd1010, %rd1011;
	ld.local.u64 	%rd1013, [%rd2+160];
	xor.b64  	%rd1014, %rd1012, %rd1013;
	st.shared.u64 	[%r4], %rd1014;

$L__BB1_185:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_187;

	ld.shared.u64 	%rd1017, [%r5];
	ld.shared.u64 	%rd1016, [%r6];
	mov.u32 	%r100, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1016;
	shf.l.wrap.b32 vl, tl, th, %r100;
	shf.l.wrap.b32 vh, th, tl, %r100;
	setp.lt.u32 p, %r100, 32;
	@!p mov.b64 %rd1015, {vl,vh};
	@p  mov.b64 %rd1015, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd1018, %rd1015, %rd1017;
	st.shared.u64 	[%r7], %rd1018;

$L__BB1_187:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd1021, [%rd2];
	ld.shared.u64 	%rd1022, [%r8];
	xor.b64  	%rd1020, %rd1021, %rd1022;
	st.local.u64 	[%rd2], %rd1020;
	ld.local.u64 	%rd1023, [%rd2+40];
	xor.b64  	%rd1024, %rd1023, %rd1022;
	st.local.u64 	[%rd2+40], %rd1024;
	ld.local.u64 	%rd1025, [%rd2+80];
	xor.b64  	%rd1026, %rd1025, %rd1022;
	st.local.u64 	[%rd2+80], %rd1026;
	ld.local.u64 	%rd1027, [%rd2+120];
	xor.b64  	%rd1028, %rd1027, %rd1022;
	st.local.u64 	[%rd2+120], %rd1028;
	ld.local.u64 	%rd1029, [%rd2+160];
	xor.b64  	%rd1030, %rd1029, %rd1022;
	st.local.u64 	[%rd2+160], %rd1030;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1020;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd1019, {vl,vh};
	@p  mov.b64 %rd1019, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd1019;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd146, [%rd2];
	ld.local.u64 	%rd147, [%rd2+40];
	ld.local.u64 	%rd148, [%rd2+80];
	ld.local.u64 	%rd149, [%rd2+120];
	ld.local.u64 	%rd150, [%rd2+160];
	@%p8 bra 	$L__BB1_189;

	st.shared.u64 	[%r4], %rd150;
	ld.shared.u64 	%rd1031, [%r6];
	not.b64 	%rd1032, %rd1031;
	ld.shared.u64 	%rd1033, [%r9];
	and.b64  	%rd1034, %rd1033, %rd1032;
	st.shared.u64 	[%r7], %rd1034;

$L__BB1_189:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd1035, [%r8];
	xor.b64  	%rd1036, %rd146, %rd1035;
	st.local.u64 	[%rd2], %rd1036;
	xor.b64  	%rd1037, %rd147, %rd1035;
	st.local.u64 	[%rd2+40], %rd1037;
	xor.b64  	%rd1038, %rd148, %rd1035;
	st.local.u64 	[%rd2+80], %rd1038;
	xor.b64  	%rd1039, %rd149, %rd1035;
	st.local.u64 	[%rd2+120], %rd1039;
	xor.b64  	%rd1040, %rd150, %rd1035;
	st.local.u64 	[%rd2+160], %rd1040;
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB1_191;

	ld.const.u64 	%rd1115, [CUDA_KECCAK_CONSTS+176];
	ld.local.u64 	%rd1041, [%rd1];
	xor.b64  	%rd1042, %rd1041, %rd1115;
	st.local.u64 	[%rd1], %rd1042;

$L__BB1_191:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_193;

	ld.local.u64 	%rd1043, [%rd2];
	ld.local.u64 	%rd1044, [%rd2+40];
	xor.b64  	%rd1045, %rd1044, %rd1043;
	ld.local.u64 	%rd1046, [%rd2+80];
	xor.b64  	%rd1047, %rd1045, %rd1046;
	ld.local.u64 	%rd1048, [%rd2+120];
	xor.b64  	%rd1049, %rd1047, %rd1048;
	ld.local.u64 	%rd1050, [%rd2+160];
	xor.b64  	%rd1051, %rd1049, %rd1050;
	st.shared.u64 	[%r4], %rd1051;

$L__BB1_193:
	bar.warp.sync 	-1;
	@%p8 bra 	$L__BB1_195;

	ld.shared.u64 	%rd1054, [%r5];
	ld.shared.u64 	%rd1053, [%r6];
	mov.u32 	%r102, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1053;
	shf.l.wrap.b32 vl, tl, th, %r102;
	shf.l.wrap.b32 vh, th, tl, %r102;
	setp.lt.u32 p, %r102, 32;
	@!p mov.b64 %rd1052, {vl,vh};
	@p  mov.b64 %rd1052, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd1055, %rd1052, %rd1054;
	st.shared.u64 	[%r7], %rd1055;

$L__BB1_195:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd1058, [%rd2];
	ld.shared.u64 	%rd1059, [%r8];
	xor.b64  	%rd1057, %rd1058, %rd1059;
	st.local.u64 	[%rd2], %rd1057;
	ld.local.u64 	%rd1060, [%rd2+40];
	xor.b64  	%rd1061, %rd1060, %rd1059;
	st.local.u64 	[%rd2+40], %rd1061;
	ld.local.u64 	%rd1062, [%rd2+80];
	xor.b64  	%rd1063, %rd1062, %rd1059;
	st.local.u64 	[%rd2+80], %rd1063;
	ld.local.u64 	%rd1064, [%rd2+120];
	xor.b64  	%rd1065, %rd1064, %rd1059;
	st.local.u64 	[%rd2+120], %rd1065;
	ld.local.u64 	%rd1066, [%rd2+160];
	xor.b64  	%rd1067, %rd1066, %rd1059;
	st.local.u64 	[%rd2+160], %rd1067;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1057;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd1056, {vl,vh};
	@p  mov.b64 %rd1056, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd35], %rd1056;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd151, [%rd2];
	ld.local.u64 	%rd152, [%rd2+40];
	ld.local.u64 	%rd153, [%rd2+80];
	ld.local.u64 	%rd154, [%rd2+120];
	ld.local.u64 	%rd155, [%rd2+160];
	@%p8 bra 	$L__BB1_197;

	st.shared.u64 	[%r4], %rd155;
	ld.shared.u64 	%rd1068, [%r6];
	not.b64 	%rd1069, %rd1068;
	ld.shared.u64 	%rd1070, [%r9];
	and.b64  	%rd1071, %rd1070, %rd1069;
	st.shared.u64 	[%r7], %rd1071;

$L__BB1_197:
	setp.eq.s32 	%p103, %r1, 0;
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd1072, [%r8];
	xor.b64  	%rd1073, %rd151, %rd1072;
	st.local.u64 	[%rd2], %rd1073;
	xor.b64  	%rd1074, %rd152, %rd1072;
	st.local.u64 	[%rd2+40], %rd1074;
	xor.b64  	%rd1075, %rd153, %rd1072;
	st.local.u64 	[%rd2+80], %rd1075;
	xor.b64  	%rd1076, %rd154, %rd1072;
	st.local.u64 	[%rd2+120], %rd1076;
	xor.b64  	%rd1077, %rd155, %rd1072;
	st.local.u64 	[%rd2+160], %rd1077;
	bar.warp.sync 	-1;
	@%p103 bra 	$L__BB1_199;

	ld.local.v4.u16 	{%rs101, %rs103, %rs105, %rs107}, [%rd1];
	shr.u16 	%rs102, %rs101, 8;
	shr.u16 	%rs104, %rs103, 8;
	shr.u16 	%rs106, %rs105, 8;
	shr.u16 	%rs108, %rs107, 8;
	bra.uni 	$L__BB1_200;

$L__BB1_199:
	ld.const.u64 	%rd1114, [CUDA_KECCAK_CONSTS+184];
	ld.local.u64 	%rd1078, [%rd1];
	xor.b64  	%rd1079, %rd1078, %rd1114;
	st.local.u64 	[%rd1], %rd1079;
	shr.u64 	%rd1080, %rd1079, 56;
	cvt.u16.u64 	%rs108, %rd1080;
	shr.u64 	%rd1081, %rd1079, 48;
	cvt.u16.u64 	%rs107, %rd1081;
	shr.u64 	%rd1082, %rd1079, 40;
	cvt.u16.u64 	%rs106, %rd1082;
	shr.u64 	%rd1083, %rd1079, 32;
	cvt.u16.u64 	%rs105, %rd1083;
	shr.u64 	%rd1084, %rd1079, 24;
	cvt.u16.u64 	%rs104, %rd1084;
	shr.u64 	%rd1085, %rd1079, 16;
	cvt.u16.u64 	%rs103, %rd1085;
	shr.u64 	%rd1086, %rd1079, 8;
	cvt.u16.u64 	%rs102, %rd1086;
	cvt.u16.u64 	%rs31, %rd1114;
	cvt.u16.u64 	%rs32, %rd1078;
	xor.b16  	%rs101, %rs32, %rs31;

$L__BB1_200:
	ld.param.u64 	%rd1113, [kernel_lilypad_pow_param_2];
	bar.warp.sync 	-1;
	ld.local.v4.u16 	{%rs33, %rs34, %rs35, %rs36}, [%rd1+24];
	shl.b16 	%rs39, %rs34, 8;
	shr.u16 	%rs40, %rs34, 8;
	shl.b16 	%rs43, %rs36, 8;
	shr.u16 	%rs44, %rs36, 8;
	ld.local.v4.u16 	{%rs45, %rs46, %rs47, %rs48}, [%rd1+16];
	shl.b16 	%rs50, %rs45, 8;
	shr.u16 	%rs51, %rs45, 8;
	shl.b16 	%rs53, %rs46, 8;
	shr.u16 	%rs54, %rs46, 8;
	shl.b16 	%rs57, %rs48, 8;
	shr.u16 	%rs58, %rs48, 8;
	shl.b16 	%rs59, %rs35, 8;
	shr.u16 	%rs60, %rs35, 8;
	shl.b16 	%rs61, %rs33, 8;
	shr.u16 	%rs62, %rs33, 8;
	shl.b16 	%rs63, %rs47, 8;
	shr.u16 	%rs64, %rs47, 8;
	or.b16  	%rs65, %rs60, %rs59;
	or.b16  	%rs66, %rs44, %rs43;
	or.b16  	%rs67, %rs62, %rs61;
	or.b16  	%rs68, %rs40, %rs39;
	or.b16  	%rs69, %rs64, %rs63;
	or.b16  	%rs70, %rs58, %rs57;
	or.b16  	%rs71, %rs51, %rs50;
	or.b16  	%rs72, %rs54, %rs53;
	add.u64 	%rd1087, %SP, 208;
	add.u64 	%rd1088, %SPL, 208;
	mov.b32 	%r104, {%rs72, %rs71};
	mov.b32 	%r105, {%rs70, %rs69};
	mov.b32 	%r106, {%rs68, %rs67};
	mov.b32 	%r107, {%rs66, %rs65};
	st.local.v4.u32 	[%rd1088], {%r107, %r106, %r105, %r104};
	ld.local.v4.u16 	{%rs73, %rs74, %rs75, %rs76}, [%rd1+8];
	shl.b16 	%rs79, %rs74, 8;
	shr.u16 	%rs80, %rs74, 8;
	shl.b16 	%rs83, %rs76, 8;
	shr.u16 	%rs84, %rs76, 8;
	shl.b16 	%rs85, %rs75, 8;
	shr.u16 	%rs86, %rs75, 8;
	shl.b16 	%rs87, %rs73, 8;
	shr.u16 	%rs88, %rs73, 8;
	or.b16  	%rs89, %rs86, %rs85;
	or.b16  	%rs90, %rs84, %rs83;
	or.b16  	%rs91, %rs88, %rs87;
	or.b16  	%rs92, %rs80, %rs79;
	shl.b16 	%rs93, %rs107, 8;
	or.b16  	%rs94, %rs108, %rs93;
	cvt.u32.u16 	%r108, %rs105;
	and.b16  	%rs95, %rs106, 255;
	cvt.u32.u16 	%r109, %rs95;
	prmt.b32 	%r110, %r108, %r109, 30212;
	cvt.u16.u32 	%rs96, %r110;
	cvt.u32.u16 	%r111, %rs103;
	and.b16  	%rs97, %rs104, 255;
	cvt.u32.u16 	%r112, %rs97;
	prmt.b32 	%r113, %r111, %r112, 30212;
	cvt.u32.u16 	%r114, %rs101;
	and.b16  	%rs98, %rs102, 255;
	cvt.u32.u16 	%r115, %rs98;
	prmt.b32 	%r116, %r114, %r115, 30212;
	prmt.b32 	%r117, %r116, %r113, 4180;
	mov.b32 	%r118, {%rs94, %rs96};
	mov.b32 	%r119, {%rs92, %rs91};
	mov.b32 	%r120, {%rs90, %rs89};
	st.local.v4.u32 	[%rd1088+16], {%r120, %r119, %r118, %r117};
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1087;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd1113;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r121, [retval0+0];
	} // callseq 1
	cvt.u16.u32 	%rs99, %r121;
	setp.eq.s16 	%p104, %rs99, 0;
	@%p104 bra 	$L__BB1_203;

	mov.u64 	%rd1119, 0;

$L__BB1_202:
	add.s64 	%rd1090, %rd29, %rd1119;
	ld.u8 	%rs100, [%rd1090];
	add.s64 	%rd1091, %rd156, %rd1119;
	st.global.u8 	[%rd1091], %rs100;
	add.s64 	%rd1119, %rd1119, 1;
	setp.lt.u64 	%p105, %rd1119, 32;
	@%p105 bra 	$L__BB1_202;

$L__BB1_203:
	ld.param.u32 	%r126, [kernel_lilypad_pow_param_4];
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd29;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 2
	mad.lo.s32 	%r125, %r2, %r126, %r126;
	add.s32 	%r127, %r127, 1;
	setp.lt.u32 	%p106, %r127, %r125;
	@%p106 bra 	$L__BB1_3;

$L__BB1_204:
	ret;

}
	// .globl	kernel_lilypad_pow_debug
.visible .entry kernel_lilypad_pow_debug(
	.param .u64 kernel_lilypad_pow_debug_param_0,
	.param .u64 kernel_lilypad_pow_debug_param_1,
	.param .u64 kernel_lilypad_pow_debug_param_2,
	.param .u32 kernel_lilypad_pow_debug_param_3,
	.param .u32 kernel_lilypad_pow_debug_param_4,
	.param .u64 kernel_lilypad_pow_debug_param_5,
	.param .u64 kernel_lilypad_pow_debug_param_6,
	.param .u64 kernel_lilypad_pow_debug_param_7
)
.maxntid 1024, 1, 1
.minnctapersm 1
{
	.local .align 16 .b8 	__local_depot2[240];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<113>;
	.reg .b16 	%rs<573>;
	.reg .b32 	%r<136>;
	.reg .b64 	%rd<1135>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd167, [kernel_lilypad_pow_debug_param_0];
	ld.param.u64 	%rd168, [kernel_lilypad_pow_debug_param_1];
	ld.param.u32 	%r16, [kernel_lilypad_pow_debug_param_3];
	ld.param.u32 	%r15, [kernel_lilypad_pow_debug_param_4];
	ld.param.u64 	%rd170, [kernel_lilypad_pow_debug_param_5];
	ld.param.u64 	%rd171, [kernel_lilypad_pow_debug_param_6];
	ld.param.u64 	%rd172, [kernel_lilypad_pow_debug_param_7];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r18, %r17, %r1;
	setp.ge.u32 	%p1, %r2, %r16;
	setp.gt.u32 	%p2, %r2, 24;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB2_209;

	mul.lo.s32 	%r134, %r2, %r15;
	add.s32 	%r19, %r134, %r15;
	setp.ge.u32 	%p4, %r134, %r19;
	@%p4 bra 	$L__BB2_209;

	mul.wide.s32 	%rd175, %r1, 8;
	add.s64 	%rd2, %rd1, %rd175;
	shl.b32 	%r20, %r1, 3;
	mov.u32 	%r21, _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE1C;
	add.s32 	%r4, %r21, %r20;
	add.s32 	%r22, %r1, 4;
	mul.hi.s32 	%r23, %r22, 1717986919;
	shr.u32 	%r24, %r23, 31;
	shr.u32 	%r25, %r23, 1;
	add.s32 	%r26, %r25, %r24;
	mul.lo.s32 	%r27, %r26, 5;
	sub.s32 	%r28, %r22, %r27;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r5, %r21, %r29;
	add.s32 	%r30, %r1, 1;
	mul.hi.s32 	%r31, %r30, 1717986919;
	shr.u32 	%r32, %r31, 31;
	shr.u32 	%r33, %r31, 1;
	add.s32 	%r34, %r33, %r32;
	mul.lo.s32 	%r35, %r34, 5;
	sub.s32 	%r36, %r30, %r35;
	shl.b32 	%r37, %r36, 3;
	add.s32 	%r6, %r21, %r37;
	mov.u32 	%r38, _ZZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e24cuda_keccak_permutationsEPyE4temp;
	add.s32 	%r7, %r38, %r20;
	mul.hi.s32 	%r39, %r1, 1717986919;
	shr.u32 	%r40, %r39, 31;
	shr.u32 	%r41, %r39, 1;
	add.s32 	%r42, %r41, %r40;
	mul.lo.s32 	%r43, %r42, 5;
	sub.s32 	%r44, %r1, %r43;
	shl.b32 	%r45, %r44, 3;
	add.s32 	%r8, %r38, %r45;
	mul.wide.s32 	%rd176, %r1, 4;
	mov.u64 	%rd177, piln;
	add.s64 	%rd3, %rd177, %rd176;
	mov.u64 	%rd178, r;
	add.s64 	%rd4, %rd178, %rd176;
	add.s32 	%r46, %r1, 2;
	mul.hi.s32 	%r47, %r46, 1717986919;
	shr.u32 	%r48, %r47, 31;
	shr.u32 	%r49, %r47, 1;
	add.s32 	%r50, %r49, %r48;
	mul.lo.s32 	%r51, %r50, 5;
	sub.s32 	%r52, %r46, %r51;
	shl.b32 	%r53, %r52, 3;
	add.s32 	%r9, %r21, %r53;
	cvta.to.global.u64 	%rd182, %rd168;
	cvta.to.global.u64 	%rd32, %rd167;
	ld.const.u32 	%r61, [%rd3];
	ld.const.u32 	%r13, [%rd4];
	cvta.to.global.u64 	%rd161, %rd171;
	cvta.to.global.u64 	%rd1097, %rd172;
	cvta.to.global.u64 	%rd164, %rd170;

$L__BB2_3:
	setp.ne.s32 	%p5, %r2, 0;
	@%p5 bra 	$L__BB2_9;

	cvt.s64.s32 	%rd180, %r134;
	mov.u64 	%rd181, 32;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd181;
	.param .b64 retval0;
	call.uni (retval0), 
	malloc, 
	(
	param0
	);
	ld.param.b64 	%rd1132, [retval0+0];
	} // callseq 3
	ld.global.nc.u64 	%rd183, [%rd182];
	add.s64 	%rd184, %rd183, %rd180;
	setp.lt.u64 	%p6, %rd184, %rd183;
	mov.u64 	%rd1130, 0;
	st.u64 	[%rd1132], %rd184;
	selp.u64 	%rd185, 1, 0, %p6;
	ld.global.nc.u64 	%rd186, [%rd182+8];
	add.s64 	%rd187, %rd186, %rd185;
	setp.lt.u64 	%p7, %rd187, %rd186;
	st.u64 	[%rd1132+8], %rd187;
	selp.u64 	%rd188, 1, 0, %p7;
	ld.global.nc.u64 	%rd189, [%rd182+16];
	add.s64 	%rd190, %rd189, %rd188;
	setp.lt.u64 	%p8, %rd190, %rd189;
	st.u64 	[%rd1132+16], %rd190;
	selp.u64 	%rd191, 1, 0, %p8;
	ld.global.nc.u64 	%rd192, [%rd182+24];
	add.s64 	%rd193, %rd192, %rd191;
	st.u64 	[%rd1132+24], %rd193;
	mov.u32 	%r135, 0;
	st.local.v2.u32 	[%rd1+72], {%r135, %r135};
	st.local.v2.u32 	[%rd1+80], {%r135, %r135};
	st.local.v2.u32 	[%rd1+88], {%r135, %r135};
	st.local.v2.u32 	[%rd1+96], {%r135, %r135};
	st.local.v2.u32 	[%rd1+104], {%r135, %r135};
	st.local.v2.u32 	[%rd1+112], {%r135, %r135};
	st.local.v2.u32 	[%rd1+120], {%r135, %r135};
	st.local.v2.u32 	[%rd1+128], {%r135, %r135};
	st.local.v2.u32 	[%rd1+136], {%r135, %r135};
	st.local.v2.u32 	[%rd1+144], {%r135, %r135};
	st.local.v2.u32 	[%rd1+152], {%r135, %r135};
	st.local.v2.u32 	[%rd1+160], {%r135, %r135};
	st.local.v2.u32 	[%rd1+168], {%r135, %r135};
	st.local.v2.u32 	[%rd1+176], {%r135, %r135};
	st.local.v2.u32 	[%rd1+184], {%r135, %r135};
	st.local.v2.u32 	[%rd1+192], {%r135, %r135};

$L__BB2_5:
	add.s64 	%rd194, %rd32, %rd1130;
	ld.global.nc.u8 	%rs337, [%rd194];
	add.s64 	%rd195, %rd1, %rd1130;
	st.local.u8 	[%rd195], %rs337;
	add.s64 	%rd1130, %rd1130, 1;
	add.s32 	%r135, %r135, 1;
	setp.lt.u32 	%p9, %r135, 32;
	@%p9 bra 	$L__BB2_5;

	add.s64 	%rd35, %rd1, 32;
	mov.u64 	%rd1131, 0;

$L__BB2_7:
	add.s64 	%rd197, %rd1132, %rd1131;
	ld.u8 	%rs338, [%rd197];
	add.s64 	%rd198, %rd35, %rd1131;
	st.local.u8 	[%rd198], %rs338;
	add.s64 	%rd1131, %rd1131, 1;
	setp.lt.u64 	%p10, %rd1131, 32;
	@%p10 bra 	$L__BB2_7;

	ld.local.v4.u16 	{%rs509, %rs511, %rs513, %rs515}, [%rd1];
	mov.u64 	%rd199, 1;
	ld.local.v4.u16 	{%rs343, %rs519, %rs521, %rs523}, [%rd1+8];
	ld.local.v4.u16 	{%rs348, %rs527, %rs529, %rs531}, [%rd1+16];
	ld.local.v4.u16 	{%rs353, %rs535, %rs537, %rs539}, [%rd1+24];
	ld.local.v4.u16 	{%rs358, %rs543, %rs545, %rs547}, [%rd1+32];
	ld.local.v4.u16 	{%rs363, %rs551, %rs553, %rs555}, [%rd1+40];
	ld.local.v4.u16 	{%rs368, %rs559, %rs561, %rs563}, [%rd1+48];
	ld.local.v4.u16 	{%rs373, %rs567, %rs569, %rs571}, [%rd1+56];
	ld.local.u8 	%rs565, [%rd1+56];
	ld.local.u8 	%rs557, [%rd1+48];
	ld.local.u8 	%rs549, [%rd1+40];
	ld.local.u8 	%rs541, [%rd1+32];
	ld.local.u8 	%rs533, [%rd1+24];
	ld.local.u8 	%rs525, [%rd1+16];
	ld.local.u8 	%rs517, [%rd1+8];
	shr.u16 	%rs572, %rs571, 8;
	shr.u16 	%rs570, %rs569, 8;
	shr.u16 	%rs568, %rs567, 8;
	shr.u16 	%rs566, %rs373, 8;
	shr.u16 	%rs564, %rs563, 8;
	shr.u16 	%rs562, %rs561, 8;
	shr.u16 	%rs560, %rs559, 8;
	shr.u16 	%rs558, %rs368, 8;
	shr.u16 	%rs556, %rs555, 8;
	shr.u16 	%rs554, %rs553, 8;
	shr.u16 	%rs552, %rs551, 8;
	shr.u16 	%rs550, %rs363, 8;
	shr.u16 	%rs548, %rs547, 8;
	shr.u16 	%rs546, %rs545, 8;
	shr.u16 	%rs544, %rs543, 8;
	shr.u16 	%rs542, %rs358, 8;
	shr.u16 	%rs540, %rs539, 8;
	shr.u16 	%rs538, %rs537, 8;
	shr.u16 	%rs536, %rs535, 8;
	shr.u16 	%rs534, %rs353, 8;
	shr.u16 	%rs532, %rs531, 8;
	shr.u16 	%rs530, %rs529, 8;
	shr.u16 	%rs528, %rs527, 8;
	shr.u16 	%rs526, %rs348, 8;
	shr.u16 	%rs524, %rs523, 8;
	shr.u16 	%rs522, %rs521, 8;
	shr.u16 	%rs520, %rs519, 8;
	shr.u16 	%rs518, %rs343, 8;
	shr.u16 	%rs516, %rs515, 8;
	shr.u16 	%rs514, %rs513, 8;
	shr.u16 	%rs512, %rs511, 8;
	shr.u16 	%rs510, %rs509, 8;
	st.local.u64 	[%rd1+64], %rd199;
	mov.u64 	%rd200, -9223372036854775808;
	st.local.u64 	[%rd1+128], %rd200;

$L__BB2_9:
	setp.gt.s32 	%p11, %r1, 4;
	@%p11 bra 	$L__BB2_11;

	ld.local.u64 	%rd201, [%rd2];
	ld.local.u64 	%rd202, [%rd2+40];
	xor.b64  	%rd203, %rd202, %rd201;
	ld.local.u64 	%rd204, [%rd2+80];
	xor.b64  	%rd205, %rd203, %rd204;
	ld.local.u64 	%rd206, [%rd2+120];
	xor.b64  	%rd207, %rd205, %rd206;
	ld.local.u64 	%rd208, [%rd2+160];
	xor.b64  	%rd209, %rd207, %rd208;
	st.shared.u64 	[%r4], %rd209;

$L__BB2_11:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_13;

	ld.shared.u64 	%rd212, [%r5];
	ld.shared.u64 	%rd211, [%r6];
	mov.u32 	%r59, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd211;
	shf.l.wrap.b32 vl, tl, th, %r59;
	shf.l.wrap.b32 vh, th, tl, %r59;
	setp.lt.u32 p, %r59, 32;
	@!p mov.b64 %rd210, {vl,vh};
	@p  mov.b64 %rd210, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd213, %rd210, %rd212;
	st.shared.u64 	[%r7], %rd213;

$L__BB2_13:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd216, [%rd2];
	ld.shared.u64 	%rd217, [%r8];
	xor.b64  	%rd215, %rd216, %rd217;
	st.local.u64 	[%rd2], %rd215;
	ld.local.u64 	%rd218, [%rd2+40];
	xor.b64  	%rd219, %rd218, %rd217;
	st.local.u64 	[%rd2+40], %rd219;
	ld.local.u64 	%rd220, [%rd2+80];
	xor.b64  	%rd221, %rd220, %rd217;
	st.local.u64 	[%rd2+80], %rd221;
	ld.local.u64 	%rd222, [%rd2+120];
	xor.b64  	%rd223, %rd222, %rd217;
	st.local.u64 	[%rd2+120], %rd223;
	ld.local.u64 	%rd224, [%rd2+160];
	xor.b64  	%rd225, %rd224, %rd217;
	st.local.u64 	[%rd2+160], %rd225;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd215;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd214, {vl,vh};
	@p  mov.b64 %rd214, {vh,vl};
	}

	// end inline asm
	mul.wide.s32 	%rd226, %r61, 8;
	add.s64 	%rd39, %rd1, %rd226;
	st.local.u64 	[%rd39], %rd214;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd40, [%rd2];
	ld.local.u64 	%rd41, [%rd2+40];
	ld.local.u64 	%rd42, [%rd2+80];
	ld.local.u64 	%rd43, [%rd2+120];
	ld.local.u64 	%rd44, [%rd2+160];
	@%p11 bra 	$L__BB2_15;

	st.shared.u64 	[%r4], %rd44;
	ld.shared.u64 	%rd227, [%r6];
	not.b64 	%rd228, %rd227;
	ld.shared.u64 	%rd229, [%r9];
	and.b64  	%rd230, %rd229, %rd228;
	st.shared.u64 	[%r7], %rd230;

$L__BB2_15:
	setp.ne.s32 	%p14, %r1, 0;
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd231, [%r8];
	xor.b64  	%rd232, %rd40, %rd231;
	st.local.u64 	[%rd2], %rd232;
	xor.b64  	%rd233, %rd41, %rd231;
	st.local.u64 	[%rd2+40], %rd233;
	xor.b64  	%rd234, %rd42, %rd231;
	st.local.u64 	[%rd2+80], %rd234;
	xor.b64  	%rd235, %rd43, %rd231;
	st.local.u64 	[%rd2+120], %rd235;
	xor.b64  	%rd236, %rd44, %rd231;
	st.local.u64 	[%rd2+160], %rd236;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_17;

	ld.const.u64 	%rd1115, [CUDA_KECCAK_CONSTS];
	ld.local.u64 	%rd237, [%rd1];
	xor.b64  	%rd238, %rd237, %rd1115;
	st.local.u64 	[%rd1], %rd238;

$L__BB2_17:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_19;

	ld.local.u64 	%rd239, [%rd2];
	ld.local.u64 	%rd240, [%rd2+40];
	xor.b64  	%rd241, %rd240, %rd239;
	ld.local.u64 	%rd242, [%rd2+80];
	xor.b64  	%rd243, %rd241, %rd242;
	ld.local.u64 	%rd244, [%rd2+120];
	xor.b64  	%rd245, %rd243, %rd244;
	ld.local.u64 	%rd246, [%rd2+160];
	xor.b64  	%rd247, %rd245, %rd246;
	st.shared.u64 	[%r4], %rd247;

$L__BB2_19:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_21;

	ld.shared.u64 	%rd250, [%r5];
	ld.shared.u64 	%rd249, [%r6];
	mov.u32 	%r62, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd249;
	shf.l.wrap.b32 vl, tl, th, %r62;
	shf.l.wrap.b32 vh, th, tl, %r62;
	setp.lt.u32 p, %r62, 32;
	@!p mov.b64 %rd248, {vl,vh};
	@p  mov.b64 %rd248, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd251, %rd248, %rd250;
	st.shared.u64 	[%r7], %rd251;

$L__BB2_21:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd254, [%rd2];
	ld.shared.u64 	%rd255, [%r8];
	xor.b64  	%rd253, %rd254, %rd255;
	st.local.u64 	[%rd2], %rd253;
	ld.local.u64 	%rd256, [%rd2+40];
	xor.b64  	%rd257, %rd256, %rd255;
	st.local.u64 	[%rd2+40], %rd257;
	ld.local.u64 	%rd258, [%rd2+80];
	xor.b64  	%rd259, %rd258, %rd255;
	st.local.u64 	[%rd2+80], %rd259;
	ld.local.u64 	%rd260, [%rd2+120];
	xor.b64  	%rd261, %rd260, %rd255;
	st.local.u64 	[%rd2+120], %rd261;
	ld.local.u64 	%rd262, [%rd2+160];
	xor.b64  	%rd263, %rd262, %rd255;
	st.local.u64 	[%rd2+160], %rd263;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd253;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd252, {vl,vh};
	@p  mov.b64 %rd252, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd252;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd45, [%rd2];
	ld.local.u64 	%rd46, [%rd2+40];
	ld.local.u64 	%rd47, [%rd2+80];
	ld.local.u64 	%rd48, [%rd2+120];
	ld.local.u64 	%rd49, [%rd2+160];
	@%p11 bra 	$L__BB2_23;

	st.shared.u64 	[%r4], %rd49;
	ld.shared.u64 	%rd264, [%r6];
	not.b64 	%rd265, %rd264;
	ld.shared.u64 	%rd266, [%r9];
	and.b64  	%rd267, %rd266, %rd265;
	st.shared.u64 	[%r7], %rd267;

$L__BB2_23:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd268, [%r8];
	xor.b64  	%rd269, %rd45, %rd268;
	st.local.u64 	[%rd2], %rd269;
	xor.b64  	%rd270, %rd46, %rd268;
	st.local.u64 	[%rd2+40], %rd270;
	xor.b64  	%rd271, %rd47, %rd268;
	st.local.u64 	[%rd2+80], %rd271;
	xor.b64  	%rd272, %rd48, %rd268;
	st.local.u64 	[%rd2+120], %rd272;
	xor.b64  	%rd273, %rd49, %rd268;
	st.local.u64 	[%rd2+160], %rd273;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_25;

	ld.const.u64 	%rd1114, [CUDA_KECCAK_CONSTS+8];
	ld.local.u64 	%rd274, [%rd1];
	xor.b64  	%rd275, %rd274, %rd1114;
	st.local.u64 	[%rd1], %rd275;

$L__BB2_25:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_27;

	ld.local.u64 	%rd276, [%rd2];
	ld.local.u64 	%rd277, [%rd2+40];
	xor.b64  	%rd278, %rd277, %rd276;
	ld.local.u64 	%rd279, [%rd2+80];
	xor.b64  	%rd280, %rd278, %rd279;
	ld.local.u64 	%rd281, [%rd2+120];
	xor.b64  	%rd282, %rd280, %rd281;
	ld.local.u64 	%rd283, [%rd2+160];
	xor.b64  	%rd284, %rd282, %rd283;
	st.shared.u64 	[%r4], %rd284;

$L__BB2_27:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_29;

	ld.shared.u64 	%rd287, [%r5];
	ld.shared.u64 	%rd286, [%r6];
	mov.u32 	%r64, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd286;
	shf.l.wrap.b32 vl, tl, th, %r64;
	shf.l.wrap.b32 vh, th, tl, %r64;
	setp.lt.u32 p, %r64, 32;
	@!p mov.b64 %rd285, {vl,vh};
	@p  mov.b64 %rd285, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd288, %rd285, %rd287;
	st.shared.u64 	[%r7], %rd288;

$L__BB2_29:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd291, [%rd2];
	ld.shared.u64 	%rd292, [%r8];
	xor.b64  	%rd290, %rd291, %rd292;
	st.local.u64 	[%rd2], %rd290;
	ld.local.u64 	%rd293, [%rd2+40];
	xor.b64  	%rd294, %rd293, %rd292;
	st.local.u64 	[%rd2+40], %rd294;
	ld.local.u64 	%rd295, [%rd2+80];
	xor.b64  	%rd296, %rd295, %rd292;
	st.local.u64 	[%rd2+80], %rd296;
	ld.local.u64 	%rd297, [%rd2+120];
	xor.b64  	%rd298, %rd297, %rd292;
	st.local.u64 	[%rd2+120], %rd298;
	ld.local.u64 	%rd299, [%rd2+160];
	xor.b64  	%rd300, %rd299, %rd292;
	st.local.u64 	[%rd2+160], %rd300;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd290;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd289, {vl,vh};
	@p  mov.b64 %rd289, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd289;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd50, [%rd2];
	ld.local.u64 	%rd51, [%rd2+40];
	ld.local.u64 	%rd52, [%rd2+80];
	ld.local.u64 	%rd53, [%rd2+120];
	ld.local.u64 	%rd54, [%rd2+160];
	@%p11 bra 	$L__BB2_31;

	st.shared.u64 	[%r4], %rd54;
	ld.shared.u64 	%rd301, [%r6];
	not.b64 	%rd302, %rd301;
	ld.shared.u64 	%rd303, [%r9];
	and.b64  	%rd304, %rd303, %rd302;
	st.shared.u64 	[%r7], %rd304;

$L__BB2_31:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd305, [%r8];
	xor.b64  	%rd306, %rd50, %rd305;
	st.local.u64 	[%rd2], %rd306;
	xor.b64  	%rd307, %rd51, %rd305;
	st.local.u64 	[%rd2+40], %rd307;
	xor.b64  	%rd308, %rd52, %rd305;
	st.local.u64 	[%rd2+80], %rd308;
	xor.b64  	%rd309, %rd53, %rd305;
	st.local.u64 	[%rd2+120], %rd309;
	xor.b64  	%rd310, %rd54, %rd305;
	st.local.u64 	[%rd2+160], %rd310;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_33;

	ld.const.u64 	%rd1113, [CUDA_KECCAK_CONSTS+16];
	ld.local.u64 	%rd311, [%rd1];
	xor.b64  	%rd312, %rd311, %rd1113;
	st.local.u64 	[%rd1], %rd312;

$L__BB2_33:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_35;

	ld.local.u64 	%rd313, [%rd2];
	ld.local.u64 	%rd314, [%rd2+40];
	xor.b64  	%rd315, %rd314, %rd313;
	ld.local.u64 	%rd316, [%rd2+80];
	xor.b64  	%rd317, %rd315, %rd316;
	ld.local.u64 	%rd318, [%rd2+120];
	xor.b64  	%rd319, %rd317, %rd318;
	ld.local.u64 	%rd320, [%rd2+160];
	xor.b64  	%rd321, %rd319, %rd320;
	st.shared.u64 	[%r4], %rd321;

$L__BB2_35:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_37;

	ld.shared.u64 	%rd324, [%r5];
	ld.shared.u64 	%rd323, [%r6];
	mov.u32 	%r66, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd323;
	shf.l.wrap.b32 vl, tl, th, %r66;
	shf.l.wrap.b32 vh, th, tl, %r66;
	setp.lt.u32 p, %r66, 32;
	@!p mov.b64 %rd322, {vl,vh};
	@p  mov.b64 %rd322, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd325, %rd322, %rd324;
	st.shared.u64 	[%r7], %rd325;

$L__BB2_37:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd328, [%rd2];
	ld.shared.u64 	%rd329, [%r8];
	xor.b64  	%rd327, %rd328, %rd329;
	st.local.u64 	[%rd2], %rd327;
	ld.local.u64 	%rd330, [%rd2+40];
	xor.b64  	%rd331, %rd330, %rd329;
	st.local.u64 	[%rd2+40], %rd331;
	ld.local.u64 	%rd332, [%rd2+80];
	xor.b64  	%rd333, %rd332, %rd329;
	st.local.u64 	[%rd2+80], %rd333;
	ld.local.u64 	%rd334, [%rd2+120];
	xor.b64  	%rd335, %rd334, %rd329;
	st.local.u64 	[%rd2+120], %rd335;
	ld.local.u64 	%rd336, [%rd2+160];
	xor.b64  	%rd337, %rd336, %rd329;
	st.local.u64 	[%rd2+160], %rd337;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd327;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd326, {vl,vh};
	@p  mov.b64 %rd326, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd326;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd55, [%rd2];
	ld.local.u64 	%rd56, [%rd2+40];
	ld.local.u64 	%rd57, [%rd2+80];
	ld.local.u64 	%rd58, [%rd2+120];
	ld.local.u64 	%rd59, [%rd2+160];
	@%p11 bra 	$L__BB2_39;

	st.shared.u64 	[%r4], %rd59;
	ld.shared.u64 	%rd338, [%r6];
	not.b64 	%rd339, %rd338;
	ld.shared.u64 	%rd340, [%r9];
	and.b64  	%rd341, %rd340, %rd339;
	st.shared.u64 	[%r7], %rd341;

$L__BB2_39:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd342, [%r8];
	xor.b64  	%rd343, %rd55, %rd342;
	st.local.u64 	[%rd2], %rd343;
	xor.b64  	%rd344, %rd56, %rd342;
	st.local.u64 	[%rd2+40], %rd344;
	xor.b64  	%rd345, %rd57, %rd342;
	st.local.u64 	[%rd2+80], %rd345;
	xor.b64  	%rd346, %rd58, %rd342;
	st.local.u64 	[%rd2+120], %rd346;
	xor.b64  	%rd347, %rd59, %rd342;
	st.local.u64 	[%rd2+160], %rd347;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_41;

	ld.const.u64 	%rd1112, [CUDA_KECCAK_CONSTS+24];
	ld.local.u64 	%rd348, [%rd1];
	xor.b64  	%rd349, %rd348, %rd1112;
	st.local.u64 	[%rd1], %rd349;

$L__BB2_41:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_43;

	ld.local.u64 	%rd350, [%rd2];
	ld.local.u64 	%rd351, [%rd2+40];
	xor.b64  	%rd352, %rd351, %rd350;
	ld.local.u64 	%rd353, [%rd2+80];
	xor.b64  	%rd354, %rd352, %rd353;
	ld.local.u64 	%rd355, [%rd2+120];
	xor.b64  	%rd356, %rd354, %rd355;
	ld.local.u64 	%rd357, [%rd2+160];
	xor.b64  	%rd358, %rd356, %rd357;
	st.shared.u64 	[%r4], %rd358;

$L__BB2_43:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_45;

	ld.shared.u64 	%rd361, [%r5];
	ld.shared.u64 	%rd360, [%r6];
	mov.u32 	%r68, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd360;
	shf.l.wrap.b32 vl, tl, th, %r68;
	shf.l.wrap.b32 vh, th, tl, %r68;
	setp.lt.u32 p, %r68, 32;
	@!p mov.b64 %rd359, {vl,vh};
	@p  mov.b64 %rd359, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd362, %rd359, %rd361;
	st.shared.u64 	[%r7], %rd362;

$L__BB2_45:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd365, [%rd2];
	ld.shared.u64 	%rd366, [%r8];
	xor.b64  	%rd364, %rd365, %rd366;
	st.local.u64 	[%rd2], %rd364;
	ld.local.u64 	%rd367, [%rd2+40];
	xor.b64  	%rd368, %rd367, %rd366;
	st.local.u64 	[%rd2+40], %rd368;
	ld.local.u64 	%rd369, [%rd2+80];
	xor.b64  	%rd370, %rd369, %rd366;
	st.local.u64 	[%rd2+80], %rd370;
	ld.local.u64 	%rd371, [%rd2+120];
	xor.b64  	%rd372, %rd371, %rd366;
	st.local.u64 	[%rd2+120], %rd372;
	ld.local.u64 	%rd373, [%rd2+160];
	xor.b64  	%rd374, %rd373, %rd366;
	st.local.u64 	[%rd2+160], %rd374;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd364;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd363, {vl,vh};
	@p  mov.b64 %rd363, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd363;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd60, [%rd2];
	ld.local.u64 	%rd61, [%rd2+40];
	ld.local.u64 	%rd62, [%rd2+80];
	ld.local.u64 	%rd63, [%rd2+120];
	ld.local.u64 	%rd64, [%rd2+160];
	@%p11 bra 	$L__BB2_47;

	st.shared.u64 	[%r4], %rd64;
	ld.shared.u64 	%rd375, [%r6];
	not.b64 	%rd376, %rd375;
	ld.shared.u64 	%rd377, [%r9];
	and.b64  	%rd378, %rd377, %rd376;
	st.shared.u64 	[%r7], %rd378;

$L__BB2_47:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd379, [%r8];
	xor.b64  	%rd380, %rd60, %rd379;
	st.local.u64 	[%rd2], %rd380;
	xor.b64  	%rd381, %rd61, %rd379;
	st.local.u64 	[%rd2+40], %rd381;
	xor.b64  	%rd382, %rd62, %rd379;
	st.local.u64 	[%rd2+80], %rd382;
	xor.b64  	%rd383, %rd63, %rd379;
	st.local.u64 	[%rd2+120], %rd383;
	xor.b64  	%rd384, %rd64, %rd379;
	st.local.u64 	[%rd2+160], %rd384;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_49;

	ld.const.u64 	%rd1111, [CUDA_KECCAK_CONSTS+32];
	ld.local.u64 	%rd385, [%rd1];
	xor.b64  	%rd386, %rd385, %rd1111;
	st.local.u64 	[%rd1], %rd386;

$L__BB2_49:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_51;

	ld.local.u64 	%rd387, [%rd2];
	ld.local.u64 	%rd388, [%rd2+40];
	xor.b64  	%rd389, %rd388, %rd387;
	ld.local.u64 	%rd390, [%rd2+80];
	xor.b64  	%rd391, %rd389, %rd390;
	ld.local.u64 	%rd392, [%rd2+120];
	xor.b64  	%rd393, %rd391, %rd392;
	ld.local.u64 	%rd394, [%rd2+160];
	xor.b64  	%rd395, %rd393, %rd394;
	st.shared.u64 	[%r4], %rd395;

$L__BB2_51:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_53;

	ld.shared.u64 	%rd398, [%r5];
	ld.shared.u64 	%rd397, [%r6];
	mov.u32 	%r70, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd397;
	shf.l.wrap.b32 vl, tl, th, %r70;
	shf.l.wrap.b32 vh, th, tl, %r70;
	setp.lt.u32 p, %r70, 32;
	@!p mov.b64 %rd396, {vl,vh};
	@p  mov.b64 %rd396, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd399, %rd396, %rd398;
	st.shared.u64 	[%r7], %rd399;

$L__BB2_53:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd402, [%rd2];
	ld.shared.u64 	%rd403, [%r8];
	xor.b64  	%rd401, %rd402, %rd403;
	st.local.u64 	[%rd2], %rd401;
	ld.local.u64 	%rd404, [%rd2+40];
	xor.b64  	%rd405, %rd404, %rd403;
	st.local.u64 	[%rd2+40], %rd405;
	ld.local.u64 	%rd406, [%rd2+80];
	xor.b64  	%rd407, %rd406, %rd403;
	st.local.u64 	[%rd2+80], %rd407;
	ld.local.u64 	%rd408, [%rd2+120];
	xor.b64  	%rd409, %rd408, %rd403;
	st.local.u64 	[%rd2+120], %rd409;
	ld.local.u64 	%rd410, [%rd2+160];
	xor.b64  	%rd411, %rd410, %rd403;
	st.local.u64 	[%rd2+160], %rd411;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd401;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd400, {vl,vh};
	@p  mov.b64 %rd400, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd400;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd65, [%rd2];
	ld.local.u64 	%rd66, [%rd2+40];
	ld.local.u64 	%rd67, [%rd2+80];
	ld.local.u64 	%rd68, [%rd2+120];
	ld.local.u64 	%rd69, [%rd2+160];
	@%p11 bra 	$L__BB2_55;

	st.shared.u64 	[%r4], %rd69;
	ld.shared.u64 	%rd412, [%r6];
	not.b64 	%rd413, %rd412;
	ld.shared.u64 	%rd414, [%r9];
	and.b64  	%rd415, %rd414, %rd413;
	st.shared.u64 	[%r7], %rd415;

$L__BB2_55:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd416, [%r8];
	xor.b64  	%rd417, %rd65, %rd416;
	st.local.u64 	[%rd2], %rd417;
	xor.b64  	%rd418, %rd66, %rd416;
	st.local.u64 	[%rd2+40], %rd418;
	xor.b64  	%rd419, %rd67, %rd416;
	st.local.u64 	[%rd2+80], %rd419;
	xor.b64  	%rd420, %rd68, %rd416;
	st.local.u64 	[%rd2+120], %rd420;
	xor.b64  	%rd421, %rd69, %rd416;
	st.local.u64 	[%rd2+160], %rd421;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_57;

	ld.const.u64 	%rd1110, [CUDA_KECCAK_CONSTS+40];
	ld.local.u64 	%rd422, [%rd1];
	xor.b64  	%rd423, %rd422, %rd1110;
	st.local.u64 	[%rd1], %rd423;

$L__BB2_57:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_59;

	ld.local.u64 	%rd424, [%rd2];
	ld.local.u64 	%rd425, [%rd2+40];
	xor.b64  	%rd426, %rd425, %rd424;
	ld.local.u64 	%rd427, [%rd2+80];
	xor.b64  	%rd428, %rd426, %rd427;
	ld.local.u64 	%rd429, [%rd2+120];
	xor.b64  	%rd430, %rd428, %rd429;
	ld.local.u64 	%rd431, [%rd2+160];
	xor.b64  	%rd432, %rd430, %rd431;
	st.shared.u64 	[%r4], %rd432;

$L__BB2_59:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_61;

	ld.shared.u64 	%rd435, [%r5];
	ld.shared.u64 	%rd434, [%r6];
	mov.u32 	%r72, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd434;
	shf.l.wrap.b32 vl, tl, th, %r72;
	shf.l.wrap.b32 vh, th, tl, %r72;
	setp.lt.u32 p, %r72, 32;
	@!p mov.b64 %rd433, {vl,vh};
	@p  mov.b64 %rd433, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd436, %rd433, %rd435;
	st.shared.u64 	[%r7], %rd436;

$L__BB2_61:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd439, [%rd2];
	ld.shared.u64 	%rd440, [%r8];
	xor.b64  	%rd438, %rd439, %rd440;
	st.local.u64 	[%rd2], %rd438;
	ld.local.u64 	%rd441, [%rd2+40];
	xor.b64  	%rd442, %rd441, %rd440;
	st.local.u64 	[%rd2+40], %rd442;
	ld.local.u64 	%rd443, [%rd2+80];
	xor.b64  	%rd444, %rd443, %rd440;
	st.local.u64 	[%rd2+80], %rd444;
	ld.local.u64 	%rd445, [%rd2+120];
	xor.b64  	%rd446, %rd445, %rd440;
	st.local.u64 	[%rd2+120], %rd446;
	ld.local.u64 	%rd447, [%rd2+160];
	xor.b64  	%rd448, %rd447, %rd440;
	st.local.u64 	[%rd2+160], %rd448;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd438;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd437, {vl,vh};
	@p  mov.b64 %rd437, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd437;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd70, [%rd2];
	ld.local.u64 	%rd71, [%rd2+40];
	ld.local.u64 	%rd72, [%rd2+80];
	ld.local.u64 	%rd73, [%rd2+120];
	ld.local.u64 	%rd74, [%rd2+160];
	@%p11 bra 	$L__BB2_63;

	st.shared.u64 	[%r4], %rd74;
	ld.shared.u64 	%rd449, [%r6];
	not.b64 	%rd450, %rd449;
	ld.shared.u64 	%rd451, [%r9];
	and.b64  	%rd452, %rd451, %rd450;
	st.shared.u64 	[%r7], %rd452;

$L__BB2_63:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd453, [%r8];
	xor.b64  	%rd454, %rd70, %rd453;
	st.local.u64 	[%rd2], %rd454;
	xor.b64  	%rd455, %rd71, %rd453;
	st.local.u64 	[%rd2+40], %rd455;
	xor.b64  	%rd456, %rd72, %rd453;
	st.local.u64 	[%rd2+80], %rd456;
	xor.b64  	%rd457, %rd73, %rd453;
	st.local.u64 	[%rd2+120], %rd457;
	xor.b64  	%rd458, %rd74, %rd453;
	st.local.u64 	[%rd2+160], %rd458;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_65;

	ld.const.u64 	%rd1109, [CUDA_KECCAK_CONSTS+48];
	ld.local.u64 	%rd459, [%rd1];
	xor.b64  	%rd460, %rd459, %rd1109;
	st.local.u64 	[%rd1], %rd460;

$L__BB2_65:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_67;

	ld.local.u64 	%rd461, [%rd2];
	ld.local.u64 	%rd462, [%rd2+40];
	xor.b64  	%rd463, %rd462, %rd461;
	ld.local.u64 	%rd464, [%rd2+80];
	xor.b64  	%rd465, %rd463, %rd464;
	ld.local.u64 	%rd466, [%rd2+120];
	xor.b64  	%rd467, %rd465, %rd466;
	ld.local.u64 	%rd468, [%rd2+160];
	xor.b64  	%rd469, %rd467, %rd468;
	st.shared.u64 	[%r4], %rd469;

$L__BB2_67:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_69;

	ld.shared.u64 	%rd472, [%r5];
	ld.shared.u64 	%rd471, [%r6];
	mov.u32 	%r74, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd471;
	shf.l.wrap.b32 vl, tl, th, %r74;
	shf.l.wrap.b32 vh, th, tl, %r74;
	setp.lt.u32 p, %r74, 32;
	@!p mov.b64 %rd470, {vl,vh};
	@p  mov.b64 %rd470, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd473, %rd470, %rd472;
	st.shared.u64 	[%r7], %rd473;

$L__BB2_69:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd476, [%rd2];
	ld.shared.u64 	%rd477, [%r8];
	xor.b64  	%rd475, %rd476, %rd477;
	st.local.u64 	[%rd2], %rd475;
	ld.local.u64 	%rd478, [%rd2+40];
	xor.b64  	%rd479, %rd478, %rd477;
	st.local.u64 	[%rd2+40], %rd479;
	ld.local.u64 	%rd480, [%rd2+80];
	xor.b64  	%rd481, %rd480, %rd477;
	st.local.u64 	[%rd2+80], %rd481;
	ld.local.u64 	%rd482, [%rd2+120];
	xor.b64  	%rd483, %rd482, %rd477;
	st.local.u64 	[%rd2+120], %rd483;
	ld.local.u64 	%rd484, [%rd2+160];
	xor.b64  	%rd485, %rd484, %rd477;
	st.local.u64 	[%rd2+160], %rd485;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd475;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd474, {vl,vh};
	@p  mov.b64 %rd474, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd474;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd75, [%rd2];
	ld.local.u64 	%rd76, [%rd2+40];
	ld.local.u64 	%rd77, [%rd2+80];
	ld.local.u64 	%rd78, [%rd2+120];
	ld.local.u64 	%rd79, [%rd2+160];
	@%p11 bra 	$L__BB2_71;

	st.shared.u64 	[%r4], %rd79;
	ld.shared.u64 	%rd486, [%r6];
	not.b64 	%rd487, %rd486;
	ld.shared.u64 	%rd488, [%r9];
	and.b64  	%rd489, %rd488, %rd487;
	st.shared.u64 	[%r7], %rd489;

$L__BB2_71:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd490, [%r8];
	xor.b64  	%rd491, %rd75, %rd490;
	st.local.u64 	[%rd2], %rd491;
	xor.b64  	%rd492, %rd76, %rd490;
	st.local.u64 	[%rd2+40], %rd492;
	xor.b64  	%rd493, %rd77, %rd490;
	st.local.u64 	[%rd2+80], %rd493;
	xor.b64  	%rd494, %rd78, %rd490;
	st.local.u64 	[%rd2+120], %rd494;
	xor.b64  	%rd495, %rd79, %rd490;
	st.local.u64 	[%rd2+160], %rd495;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_73;

	ld.const.u64 	%rd1108, [CUDA_KECCAK_CONSTS+56];
	ld.local.u64 	%rd496, [%rd1];
	xor.b64  	%rd497, %rd496, %rd1108;
	st.local.u64 	[%rd1], %rd497;

$L__BB2_73:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_75;

	ld.local.u64 	%rd498, [%rd2];
	ld.local.u64 	%rd499, [%rd2+40];
	xor.b64  	%rd500, %rd499, %rd498;
	ld.local.u64 	%rd501, [%rd2+80];
	xor.b64  	%rd502, %rd500, %rd501;
	ld.local.u64 	%rd503, [%rd2+120];
	xor.b64  	%rd504, %rd502, %rd503;
	ld.local.u64 	%rd505, [%rd2+160];
	xor.b64  	%rd506, %rd504, %rd505;
	st.shared.u64 	[%r4], %rd506;

$L__BB2_75:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_77;

	ld.shared.u64 	%rd509, [%r5];
	ld.shared.u64 	%rd508, [%r6];
	mov.u32 	%r76, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd508;
	shf.l.wrap.b32 vl, tl, th, %r76;
	shf.l.wrap.b32 vh, th, tl, %r76;
	setp.lt.u32 p, %r76, 32;
	@!p mov.b64 %rd507, {vl,vh};
	@p  mov.b64 %rd507, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd510, %rd507, %rd509;
	st.shared.u64 	[%r7], %rd510;

$L__BB2_77:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd513, [%rd2];
	ld.shared.u64 	%rd514, [%r8];
	xor.b64  	%rd512, %rd513, %rd514;
	st.local.u64 	[%rd2], %rd512;
	ld.local.u64 	%rd515, [%rd2+40];
	xor.b64  	%rd516, %rd515, %rd514;
	st.local.u64 	[%rd2+40], %rd516;
	ld.local.u64 	%rd517, [%rd2+80];
	xor.b64  	%rd518, %rd517, %rd514;
	st.local.u64 	[%rd2+80], %rd518;
	ld.local.u64 	%rd519, [%rd2+120];
	xor.b64  	%rd520, %rd519, %rd514;
	st.local.u64 	[%rd2+120], %rd520;
	ld.local.u64 	%rd521, [%rd2+160];
	xor.b64  	%rd522, %rd521, %rd514;
	st.local.u64 	[%rd2+160], %rd522;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd512;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd511, {vl,vh};
	@p  mov.b64 %rd511, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd511;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd80, [%rd2];
	ld.local.u64 	%rd81, [%rd2+40];
	ld.local.u64 	%rd82, [%rd2+80];
	ld.local.u64 	%rd83, [%rd2+120];
	ld.local.u64 	%rd84, [%rd2+160];
	@%p11 bra 	$L__BB2_79;

	st.shared.u64 	[%r4], %rd84;
	ld.shared.u64 	%rd523, [%r6];
	not.b64 	%rd524, %rd523;
	ld.shared.u64 	%rd525, [%r9];
	and.b64  	%rd526, %rd525, %rd524;
	st.shared.u64 	[%r7], %rd526;

$L__BB2_79:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd527, [%r8];
	xor.b64  	%rd528, %rd80, %rd527;
	st.local.u64 	[%rd2], %rd528;
	xor.b64  	%rd529, %rd81, %rd527;
	st.local.u64 	[%rd2+40], %rd529;
	xor.b64  	%rd530, %rd82, %rd527;
	st.local.u64 	[%rd2+80], %rd530;
	xor.b64  	%rd531, %rd83, %rd527;
	st.local.u64 	[%rd2+120], %rd531;
	xor.b64  	%rd532, %rd84, %rd527;
	st.local.u64 	[%rd2+160], %rd532;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_81;

	ld.const.u64 	%rd1107, [CUDA_KECCAK_CONSTS+64];
	ld.local.u64 	%rd533, [%rd1];
	xor.b64  	%rd534, %rd533, %rd1107;
	st.local.u64 	[%rd1], %rd534;

$L__BB2_81:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_83;

	ld.local.u64 	%rd535, [%rd2];
	ld.local.u64 	%rd536, [%rd2+40];
	xor.b64  	%rd537, %rd536, %rd535;
	ld.local.u64 	%rd538, [%rd2+80];
	xor.b64  	%rd539, %rd537, %rd538;
	ld.local.u64 	%rd540, [%rd2+120];
	xor.b64  	%rd541, %rd539, %rd540;
	ld.local.u64 	%rd542, [%rd2+160];
	xor.b64  	%rd543, %rd541, %rd542;
	st.shared.u64 	[%r4], %rd543;

$L__BB2_83:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_85;

	ld.shared.u64 	%rd546, [%r5];
	ld.shared.u64 	%rd545, [%r6];
	mov.u32 	%r78, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd545;
	shf.l.wrap.b32 vl, tl, th, %r78;
	shf.l.wrap.b32 vh, th, tl, %r78;
	setp.lt.u32 p, %r78, 32;
	@!p mov.b64 %rd544, {vl,vh};
	@p  mov.b64 %rd544, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd547, %rd544, %rd546;
	st.shared.u64 	[%r7], %rd547;

$L__BB2_85:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd550, [%rd2];
	ld.shared.u64 	%rd551, [%r8];
	xor.b64  	%rd549, %rd550, %rd551;
	st.local.u64 	[%rd2], %rd549;
	ld.local.u64 	%rd552, [%rd2+40];
	xor.b64  	%rd553, %rd552, %rd551;
	st.local.u64 	[%rd2+40], %rd553;
	ld.local.u64 	%rd554, [%rd2+80];
	xor.b64  	%rd555, %rd554, %rd551;
	st.local.u64 	[%rd2+80], %rd555;
	ld.local.u64 	%rd556, [%rd2+120];
	xor.b64  	%rd557, %rd556, %rd551;
	st.local.u64 	[%rd2+120], %rd557;
	ld.local.u64 	%rd558, [%rd2+160];
	xor.b64  	%rd559, %rd558, %rd551;
	st.local.u64 	[%rd2+160], %rd559;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd549;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd548, {vl,vh};
	@p  mov.b64 %rd548, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd548;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd85, [%rd2];
	ld.local.u64 	%rd86, [%rd2+40];
	ld.local.u64 	%rd87, [%rd2+80];
	ld.local.u64 	%rd88, [%rd2+120];
	ld.local.u64 	%rd89, [%rd2+160];
	@%p11 bra 	$L__BB2_87;

	st.shared.u64 	[%r4], %rd89;
	ld.shared.u64 	%rd560, [%r6];
	not.b64 	%rd561, %rd560;
	ld.shared.u64 	%rd562, [%r9];
	and.b64  	%rd563, %rd562, %rd561;
	st.shared.u64 	[%r7], %rd563;

$L__BB2_87:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd564, [%r8];
	xor.b64  	%rd565, %rd85, %rd564;
	st.local.u64 	[%rd2], %rd565;
	xor.b64  	%rd566, %rd86, %rd564;
	st.local.u64 	[%rd2+40], %rd566;
	xor.b64  	%rd567, %rd87, %rd564;
	st.local.u64 	[%rd2+80], %rd567;
	xor.b64  	%rd568, %rd88, %rd564;
	st.local.u64 	[%rd2+120], %rd568;
	xor.b64  	%rd569, %rd89, %rd564;
	st.local.u64 	[%rd2+160], %rd569;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_89;

	ld.const.u64 	%rd1106, [CUDA_KECCAK_CONSTS+72];
	ld.local.u64 	%rd570, [%rd1];
	xor.b64  	%rd571, %rd570, %rd1106;
	st.local.u64 	[%rd1], %rd571;

$L__BB2_89:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_91;

	ld.local.u64 	%rd572, [%rd2];
	ld.local.u64 	%rd573, [%rd2+40];
	xor.b64  	%rd574, %rd573, %rd572;
	ld.local.u64 	%rd575, [%rd2+80];
	xor.b64  	%rd576, %rd574, %rd575;
	ld.local.u64 	%rd577, [%rd2+120];
	xor.b64  	%rd578, %rd576, %rd577;
	ld.local.u64 	%rd579, [%rd2+160];
	xor.b64  	%rd580, %rd578, %rd579;
	st.shared.u64 	[%r4], %rd580;

$L__BB2_91:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_93;

	ld.shared.u64 	%rd583, [%r5];
	ld.shared.u64 	%rd582, [%r6];
	mov.u32 	%r80, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd582;
	shf.l.wrap.b32 vl, tl, th, %r80;
	shf.l.wrap.b32 vh, th, tl, %r80;
	setp.lt.u32 p, %r80, 32;
	@!p mov.b64 %rd581, {vl,vh};
	@p  mov.b64 %rd581, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd584, %rd581, %rd583;
	st.shared.u64 	[%r7], %rd584;

$L__BB2_93:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd587, [%rd2];
	ld.shared.u64 	%rd588, [%r8];
	xor.b64  	%rd586, %rd587, %rd588;
	st.local.u64 	[%rd2], %rd586;
	ld.local.u64 	%rd589, [%rd2+40];
	xor.b64  	%rd590, %rd589, %rd588;
	st.local.u64 	[%rd2+40], %rd590;
	ld.local.u64 	%rd591, [%rd2+80];
	xor.b64  	%rd592, %rd591, %rd588;
	st.local.u64 	[%rd2+80], %rd592;
	ld.local.u64 	%rd593, [%rd2+120];
	xor.b64  	%rd594, %rd593, %rd588;
	st.local.u64 	[%rd2+120], %rd594;
	ld.local.u64 	%rd595, [%rd2+160];
	xor.b64  	%rd596, %rd595, %rd588;
	st.local.u64 	[%rd2+160], %rd596;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd586;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd585, {vl,vh};
	@p  mov.b64 %rd585, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd585;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd90, [%rd2];
	ld.local.u64 	%rd91, [%rd2+40];
	ld.local.u64 	%rd92, [%rd2+80];
	ld.local.u64 	%rd93, [%rd2+120];
	ld.local.u64 	%rd94, [%rd2+160];
	@%p11 bra 	$L__BB2_95;

	st.shared.u64 	[%r4], %rd94;
	ld.shared.u64 	%rd597, [%r6];
	not.b64 	%rd598, %rd597;
	ld.shared.u64 	%rd599, [%r9];
	and.b64  	%rd600, %rd599, %rd598;
	st.shared.u64 	[%r7], %rd600;

$L__BB2_95:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd601, [%r8];
	xor.b64  	%rd602, %rd90, %rd601;
	st.local.u64 	[%rd2], %rd602;
	xor.b64  	%rd603, %rd91, %rd601;
	st.local.u64 	[%rd2+40], %rd603;
	xor.b64  	%rd604, %rd92, %rd601;
	st.local.u64 	[%rd2+80], %rd604;
	xor.b64  	%rd605, %rd93, %rd601;
	st.local.u64 	[%rd2+120], %rd605;
	xor.b64  	%rd606, %rd94, %rd601;
	st.local.u64 	[%rd2+160], %rd606;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_97;

	ld.const.u64 	%rd1105, [CUDA_KECCAK_CONSTS+80];
	ld.local.u64 	%rd607, [%rd1];
	xor.b64  	%rd608, %rd607, %rd1105;
	st.local.u64 	[%rd1], %rd608;

$L__BB2_97:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_99;

	ld.local.u64 	%rd609, [%rd2];
	ld.local.u64 	%rd610, [%rd2+40];
	xor.b64  	%rd611, %rd610, %rd609;
	ld.local.u64 	%rd612, [%rd2+80];
	xor.b64  	%rd613, %rd611, %rd612;
	ld.local.u64 	%rd614, [%rd2+120];
	xor.b64  	%rd615, %rd613, %rd614;
	ld.local.u64 	%rd616, [%rd2+160];
	xor.b64  	%rd617, %rd615, %rd616;
	st.shared.u64 	[%r4], %rd617;

$L__BB2_99:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_101;

	ld.shared.u64 	%rd620, [%r5];
	ld.shared.u64 	%rd619, [%r6];
	mov.u32 	%r82, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd619;
	shf.l.wrap.b32 vl, tl, th, %r82;
	shf.l.wrap.b32 vh, th, tl, %r82;
	setp.lt.u32 p, %r82, 32;
	@!p mov.b64 %rd618, {vl,vh};
	@p  mov.b64 %rd618, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd621, %rd618, %rd620;
	st.shared.u64 	[%r7], %rd621;

$L__BB2_101:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd624, [%rd2];
	ld.shared.u64 	%rd625, [%r8];
	xor.b64  	%rd623, %rd624, %rd625;
	st.local.u64 	[%rd2], %rd623;
	ld.local.u64 	%rd626, [%rd2+40];
	xor.b64  	%rd627, %rd626, %rd625;
	st.local.u64 	[%rd2+40], %rd627;
	ld.local.u64 	%rd628, [%rd2+80];
	xor.b64  	%rd629, %rd628, %rd625;
	st.local.u64 	[%rd2+80], %rd629;
	ld.local.u64 	%rd630, [%rd2+120];
	xor.b64  	%rd631, %rd630, %rd625;
	st.local.u64 	[%rd2+120], %rd631;
	ld.local.u64 	%rd632, [%rd2+160];
	xor.b64  	%rd633, %rd632, %rd625;
	st.local.u64 	[%rd2+160], %rd633;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd623;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd622, {vl,vh};
	@p  mov.b64 %rd622, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd622;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd95, [%rd2];
	ld.local.u64 	%rd96, [%rd2+40];
	ld.local.u64 	%rd97, [%rd2+80];
	ld.local.u64 	%rd98, [%rd2+120];
	ld.local.u64 	%rd99, [%rd2+160];
	@%p11 bra 	$L__BB2_103;

	st.shared.u64 	[%r4], %rd99;
	ld.shared.u64 	%rd634, [%r6];
	not.b64 	%rd635, %rd634;
	ld.shared.u64 	%rd636, [%r9];
	and.b64  	%rd637, %rd636, %rd635;
	st.shared.u64 	[%r7], %rd637;

$L__BB2_103:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd638, [%r8];
	xor.b64  	%rd639, %rd95, %rd638;
	st.local.u64 	[%rd2], %rd639;
	xor.b64  	%rd640, %rd96, %rd638;
	st.local.u64 	[%rd2+40], %rd640;
	xor.b64  	%rd641, %rd97, %rd638;
	st.local.u64 	[%rd2+80], %rd641;
	xor.b64  	%rd642, %rd98, %rd638;
	st.local.u64 	[%rd2+120], %rd642;
	xor.b64  	%rd643, %rd99, %rd638;
	st.local.u64 	[%rd2+160], %rd643;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_105;

	ld.const.u64 	%rd1104, [CUDA_KECCAK_CONSTS+88];
	ld.local.u64 	%rd644, [%rd1];
	xor.b64  	%rd645, %rd644, %rd1104;
	st.local.u64 	[%rd1], %rd645;

$L__BB2_105:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_107;

	ld.local.u64 	%rd646, [%rd2];
	ld.local.u64 	%rd647, [%rd2+40];
	xor.b64  	%rd648, %rd647, %rd646;
	ld.local.u64 	%rd649, [%rd2+80];
	xor.b64  	%rd650, %rd648, %rd649;
	ld.local.u64 	%rd651, [%rd2+120];
	xor.b64  	%rd652, %rd650, %rd651;
	ld.local.u64 	%rd653, [%rd2+160];
	xor.b64  	%rd654, %rd652, %rd653;
	st.shared.u64 	[%r4], %rd654;

$L__BB2_107:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_109;

	ld.shared.u64 	%rd657, [%r5];
	ld.shared.u64 	%rd656, [%r6];
	mov.u32 	%r84, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd656;
	shf.l.wrap.b32 vl, tl, th, %r84;
	shf.l.wrap.b32 vh, th, tl, %r84;
	setp.lt.u32 p, %r84, 32;
	@!p mov.b64 %rd655, {vl,vh};
	@p  mov.b64 %rd655, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd658, %rd655, %rd657;
	st.shared.u64 	[%r7], %rd658;

$L__BB2_109:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd661, [%rd2];
	ld.shared.u64 	%rd662, [%r8];
	xor.b64  	%rd660, %rd661, %rd662;
	st.local.u64 	[%rd2], %rd660;
	ld.local.u64 	%rd663, [%rd2+40];
	xor.b64  	%rd664, %rd663, %rd662;
	st.local.u64 	[%rd2+40], %rd664;
	ld.local.u64 	%rd665, [%rd2+80];
	xor.b64  	%rd666, %rd665, %rd662;
	st.local.u64 	[%rd2+80], %rd666;
	ld.local.u64 	%rd667, [%rd2+120];
	xor.b64  	%rd668, %rd667, %rd662;
	st.local.u64 	[%rd2+120], %rd668;
	ld.local.u64 	%rd669, [%rd2+160];
	xor.b64  	%rd670, %rd669, %rd662;
	st.local.u64 	[%rd2+160], %rd670;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd660;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd659, {vl,vh};
	@p  mov.b64 %rd659, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd659;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd100, [%rd2];
	ld.local.u64 	%rd101, [%rd2+40];
	ld.local.u64 	%rd102, [%rd2+80];
	ld.local.u64 	%rd103, [%rd2+120];
	ld.local.u64 	%rd104, [%rd2+160];
	@%p11 bra 	$L__BB2_111;

	st.shared.u64 	[%r4], %rd104;
	ld.shared.u64 	%rd671, [%r6];
	not.b64 	%rd672, %rd671;
	ld.shared.u64 	%rd673, [%r9];
	and.b64  	%rd674, %rd673, %rd672;
	st.shared.u64 	[%r7], %rd674;

$L__BB2_111:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd675, [%r8];
	xor.b64  	%rd676, %rd100, %rd675;
	st.local.u64 	[%rd2], %rd676;
	xor.b64  	%rd677, %rd101, %rd675;
	st.local.u64 	[%rd2+40], %rd677;
	xor.b64  	%rd678, %rd102, %rd675;
	st.local.u64 	[%rd2+80], %rd678;
	xor.b64  	%rd679, %rd103, %rd675;
	st.local.u64 	[%rd2+120], %rd679;
	xor.b64  	%rd680, %rd104, %rd675;
	st.local.u64 	[%rd2+160], %rd680;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_113;

	ld.const.u64 	%rd1103, [CUDA_KECCAK_CONSTS+96];
	ld.local.u64 	%rd681, [%rd1];
	xor.b64  	%rd682, %rd681, %rd1103;
	st.local.u64 	[%rd1], %rd682;

$L__BB2_113:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_115;

	ld.local.u64 	%rd683, [%rd2];
	ld.local.u64 	%rd684, [%rd2+40];
	xor.b64  	%rd685, %rd684, %rd683;
	ld.local.u64 	%rd686, [%rd2+80];
	xor.b64  	%rd687, %rd685, %rd686;
	ld.local.u64 	%rd688, [%rd2+120];
	xor.b64  	%rd689, %rd687, %rd688;
	ld.local.u64 	%rd690, [%rd2+160];
	xor.b64  	%rd691, %rd689, %rd690;
	st.shared.u64 	[%r4], %rd691;

$L__BB2_115:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_117;

	ld.shared.u64 	%rd694, [%r5];
	ld.shared.u64 	%rd693, [%r6];
	mov.u32 	%r86, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd693;
	shf.l.wrap.b32 vl, tl, th, %r86;
	shf.l.wrap.b32 vh, th, tl, %r86;
	setp.lt.u32 p, %r86, 32;
	@!p mov.b64 %rd692, {vl,vh};
	@p  mov.b64 %rd692, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd695, %rd692, %rd694;
	st.shared.u64 	[%r7], %rd695;

$L__BB2_117:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd698, [%rd2];
	ld.shared.u64 	%rd699, [%r8];
	xor.b64  	%rd697, %rd698, %rd699;
	st.local.u64 	[%rd2], %rd697;
	ld.local.u64 	%rd700, [%rd2+40];
	xor.b64  	%rd701, %rd700, %rd699;
	st.local.u64 	[%rd2+40], %rd701;
	ld.local.u64 	%rd702, [%rd2+80];
	xor.b64  	%rd703, %rd702, %rd699;
	st.local.u64 	[%rd2+80], %rd703;
	ld.local.u64 	%rd704, [%rd2+120];
	xor.b64  	%rd705, %rd704, %rd699;
	st.local.u64 	[%rd2+120], %rd705;
	ld.local.u64 	%rd706, [%rd2+160];
	xor.b64  	%rd707, %rd706, %rd699;
	st.local.u64 	[%rd2+160], %rd707;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd697;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd696, {vl,vh};
	@p  mov.b64 %rd696, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd696;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd105, [%rd2];
	ld.local.u64 	%rd106, [%rd2+40];
	ld.local.u64 	%rd107, [%rd2+80];
	ld.local.u64 	%rd108, [%rd2+120];
	ld.local.u64 	%rd109, [%rd2+160];
	@%p11 bra 	$L__BB2_119;

	st.shared.u64 	[%r4], %rd109;
	ld.shared.u64 	%rd708, [%r6];
	not.b64 	%rd709, %rd708;
	ld.shared.u64 	%rd710, [%r9];
	and.b64  	%rd711, %rd710, %rd709;
	st.shared.u64 	[%r7], %rd711;

$L__BB2_119:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd712, [%r8];
	xor.b64  	%rd713, %rd105, %rd712;
	st.local.u64 	[%rd2], %rd713;
	xor.b64  	%rd714, %rd106, %rd712;
	st.local.u64 	[%rd2+40], %rd714;
	xor.b64  	%rd715, %rd107, %rd712;
	st.local.u64 	[%rd2+80], %rd715;
	xor.b64  	%rd716, %rd108, %rd712;
	st.local.u64 	[%rd2+120], %rd716;
	xor.b64  	%rd717, %rd109, %rd712;
	st.local.u64 	[%rd2+160], %rd717;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_121;

	ld.const.u64 	%rd1102, [CUDA_KECCAK_CONSTS+104];
	ld.local.u64 	%rd718, [%rd1];
	xor.b64  	%rd719, %rd718, %rd1102;
	st.local.u64 	[%rd1], %rd719;

$L__BB2_121:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_123;

	ld.local.u64 	%rd720, [%rd2];
	ld.local.u64 	%rd721, [%rd2+40];
	xor.b64  	%rd722, %rd721, %rd720;
	ld.local.u64 	%rd723, [%rd2+80];
	xor.b64  	%rd724, %rd722, %rd723;
	ld.local.u64 	%rd725, [%rd2+120];
	xor.b64  	%rd726, %rd724, %rd725;
	ld.local.u64 	%rd727, [%rd2+160];
	xor.b64  	%rd728, %rd726, %rd727;
	st.shared.u64 	[%r4], %rd728;

$L__BB2_123:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_125;

	ld.shared.u64 	%rd731, [%r5];
	ld.shared.u64 	%rd730, [%r6];
	mov.u32 	%r88, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd730;
	shf.l.wrap.b32 vl, tl, th, %r88;
	shf.l.wrap.b32 vh, th, tl, %r88;
	setp.lt.u32 p, %r88, 32;
	@!p mov.b64 %rd729, {vl,vh};
	@p  mov.b64 %rd729, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd732, %rd729, %rd731;
	st.shared.u64 	[%r7], %rd732;

$L__BB2_125:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd735, [%rd2];
	ld.shared.u64 	%rd736, [%r8];
	xor.b64  	%rd734, %rd735, %rd736;
	st.local.u64 	[%rd2], %rd734;
	ld.local.u64 	%rd737, [%rd2+40];
	xor.b64  	%rd738, %rd737, %rd736;
	st.local.u64 	[%rd2+40], %rd738;
	ld.local.u64 	%rd739, [%rd2+80];
	xor.b64  	%rd740, %rd739, %rd736;
	st.local.u64 	[%rd2+80], %rd740;
	ld.local.u64 	%rd741, [%rd2+120];
	xor.b64  	%rd742, %rd741, %rd736;
	st.local.u64 	[%rd2+120], %rd742;
	ld.local.u64 	%rd743, [%rd2+160];
	xor.b64  	%rd744, %rd743, %rd736;
	st.local.u64 	[%rd2+160], %rd744;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd734;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd733, {vl,vh};
	@p  mov.b64 %rd733, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd733;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd110, [%rd2];
	ld.local.u64 	%rd111, [%rd2+40];
	ld.local.u64 	%rd112, [%rd2+80];
	ld.local.u64 	%rd113, [%rd2+120];
	ld.local.u64 	%rd114, [%rd2+160];
	@%p11 bra 	$L__BB2_127;

	st.shared.u64 	[%r4], %rd114;
	ld.shared.u64 	%rd745, [%r6];
	not.b64 	%rd746, %rd745;
	ld.shared.u64 	%rd747, [%r9];
	and.b64  	%rd748, %rd747, %rd746;
	st.shared.u64 	[%r7], %rd748;

$L__BB2_127:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd749, [%r8];
	xor.b64  	%rd750, %rd110, %rd749;
	st.local.u64 	[%rd2], %rd750;
	xor.b64  	%rd751, %rd111, %rd749;
	st.local.u64 	[%rd2+40], %rd751;
	xor.b64  	%rd752, %rd112, %rd749;
	st.local.u64 	[%rd2+80], %rd752;
	xor.b64  	%rd753, %rd113, %rd749;
	st.local.u64 	[%rd2+120], %rd753;
	xor.b64  	%rd754, %rd114, %rd749;
	st.local.u64 	[%rd2+160], %rd754;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_129;

	ld.const.u64 	%rd1101, [CUDA_KECCAK_CONSTS+112];
	ld.local.u64 	%rd755, [%rd1];
	xor.b64  	%rd756, %rd755, %rd1101;
	st.local.u64 	[%rd1], %rd756;

$L__BB2_129:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_131;

	ld.local.u64 	%rd757, [%rd2];
	ld.local.u64 	%rd758, [%rd2+40];
	xor.b64  	%rd759, %rd758, %rd757;
	ld.local.u64 	%rd760, [%rd2+80];
	xor.b64  	%rd761, %rd759, %rd760;
	ld.local.u64 	%rd762, [%rd2+120];
	xor.b64  	%rd763, %rd761, %rd762;
	ld.local.u64 	%rd764, [%rd2+160];
	xor.b64  	%rd765, %rd763, %rd764;
	st.shared.u64 	[%r4], %rd765;

$L__BB2_131:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_133;

	ld.shared.u64 	%rd768, [%r5];
	ld.shared.u64 	%rd767, [%r6];
	mov.u32 	%r90, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd767;
	shf.l.wrap.b32 vl, tl, th, %r90;
	shf.l.wrap.b32 vh, th, tl, %r90;
	setp.lt.u32 p, %r90, 32;
	@!p mov.b64 %rd766, {vl,vh};
	@p  mov.b64 %rd766, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd769, %rd766, %rd768;
	st.shared.u64 	[%r7], %rd769;

$L__BB2_133:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd772, [%rd2];
	ld.shared.u64 	%rd773, [%r8];
	xor.b64  	%rd771, %rd772, %rd773;
	st.local.u64 	[%rd2], %rd771;
	ld.local.u64 	%rd774, [%rd2+40];
	xor.b64  	%rd775, %rd774, %rd773;
	st.local.u64 	[%rd2+40], %rd775;
	ld.local.u64 	%rd776, [%rd2+80];
	xor.b64  	%rd777, %rd776, %rd773;
	st.local.u64 	[%rd2+80], %rd777;
	ld.local.u64 	%rd778, [%rd2+120];
	xor.b64  	%rd779, %rd778, %rd773;
	st.local.u64 	[%rd2+120], %rd779;
	ld.local.u64 	%rd780, [%rd2+160];
	xor.b64  	%rd781, %rd780, %rd773;
	st.local.u64 	[%rd2+160], %rd781;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd771;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd770, {vl,vh};
	@p  mov.b64 %rd770, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd770;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd115, [%rd2];
	ld.local.u64 	%rd116, [%rd2+40];
	ld.local.u64 	%rd117, [%rd2+80];
	ld.local.u64 	%rd118, [%rd2+120];
	ld.local.u64 	%rd119, [%rd2+160];
	@%p11 bra 	$L__BB2_135;

	st.shared.u64 	[%r4], %rd119;
	ld.shared.u64 	%rd782, [%r6];
	not.b64 	%rd783, %rd782;
	ld.shared.u64 	%rd784, [%r9];
	and.b64  	%rd785, %rd784, %rd783;
	st.shared.u64 	[%r7], %rd785;

$L__BB2_135:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd786, [%r8];
	xor.b64  	%rd787, %rd115, %rd786;
	st.local.u64 	[%rd2], %rd787;
	xor.b64  	%rd788, %rd116, %rd786;
	st.local.u64 	[%rd2+40], %rd788;
	xor.b64  	%rd789, %rd117, %rd786;
	st.local.u64 	[%rd2+80], %rd789;
	xor.b64  	%rd790, %rd118, %rd786;
	st.local.u64 	[%rd2+120], %rd790;
	xor.b64  	%rd791, %rd119, %rd786;
	st.local.u64 	[%rd2+160], %rd791;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_137;

	ld.const.u64 	%rd1100, [CUDA_KECCAK_CONSTS+120];
	ld.local.u64 	%rd792, [%rd1];
	xor.b64  	%rd793, %rd792, %rd1100;
	st.local.u64 	[%rd1], %rd793;

$L__BB2_137:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_139;

	ld.local.u64 	%rd794, [%rd2];
	ld.local.u64 	%rd795, [%rd2+40];
	xor.b64  	%rd796, %rd795, %rd794;
	ld.local.u64 	%rd797, [%rd2+80];
	xor.b64  	%rd798, %rd796, %rd797;
	ld.local.u64 	%rd799, [%rd2+120];
	xor.b64  	%rd800, %rd798, %rd799;
	ld.local.u64 	%rd801, [%rd2+160];
	xor.b64  	%rd802, %rd800, %rd801;
	st.shared.u64 	[%r4], %rd802;

$L__BB2_139:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_141;

	ld.shared.u64 	%rd805, [%r5];
	ld.shared.u64 	%rd804, [%r6];
	mov.u32 	%r92, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd804;
	shf.l.wrap.b32 vl, tl, th, %r92;
	shf.l.wrap.b32 vh, th, tl, %r92;
	setp.lt.u32 p, %r92, 32;
	@!p mov.b64 %rd803, {vl,vh};
	@p  mov.b64 %rd803, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd806, %rd803, %rd805;
	st.shared.u64 	[%r7], %rd806;

$L__BB2_141:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd809, [%rd2];
	ld.shared.u64 	%rd810, [%r8];
	xor.b64  	%rd808, %rd809, %rd810;
	st.local.u64 	[%rd2], %rd808;
	ld.local.u64 	%rd811, [%rd2+40];
	xor.b64  	%rd812, %rd811, %rd810;
	st.local.u64 	[%rd2+40], %rd812;
	ld.local.u64 	%rd813, [%rd2+80];
	xor.b64  	%rd814, %rd813, %rd810;
	st.local.u64 	[%rd2+80], %rd814;
	ld.local.u64 	%rd815, [%rd2+120];
	xor.b64  	%rd816, %rd815, %rd810;
	st.local.u64 	[%rd2+120], %rd816;
	ld.local.u64 	%rd817, [%rd2+160];
	xor.b64  	%rd818, %rd817, %rd810;
	st.local.u64 	[%rd2+160], %rd818;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd808;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd807, {vl,vh};
	@p  mov.b64 %rd807, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd807;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd120, [%rd2];
	ld.local.u64 	%rd121, [%rd2+40];
	ld.local.u64 	%rd122, [%rd2+80];
	ld.local.u64 	%rd123, [%rd2+120];
	ld.local.u64 	%rd124, [%rd2+160];
	@%p11 bra 	$L__BB2_143;

	st.shared.u64 	[%r4], %rd124;
	ld.shared.u64 	%rd819, [%r6];
	not.b64 	%rd820, %rd819;
	ld.shared.u64 	%rd821, [%r9];
	and.b64  	%rd822, %rd821, %rd820;
	st.shared.u64 	[%r7], %rd822;

$L__BB2_143:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd823, [%r8];
	xor.b64  	%rd824, %rd120, %rd823;
	st.local.u64 	[%rd2], %rd824;
	xor.b64  	%rd825, %rd121, %rd823;
	st.local.u64 	[%rd2+40], %rd825;
	xor.b64  	%rd826, %rd122, %rd823;
	st.local.u64 	[%rd2+80], %rd826;
	xor.b64  	%rd827, %rd123, %rd823;
	st.local.u64 	[%rd2+120], %rd827;
	xor.b64  	%rd828, %rd124, %rd823;
	st.local.u64 	[%rd2+160], %rd828;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_145;

	ld.const.u64 	%rd1123, [CUDA_KECCAK_CONSTS+128];
	ld.local.u64 	%rd829, [%rd1];
	xor.b64  	%rd830, %rd829, %rd1123;
	st.local.u64 	[%rd1], %rd830;

$L__BB2_145:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_147;

	ld.local.u64 	%rd831, [%rd2];
	ld.local.u64 	%rd832, [%rd2+40];
	xor.b64  	%rd833, %rd832, %rd831;
	ld.local.u64 	%rd834, [%rd2+80];
	xor.b64  	%rd835, %rd833, %rd834;
	ld.local.u64 	%rd836, [%rd2+120];
	xor.b64  	%rd837, %rd835, %rd836;
	ld.local.u64 	%rd838, [%rd2+160];
	xor.b64  	%rd839, %rd837, %rd838;
	st.shared.u64 	[%r4], %rd839;

$L__BB2_147:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_149;

	ld.shared.u64 	%rd842, [%r5];
	ld.shared.u64 	%rd841, [%r6];
	mov.u32 	%r94, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd841;
	shf.l.wrap.b32 vl, tl, th, %r94;
	shf.l.wrap.b32 vh, th, tl, %r94;
	setp.lt.u32 p, %r94, 32;
	@!p mov.b64 %rd840, {vl,vh};
	@p  mov.b64 %rd840, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd843, %rd840, %rd842;
	st.shared.u64 	[%r7], %rd843;

$L__BB2_149:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd846, [%rd2];
	ld.shared.u64 	%rd847, [%r8];
	xor.b64  	%rd845, %rd846, %rd847;
	st.local.u64 	[%rd2], %rd845;
	ld.local.u64 	%rd848, [%rd2+40];
	xor.b64  	%rd849, %rd848, %rd847;
	st.local.u64 	[%rd2+40], %rd849;
	ld.local.u64 	%rd850, [%rd2+80];
	xor.b64  	%rd851, %rd850, %rd847;
	st.local.u64 	[%rd2+80], %rd851;
	ld.local.u64 	%rd852, [%rd2+120];
	xor.b64  	%rd853, %rd852, %rd847;
	st.local.u64 	[%rd2+120], %rd853;
	ld.local.u64 	%rd854, [%rd2+160];
	xor.b64  	%rd855, %rd854, %rd847;
	st.local.u64 	[%rd2+160], %rd855;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd845;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd844, {vl,vh};
	@p  mov.b64 %rd844, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd844;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd125, [%rd2];
	ld.local.u64 	%rd126, [%rd2+40];
	ld.local.u64 	%rd127, [%rd2+80];
	ld.local.u64 	%rd128, [%rd2+120];
	ld.local.u64 	%rd129, [%rd2+160];
	@%p11 bra 	$L__BB2_151;

	st.shared.u64 	[%r4], %rd129;
	ld.shared.u64 	%rd856, [%r6];
	not.b64 	%rd857, %rd856;
	ld.shared.u64 	%rd858, [%r9];
	and.b64  	%rd859, %rd858, %rd857;
	st.shared.u64 	[%r7], %rd859;

$L__BB2_151:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd860, [%r8];
	xor.b64  	%rd861, %rd125, %rd860;
	st.local.u64 	[%rd2], %rd861;
	xor.b64  	%rd862, %rd126, %rd860;
	st.local.u64 	[%rd2+40], %rd862;
	xor.b64  	%rd863, %rd127, %rd860;
	st.local.u64 	[%rd2+80], %rd863;
	xor.b64  	%rd864, %rd128, %rd860;
	st.local.u64 	[%rd2+120], %rd864;
	xor.b64  	%rd865, %rd129, %rd860;
	st.local.u64 	[%rd2+160], %rd865;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_153;

	ld.const.u64 	%rd1122, [CUDA_KECCAK_CONSTS+136];
	ld.local.u64 	%rd866, [%rd1];
	xor.b64  	%rd867, %rd866, %rd1122;
	st.local.u64 	[%rd1], %rd867;

$L__BB2_153:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_155;

	ld.local.u64 	%rd868, [%rd2];
	ld.local.u64 	%rd869, [%rd2+40];
	xor.b64  	%rd870, %rd869, %rd868;
	ld.local.u64 	%rd871, [%rd2+80];
	xor.b64  	%rd872, %rd870, %rd871;
	ld.local.u64 	%rd873, [%rd2+120];
	xor.b64  	%rd874, %rd872, %rd873;
	ld.local.u64 	%rd875, [%rd2+160];
	xor.b64  	%rd876, %rd874, %rd875;
	st.shared.u64 	[%r4], %rd876;

$L__BB2_155:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_157;

	ld.shared.u64 	%rd879, [%r5];
	ld.shared.u64 	%rd878, [%r6];
	mov.u32 	%r96, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd878;
	shf.l.wrap.b32 vl, tl, th, %r96;
	shf.l.wrap.b32 vh, th, tl, %r96;
	setp.lt.u32 p, %r96, 32;
	@!p mov.b64 %rd877, {vl,vh};
	@p  mov.b64 %rd877, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd880, %rd877, %rd879;
	st.shared.u64 	[%r7], %rd880;

$L__BB2_157:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd883, [%rd2];
	ld.shared.u64 	%rd884, [%r8];
	xor.b64  	%rd882, %rd883, %rd884;
	st.local.u64 	[%rd2], %rd882;
	ld.local.u64 	%rd885, [%rd2+40];
	xor.b64  	%rd886, %rd885, %rd884;
	st.local.u64 	[%rd2+40], %rd886;
	ld.local.u64 	%rd887, [%rd2+80];
	xor.b64  	%rd888, %rd887, %rd884;
	st.local.u64 	[%rd2+80], %rd888;
	ld.local.u64 	%rd889, [%rd2+120];
	xor.b64  	%rd890, %rd889, %rd884;
	st.local.u64 	[%rd2+120], %rd890;
	ld.local.u64 	%rd891, [%rd2+160];
	xor.b64  	%rd892, %rd891, %rd884;
	st.local.u64 	[%rd2+160], %rd892;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd882;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd881, {vl,vh};
	@p  mov.b64 %rd881, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd881;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd130, [%rd2];
	ld.local.u64 	%rd131, [%rd2+40];
	ld.local.u64 	%rd132, [%rd2+80];
	ld.local.u64 	%rd133, [%rd2+120];
	ld.local.u64 	%rd134, [%rd2+160];
	@%p11 bra 	$L__BB2_159;

	st.shared.u64 	[%r4], %rd134;
	ld.shared.u64 	%rd893, [%r6];
	not.b64 	%rd894, %rd893;
	ld.shared.u64 	%rd895, [%r9];
	and.b64  	%rd896, %rd895, %rd894;
	st.shared.u64 	[%r7], %rd896;

$L__BB2_159:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd897, [%r8];
	xor.b64  	%rd898, %rd130, %rd897;
	st.local.u64 	[%rd2], %rd898;
	xor.b64  	%rd899, %rd131, %rd897;
	st.local.u64 	[%rd2+40], %rd899;
	xor.b64  	%rd900, %rd132, %rd897;
	st.local.u64 	[%rd2+80], %rd900;
	xor.b64  	%rd901, %rd133, %rd897;
	st.local.u64 	[%rd2+120], %rd901;
	xor.b64  	%rd902, %rd134, %rd897;
	st.local.u64 	[%rd2+160], %rd902;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_161;

	ld.const.u64 	%rd1121, [CUDA_KECCAK_CONSTS+144];
	ld.local.u64 	%rd903, [%rd1];
	xor.b64  	%rd904, %rd903, %rd1121;
	st.local.u64 	[%rd1], %rd904;

$L__BB2_161:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_163;

	ld.local.u64 	%rd905, [%rd2];
	ld.local.u64 	%rd906, [%rd2+40];
	xor.b64  	%rd907, %rd906, %rd905;
	ld.local.u64 	%rd908, [%rd2+80];
	xor.b64  	%rd909, %rd907, %rd908;
	ld.local.u64 	%rd910, [%rd2+120];
	xor.b64  	%rd911, %rd909, %rd910;
	ld.local.u64 	%rd912, [%rd2+160];
	xor.b64  	%rd913, %rd911, %rd912;
	st.shared.u64 	[%r4], %rd913;

$L__BB2_163:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_165;

	ld.shared.u64 	%rd916, [%r5];
	ld.shared.u64 	%rd915, [%r6];
	mov.u32 	%r98, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd915;
	shf.l.wrap.b32 vl, tl, th, %r98;
	shf.l.wrap.b32 vh, th, tl, %r98;
	setp.lt.u32 p, %r98, 32;
	@!p mov.b64 %rd914, {vl,vh};
	@p  mov.b64 %rd914, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd917, %rd914, %rd916;
	st.shared.u64 	[%r7], %rd917;

$L__BB2_165:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd920, [%rd2];
	ld.shared.u64 	%rd921, [%r8];
	xor.b64  	%rd919, %rd920, %rd921;
	st.local.u64 	[%rd2], %rd919;
	ld.local.u64 	%rd922, [%rd2+40];
	xor.b64  	%rd923, %rd922, %rd921;
	st.local.u64 	[%rd2+40], %rd923;
	ld.local.u64 	%rd924, [%rd2+80];
	xor.b64  	%rd925, %rd924, %rd921;
	st.local.u64 	[%rd2+80], %rd925;
	ld.local.u64 	%rd926, [%rd2+120];
	xor.b64  	%rd927, %rd926, %rd921;
	st.local.u64 	[%rd2+120], %rd927;
	ld.local.u64 	%rd928, [%rd2+160];
	xor.b64  	%rd929, %rd928, %rd921;
	st.local.u64 	[%rd2+160], %rd929;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd919;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd918, {vl,vh};
	@p  mov.b64 %rd918, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd918;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd135, [%rd2];
	ld.local.u64 	%rd136, [%rd2+40];
	ld.local.u64 	%rd137, [%rd2+80];
	ld.local.u64 	%rd138, [%rd2+120];
	ld.local.u64 	%rd139, [%rd2+160];
	@%p11 bra 	$L__BB2_167;

	st.shared.u64 	[%r4], %rd139;
	ld.shared.u64 	%rd930, [%r6];
	not.b64 	%rd931, %rd930;
	ld.shared.u64 	%rd932, [%r9];
	and.b64  	%rd933, %rd932, %rd931;
	st.shared.u64 	[%r7], %rd933;

$L__BB2_167:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd934, [%r8];
	xor.b64  	%rd935, %rd135, %rd934;
	st.local.u64 	[%rd2], %rd935;
	xor.b64  	%rd936, %rd136, %rd934;
	st.local.u64 	[%rd2+40], %rd936;
	xor.b64  	%rd937, %rd137, %rd934;
	st.local.u64 	[%rd2+80], %rd937;
	xor.b64  	%rd938, %rd138, %rd934;
	st.local.u64 	[%rd2+120], %rd938;
	xor.b64  	%rd939, %rd139, %rd934;
	st.local.u64 	[%rd2+160], %rd939;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_169;

	ld.const.u64 	%rd1120, [CUDA_KECCAK_CONSTS+152];
	ld.local.u64 	%rd940, [%rd1];
	xor.b64  	%rd941, %rd940, %rd1120;
	st.local.u64 	[%rd1], %rd941;

$L__BB2_169:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_171;

	ld.local.u64 	%rd942, [%rd2];
	ld.local.u64 	%rd943, [%rd2+40];
	xor.b64  	%rd944, %rd943, %rd942;
	ld.local.u64 	%rd945, [%rd2+80];
	xor.b64  	%rd946, %rd944, %rd945;
	ld.local.u64 	%rd947, [%rd2+120];
	xor.b64  	%rd948, %rd946, %rd947;
	ld.local.u64 	%rd949, [%rd2+160];
	xor.b64  	%rd950, %rd948, %rd949;
	st.shared.u64 	[%r4], %rd950;

$L__BB2_171:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_173;

	ld.shared.u64 	%rd953, [%r5];
	ld.shared.u64 	%rd952, [%r6];
	mov.u32 	%r100, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd952;
	shf.l.wrap.b32 vl, tl, th, %r100;
	shf.l.wrap.b32 vh, th, tl, %r100;
	setp.lt.u32 p, %r100, 32;
	@!p mov.b64 %rd951, {vl,vh};
	@p  mov.b64 %rd951, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd954, %rd951, %rd953;
	st.shared.u64 	[%r7], %rd954;

$L__BB2_173:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd957, [%rd2];
	ld.shared.u64 	%rd958, [%r8];
	xor.b64  	%rd956, %rd957, %rd958;
	st.local.u64 	[%rd2], %rd956;
	ld.local.u64 	%rd959, [%rd2+40];
	xor.b64  	%rd960, %rd959, %rd958;
	st.local.u64 	[%rd2+40], %rd960;
	ld.local.u64 	%rd961, [%rd2+80];
	xor.b64  	%rd962, %rd961, %rd958;
	st.local.u64 	[%rd2+80], %rd962;
	ld.local.u64 	%rd963, [%rd2+120];
	xor.b64  	%rd964, %rd963, %rd958;
	st.local.u64 	[%rd2+120], %rd964;
	ld.local.u64 	%rd965, [%rd2+160];
	xor.b64  	%rd966, %rd965, %rd958;
	st.local.u64 	[%rd2+160], %rd966;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd956;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd955, {vl,vh};
	@p  mov.b64 %rd955, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd955;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd140, [%rd2];
	ld.local.u64 	%rd141, [%rd2+40];
	ld.local.u64 	%rd142, [%rd2+80];
	ld.local.u64 	%rd143, [%rd2+120];
	ld.local.u64 	%rd144, [%rd2+160];
	@%p11 bra 	$L__BB2_175;

	st.shared.u64 	[%r4], %rd144;
	ld.shared.u64 	%rd967, [%r6];
	not.b64 	%rd968, %rd967;
	ld.shared.u64 	%rd969, [%r9];
	and.b64  	%rd970, %rd969, %rd968;
	st.shared.u64 	[%r7], %rd970;

$L__BB2_175:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd971, [%r8];
	xor.b64  	%rd972, %rd140, %rd971;
	st.local.u64 	[%rd2], %rd972;
	xor.b64  	%rd973, %rd141, %rd971;
	st.local.u64 	[%rd2+40], %rd973;
	xor.b64  	%rd974, %rd142, %rd971;
	st.local.u64 	[%rd2+80], %rd974;
	xor.b64  	%rd975, %rd143, %rd971;
	st.local.u64 	[%rd2+120], %rd975;
	xor.b64  	%rd976, %rd144, %rd971;
	st.local.u64 	[%rd2+160], %rd976;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_177;

	ld.const.u64 	%rd1119, [CUDA_KECCAK_CONSTS+160];
	ld.local.u64 	%rd977, [%rd1];
	xor.b64  	%rd978, %rd977, %rd1119;
	st.local.u64 	[%rd1], %rd978;

$L__BB2_177:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_179;

	ld.local.u64 	%rd979, [%rd2];
	ld.local.u64 	%rd980, [%rd2+40];
	xor.b64  	%rd981, %rd980, %rd979;
	ld.local.u64 	%rd982, [%rd2+80];
	xor.b64  	%rd983, %rd981, %rd982;
	ld.local.u64 	%rd984, [%rd2+120];
	xor.b64  	%rd985, %rd983, %rd984;
	ld.local.u64 	%rd986, [%rd2+160];
	xor.b64  	%rd987, %rd985, %rd986;
	st.shared.u64 	[%r4], %rd987;

$L__BB2_179:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_181;

	ld.shared.u64 	%rd990, [%r5];
	ld.shared.u64 	%rd989, [%r6];
	mov.u32 	%r102, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd989;
	shf.l.wrap.b32 vl, tl, th, %r102;
	shf.l.wrap.b32 vh, th, tl, %r102;
	setp.lt.u32 p, %r102, 32;
	@!p mov.b64 %rd988, {vl,vh};
	@p  mov.b64 %rd988, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd991, %rd988, %rd990;
	st.shared.u64 	[%r7], %rd991;

$L__BB2_181:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd994, [%rd2];
	ld.shared.u64 	%rd995, [%r8];
	xor.b64  	%rd993, %rd994, %rd995;
	st.local.u64 	[%rd2], %rd993;
	ld.local.u64 	%rd996, [%rd2+40];
	xor.b64  	%rd997, %rd996, %rd995;
	st.local.u64 	[%rd2+40], %rd997;
	ld.local.u64 	%rd998, [%rd2+80];
	xor.b64  	%rd999, %rd998, %rd995;
	st.local.u64 	[%rd2+80], %rd999;
	ld.local.u64 	%rd1000, [%rd2+120];
	xor.b64  	%rd1001, %rd1000, %rd995;
	st.local.u64 	[%rd2+120], %rd1001;
	ld.local.u64 	%rd1002, [%rd2+160];
	xor.b64  	%rd1003, %rd1002, %rd995;
	st.local.u64 	[%rd2+160], %rd1003;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd993;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd992, {vl,vh};
	@p  mov.b64 %rd992, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd992;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd145, [%rd2];
	ld.local.u64 	%rd146, [%rd2+40];
	ld.local.u64 	%rd147, [%rd2+80];
	ld.local.u64 	%rd148, [%rd2+120];
	ld.local.u64 	%rd149, [%rd2+160];
	@%p11 bra 	$L__BB2_183;

	st.shared.u64 	[%r4], %rd149;
	ld.shared.u64 	%rd1004, [%r6];
	not.b64 	%rd1005, %rd1004;
	ld.shared.u64 	%rd1006, [%r9];
	and.b64  	%rd1007, %rd1006, %rd1005;
	st.shared.u64 	[%r7], %rd1007;

$L__BB2_183:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd1008, [%r8];
	xor.b64  	%rd1009, %rd145, %rd1008;
	st.local.u64 	[%rd2], %rd1009;
	xor.b64  	%rd1010, %rd146, %rd1008;
	st.local.u64 	[%rd2+40], %rd1010;
	xor.b64  	%rd1011, %rd147, %rd1008;
	st.local.u64 	[%rd2+80], %rd1011;
	xor.b64  	%rd1012, %rd148, %rd1008;
	st.local.u64 	[%rd2+120], %rd1012;
	xor.b64  	%rd1013, %rd149, %rd1008;
	st.local.u64 	[%rd2+160], %rd1013;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_185;

	ld.const.u64 	%rd1118, [CUDA_KECCAK_CONSTS+168];
	ld.local.u64 	%rd1014, [%rd1];
	xor.b64  	%rd1015, %rd1014, %rd1118;
	st.local.u64 	[%rd1], %rd1015;

$L__BB2_185:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_187;

	ld.local.u64 	%rd1016, [%rd2];
	ld.local.u64 	%rd1017, [%rd2+40];
	xor.b64  	%rd1018, %rd1017, %rd1016;
	ld.local.u64 	%rd1019, [%rd2+80];
	xor.b64  	%rd1020, %rd1018, %rd1019;
	ld.local.u64 	%rd1021, [%rd2+120];
	xor.b64  	%rd1022, %rd1020, %rd1021;
	ld.local.u64 	%rd1023, [%rd2+160];
	xor.b64  	%rd1024, %rd1022, %rd1023;
	st.shared.u64 	[%r4], %rd1024;

$L__BB2_187:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_189;

	ld.shared.u64 	%rd1027, [%r5];
	ld.shared.u64 	%rd1026, [%r6];
	mov.u32 	%r104, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1026;
	shf.l.wrap.b32 vl, tl, th, %r104;
	shf.l.wrap.b32 vh, th, tl, %r104;
	setp.lt.u32 p, %r104, 32;
	@!p mov.b64 %rd1025, {vl,vh};
	@p  mov.b64 %rd1025, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd1028, %rd1025, %rd1027;
	st.shared.u64 	[%r7], %rd1028;

$L__BB2_189:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd1031, [%rd2];
	ld.shared.u64 	%rd1032, [%r8];
	xor.b64  	%rd1030, %rd1031, %rd1032;
	st.local.u64 	[%rd2], %rd1030;
	ld.local.u64 	%rd1033, [%rd2+40];
	xor.b64  	%rd1034, %rd1033, %rd1032;
	st.local.u64 	[%rd2+40], %rd1034;
	ld.local.u64 	%rd1035, [%rd2+80];
	xor.b64  	%rd1036, %rd1035, %rd1032;
	st.local.u64 	[%rd2+80], %rd1036;
	ld.local.u64 	%rd1037, [%rd2+120];
	xor.b64  	%rd1038, %rd1037, %rd1032;
	st.local.u64 	[%rd2+120], %rd1038;
	ld.local.u64 	%rd1039, [%rd2+160];
	xor.b64  	%rd1040, %rd1039, %rd1032;
	st.local.u64 	[%rd2+160], %rd1040;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1030;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd1029, {vl,vh};
	@p  mov.b64 %rd1029, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd1029;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd150, [%rd2];
	ld.local.u64 	%rd151, [%rd2+40];
	ld.local.u64 	%rd152, [%rd2+80];
	ld.local.u64 	%rd153, [%rd2+120];
	ld.local.u64 	%rd154, [%rd2+160];
	@%p11 bra 	$L__BB2_191;

	st.shared.u64 	[%r4], %rd154;
	ld.shared.u64 	%rd1041, [%r6];
	not.b64 	%rd1042, %rd1041;
	ld.shared.u64 	%rd1043, [%r9];
	and.b64  	%rd1044, %rd1043, %rd1042;
	st.shared.u64 	[%r7], %rd1044;

$L__BB2_191:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd1045, [%r8];
	xor.b64  	%rd1046, %rd150, %rd1045;
	st.local.u64 	[%rd2], %rd1046;
	xor.b64  	%rd1047, %rd151, %rd1045;
	st.local.u64 	[%rd2+40], %rd1047;
	xor.b64  	%rd1048, %rd152, %rd1045;
	st.local.u64 	[%rd2+80], %rd1048;
	xor.b64  	%rd1049, %rd153, %rd1045;
	st.local.u64 	[%rd2+120], %rd1049;
	xor.b64  	%rd1050, %rd154, %rd1045;
	st.local.u64 	[%rd2+160], %rd1050;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_193;

	ld.const.u64 	%rd1117, [CUDA_KECCAK_CONSTS+176];
	ld.local.u64 	%rd1051, [%rd1];
	xor.b64  	%rd1052, %rd1051, %rd1117;
	st.local.u64 	[%rd1], %rd1052;

$L__BB2_193:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_195;

	ld.local.u64 	%rd1053, [%rd2];
	ld.local.u64 	%rd1054, [%rd2+40];
	xor.b64  	%rd1055, %rd1054, %rd1053;
	ld.local.u64 	%rd1056, [%rd2+80];
	xor.b64  	%rd1057, %rd1055, %rd1056;
	ld.local.u64 	%rd1058, [%rd2+120];
	xor.b64  	%rd1059, %rd1057, %rd1058;
	ld.local.u64 	%rd1060, [%rd2+160];
	xor.b64  	%rd1061, %rd1059, %rd1060;
	st.shared.u64 	[%r4], %rd1061;

$L__BB2_195:
	bar.warp.sync 	-1;
	@%p11 bra 	$L__BB2_197;

	ld.shared.u64 	%rd1064, [%r5];
	ld.shared.u64 	%rd1063, [%r6];
	mov.u32 	%r106, 1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1063;
	shf.l.wrap.b32 vl, tl, th, %r106;
	shf.l.wrap.b32 vh, th, tl, %r106;
	setp.lt.u32 p, %r106, 32;
	@!p mov.b64 %rd1062, {vl,vh};
	@p  mov.b64 %rd1062, {vh,vl};
	}

	// end inline asm
	xor.b64  	%rd1065, %rd1062, %rd1064;
	st.shared.u64 	[%r7], %rd1065;

$L__BB2_197:
	bar.warp.sync 	-1;
	ld.local.u64 	%rd1068, [%rd2];
	ld.shared.u64 	%rd1069, [%r8];
	xor.b64  	%rd1067, %rd1068, %rd1069;
	st.local.u64 	[%rd2], %rd1067;
	ld.local.u64 	%rd1070, [%rd2+40];
	xor.b64  	%rd1071, %rd1070, %rd1069;
	st.local.u64 	[%rd2+40], %rd1071;
	ld.local.u64 	%rd1072, [%rd2+80];
	xor.b64  	%rd1073, %rd1072, %rd1069;
	st.local.u64 	[%rd2+80], %rd1073;
	ld.local.u64 	%rd1074, [%rd2+120];
	xor.b64  	%rd1075, %rd1074, %rd1069;
	st.local.u64 	[%rd2+120], %rd1075;
	ld.local.u64 	%rd1076, [%rd2+160];
	xor.b64  	%rd1077, %rd1076, %rd1069;
	st.local.u64 	[%rd2+160], %rd1077;
	bar.warp.sync 	-1;
	// begin inline asm
	{ // ROTL64 
	.reg .u32 tl,th,vl,vh;
	.reg .pred p;
	mov.b64 {tl,th}, %rd1067;
	shf.l.wrap.b32 vl, tl, th, %r13;
	shf.l.wrap.b32 vh, th, tl, %r13;
	setp.lt.u32 p, %r13, 32;
	@!p mov.b64 %rd1066, {vl,vh};
	@p  mov.b64 %rd1066, {vh,vl};
	}

	// end inline asm
	st.local.u64 	[%rd39], %rd1066;
	bar.warp.sync 	-1;
	ld.local.u64 	%rd155, [%rd2];
	ld.local.u64 	%rd156, [%rd2+40];
	ld.local.u64 	%rd157, [%rd2+80];
	ld.local.u64 	%rd158, [%rd2+120];
	ld.local.u64 	%rd159, [%rd2+160];
	@%p11 bra 	$L__BB2_199;

	st.shared.u64 	[%r4], %rd159;
	ld.shared.u64 	%rd1078, [%r6];
	not.b64 	%rd1079, %rd1078;
	ld.shared.u64 	%rd1080, [%r9];
	and.b64  	%rd1081, %rd1080, %rd1079;
	st.shared.u64 	[%r7], %rd1081;

$L__BB2_199:
	bar.warp.sync 	-1;
	ld.shared.u64 	%rd1082, [%r8];
	xor.b64  	%rd1083, %rd155, %rd1082;
	st.local.u64 	[%rd2], %rd1083;
	xor.b64  	%rd1084, %rd156, %rd1082;
	st.local.u64 	[%rd2+40], %rd1084;
	xor.b64  	%rd1085, %rd157, %rd1082;
	st.local.u64 	[%rd2+80], %rd1085;
	xor.b64  	%rd1086, %rd158, %rd1082;
	st.local.u64 	[%rd2+120], %rd1086;
	xor.b64  	%rd1087, %rd159, %rd1082;
	st.local.u64 	[%rd2+160], %rd1087;
	bar.warp.sync 	-1;
	@%p14 bra 	$L__BB2_201;

	ld.const.u64 	%rd1116, [CUDA_KECCAK_CONSTS+184];
	ld.local.u64 	%rd1088, [%rd1];
	xor.b64  	%rd1089, %rd1088, %rd1116;
	st.local.u64 	[%rd1], %rd1089;

$L__BB2_201:
	setp.ne.s32 	%p112, %r2, 0;
	bar.warp.sync 	-1;
	@%p112 bra 	$L__BB2_208;

	ld.param.u64 	%rd1124, [kernel_lilypad_pow_debug_param_2];
	ld.local.v4.u16 	{%rs378, %rs379, %rs380, %rs381}, [%rd1+24];
	shl.b16 	%rs382, %rs379, 8;
	shr.u16 	%rs383, %rs379, 8;
	shl.b16 	%rs384, %rs381, 8;
	shr.u16 	%rs385, %rs381, 8;
	ld.local.v4.u16 	{%rs386, %rs387, %rs388, %rs389}, [%rd1+16];
	shl.b16 	%rs390, %rs386, 8;
	shr.u16 	%rs391, %rs386, 8;
	shl.b16 	%rs392, %rs387, 8;
	shr.u16 	%rs393, %rs387, 8;
	shl.b16 	%rs394, %rs389, 8;
	shr.u16 	%rs395, %rs389, 8;
	shl.b16 	%rs396, %rs380, 8;
	shr.u16 	%rs397, %rs380, 8;
	shl.b16 	%rs398, %rs378, 8;
	shr.u16 	%rs399, %rs378, 8;
	shl.b16 	%rs400, %rs388, 8;
	shr.u16 	%rs401, %rs388, 8;
	or.b16  	%rs402, %rs397, %rs396;
	or.b16  	%rs403, %rs385, %rs384;
	or.b16  	%rs404, %rs399, %rs398;
	or.b16  	%rs405, %rs383, %rs382;
	or.b16  	%rs406, %rs401, %rs400;
	or.b16  	%rs407, %rs395, %rs394;
	or.b16  	%rs408, %rs391, %rs390;
	or.b16  	%rs409, %rs393, %rs392;
	add.u64 	%rd1090, %SP, 208;
	add.u64 	%rd1091, %SPL, 208;
	mov.b32 	%r112, {%rs409, %rs408};
	mov.b32 	%r113, {%rs407, %rs406};
	mov.b32 	%r114, {%rs405, %rs404};
	mov.b32 	%r115, {%rs403, %rs402};
	st.local.v4.u32 	[%rd1091], {%r115, %r114, %r113, %r112};
	ld.local.v4.u16 	{%rs410, %rs411, %rs412, %rs413}, [%rd1+8];
	shl.b16 	%rs414, %rs410, 8;
	shr.u16 	%rs415, %rs410, 8;
	shl.b16 	%rs416, %rs411, 8;
	shr.u16 	%rs417, %rs411, 8;
	shl.b16 	%rs418, %rs413, 8;
	shr.u16 	%rs419, %rs413, 8;
	ld.local.v4.u16 	{%rs420, %rs421, %rs422, %rs423}, [%rd1];
	shl.b16 	%rs424, %rs421, 8;
	shr.u16 	%rs425, %rs421, 8;
	shl.b16 	%rs426, %rs423, 8;
	shr.u16 	%rs427, %rs423, 8;
	shl.b16 	%rs428, %rs412, 8;
	shr.u16 	%rs429, %rs412, 8;
	shl.b16 	%rs430, %rs422, 8;
	shr.u16 	%rs431, %rs422, 8;
	shl.b16 	%rs432, %rs420, 8;
	shr.u16 	%rs433, %rs420, 8;
	or.b16  	%rs434, %rs429, %rs428;
	or.b16  	%rs435, %rs419, %rs418;
	or.b16  	%rs436, %rs415, %rs414;
	or.b16  	%rs437, %rs417, %rs416;
	or.b16  	%rs438, %rs431, %rs430;
	or.b16  	%rs439, %rs427, %rs426;
	or.b16  	%rs440, %rs433, %rs432;
	or.b16  	%rs441, %rs425, %rs424;
	mov.b32 	%r116, {%rs441, %rs440};
	mov.b32 	%r117, {%rs439, %rs438};
	mov.b32 	%r118, {%rs437, %rs436};
	mov.b32 	%r119, {%rs435, %rs434};
	st.local.v4.u32 	[%rd1091+16], {%r119, %r118, %r117, %r116};
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1090;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd1124;
	.param .b32 retval0;
	call.uni (retval0), 
	_ZN39_INTERNAL_9445990f_9_keccak_cu_bbb2fa6e15hashbelowtargetEPKyS1_, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r120, [retval0+0];
	} // callseq 4
	cvt.u16.u32 	%rs442, %r120;
	setp.eq.s16 	%p108, %rs442, 0;
	@%p108 bra 	$L__BB2_207;

	add.u64 	%rd1125, %SPL, 208;
	mov.u64 	%rd1133, 0;
	mov.b32 	%r121, {%rs410, %rs411};
	mov.b32 	%r122, {%rs420, %rs421};
	mov.b32 	%r123, {%rs412, %rs413};
	mov.b32 	%r124, {%rs422, %rs423};
	st.local.v4.u32 	[%rd1125], {%r122, %r124, %r121, %r123};
	mov.b32 	%r125, {%rs378, %rs379};
	mov.b32 	%r126, {%rs386, %rs387};
	mov.b32 	%r127, {%rs380, %rs381};
	mov.b32 	%r128, {%rs388, %rs389};
	st.local.v4.u32 	[%rd1125+16], {%r126, %r128, %r125, %r127};

$L__BB2_204:
	add.u64 	%rd1127, %SPL, 208;
	add.s64 	%rd1094, %rd1127, %rd1133;
	ld.local.u8 	%rs443, [%rd1094];
	add.s64 	%rd1095, %rd161, %rd1133;
	st.global.u8 	[%rd1095], %rs443;
	add.s64 	%rd1133, %rd1133, 1;
	setp.lt.u64 	%p109, %rd1133, 32;
	@%p109 bra 	$L__BB2_204;

	mov.u64 	%rd1134, 0;
	st.global.u8 	[%rd1097], %rs509;
	st.global.u8 	[%rd1097+1], %rs510;
	st.global.u8 	[%rd1097+2], %rs511;
	st.global.u8 	[%rd1097+3], %rs512;
	st.global.u8 	[%rd1097+4], %rs513;
	st.global.u8 	[%rd1097+5], %rs514;
	st.global.u8 	[%rd1097+6], %rs515;
	st.global.u8 	[%rd1097+7], %rs516;
	st.global.u8 	[%rd1097+8], %rs517;
	st.global.u8 	[%rd1097+9], %rs518;
	st.global.u8 	[%rd1097+10], %rs519;
	st.global.u8 	[%rd1097+11], %rs520;
	st.global.u8 	[%rd1097+12], %rs521;
	st.global.u8 	[%rd1097+13], %rs522;
	st.global.u8 	[%rd1097+14], %rs523;
	st.global.u8 	[%rd1097+15], %rs524;
	st.global.u8 	[%rd1097+16], %rs525;
	st.global.u8 	[%rd1097+17], %rs526;
	st.global.u8 	[%rd1097+18], %rs527;
	st.global.u8 	[%rd1097+19], %rs528;
	st.global.u8 	[%rd1097+20], %rs529;
	st.global.u8 	[%rd1097+21], %rs530;
	st.global.u8 	[%rd1097+22], %rs531;
	st.global.u8 	[%rd1097+23], %rs532;
	st.global.u8 	[%rd1097+24], %rs533;
	st.global.u8 	[%rd1097+25], %rs534;
	st.global.u8 	[%rd1097+26], %rs535;
	st.global.u8 	[%rd1097+27], %rs536;
	st.global.u8 	[%rd1097+28], %rs537;
	st.global.u8 	[%rd1097+29], %rs538;
	st.global.u8 	[%rd1097+30], %rs539;
	st.global.u8 	[%rd1097+31], %rs540;
	st.global.u8 	[%rd1097+32], %rs541;
	st.global.u8 	[%rd1097+33], %rs542;
	st.global.u8 	[%rd1097+34], %rs543;
	st.global.u8 	[%rd1097+35], %rs544;
	st.global.u8 	[%rd1097+36], %rs545;
	st.global.u8 	[%rd1097+37], %rs546;
	st.global.u8 	[%rd1097+38], %rs547;
	st.global.u8 	[%rd1097+39], %rs548;
	st.global.u8 	[%rd1097+40], %rs549;
	st.global.u8 	[%rd1097+41], %rs550;
	st.global.u8 	[%rd1097+42], %rs551;
	st.global.u8 	[%rd1097+43], %rs552;
	st.global.u8 	[%rd1097+44], %rs553;
	st.global.u8 	[%rd1097+45], %rs554;
	st.global.u8 	[%rd1097+46], %rs555;
	st.global.u8 	[%rd1097+47], %rs556;
	st.global.u8 	[%rd1097+48], %rs557;
	st.global.u8 	[%rd1097+49], %rs558;
	st.global.u8 	[%rd1097+50], %rs559;
	st.global.u8 	[%rd1097+51], %rs560;
	st.global.u8 	[%rd1097+52], %rs561;
	st.global.u8 	[%rd1097+53], %rs562;
	st.global.u8 	[%rd1097+54], %rs563;
	st.global.u8 	[%rd1097+55], %rs564;
	st.global.u8 	[%rd1097+56], %rs565;
	st.global.u8 	[%rd1097+57], %rs566;
	st.global.u8 	[%rd1097+58], %rs567;
	st.global.u8 	[%rd1097+59], %rs568;
	st.global.u8 	[%rd1097+60], %rs569;
	st.global.u8 	[%rd1097+61], %rs570;
	st.global.u8 	[%rd1097+62], %rs571;
	st.global.u8 	[%rd1097+63], %rs572;

$L__BB2_206:
	add.s64 	%rd1098, %rd1132, %rd1134;
	ld.u8 	%rs444, [%rd1098];
	add.s64 	%rd1099, %rd164, %rd1134;
	st.global.u8 	[%rd1099], %rs444;
	add.s64 	%rd1134, %rd1134, 1;
	setp.lt.u64 	%p110, %rd1134, 32;
	@%p110 bra 	$L__BB2_206;

$L__BB2_207:
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1132;
	call.uni 
	free, 
	(
	param0
	);
	} // callseq 5

$L__BB2_208:
	ld.param.u32 	%r133, [kernel_lilypad_pow_debug_param_4];
	add.s32 	%r134, %r134, 1;
	mad.lo.s32 	%r132, %r2, %r133, %r133;
	setp.lt.u32 	%p111, %r134, %r132;
	@%p111 bra 	$L__BB2_3;

$L__BB2_209:
	ret;

}

